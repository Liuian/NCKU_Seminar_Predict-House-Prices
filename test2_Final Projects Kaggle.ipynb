{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Competition for House Prices: Advanced Regression Techniques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RL         1151\n",
       "RM          218\n",
       "FV           65\n",
       "RH           16\n",
       "C (all)      10\n",
       "Name: MSZoning, dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['MSZoning'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(df.isnull(),yticklabels=False,cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 81 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Id             1460 non-null   int64  \n",
      " 1   MSSubClass     1460 non-null   int64  \n",
      " 2   MSZoning       1460 non-null   object \n",
      " 3   LotFrontage    1201 non-null   float64\n",
      " 4   LotArea        1460 non-null   int64  \n",
      " 5   Street         1460 non-null   object \n",
      " 6   Alley          91 non-null     object \n",
      " 7   LotShape       1460 non-null   object \n",
      " 8   LandContour    1460 non-null   object \n",
      " 9   Utilities      1460 non-null   object \n",
      " 10  LotConfig      1460 non-null   object \n",
      " 11  LandSlope      1460 non-null   object \n",
      " 12  Neighborhood   1460 non-null   object \n",
      " 13  Condition1     1460 non-null   object \n",
      " 14  Condition2     1460 non-null   object \n",
      " 15  BldgType       1460 non-null   object \n",
      " 16  HouseStyle     1460 non-null   object \n",
      " 17  OverallQual    1460 non-null   int64  \n",
      " 18  OverallCond    1460 non-null   int64  \n",
      " 19  YearBuilt      1460 non-null   int64  \n",
      " 20  YearRemodAdd   1460 non-null   int64  \n",
      " 21  RoofStyle      1460 non-null   object \n",
      " 22  RoofMatl       1460 non-null   object \n",
      " 23  Exterior1st    1460 non-null   object \n",
      " 24  Exterior2nd    1460 non-null   object \n",
      " 25  MasVnrType     1452 non-null   object \n",
      " 26  MasVnrArea     1452 non-null   float64\n",
      " 27  ExterQual      1460 non-null   object \n",
      " 28  ExterCond      1460 non-null   object \n",
      " 29  Foundation     1460 non-null   object \n",
      " 30  BsmtQual       1423 non-null   object \n",
      " 31  BsmtCond       1423 non-null   object \n",
      " 32  BsmtExposure   1422 non-null   object \n",
      " 33  BsmtFinType1   1423 non-null   object \n",
      " 34  BsmtFinSF1     1460 non-null   int64  \n",
      " 35  BsmtFinType2   1422 non-null   object \n",
      " 36  BsmtFinSF2     1460 non-null   int64  \n",
      " 37  BsmtUnfSF      1460 non-null   int64  \n",
      " 38  TotalBsmtSF    1460 non-null   int64  \n",
      " 39  Heating        1460 non-null   object \n",
      " 40  HeatingQC      1460 non-null   object \n",
      " 41  CentralAir     1460 non-null   object \n",
      " 42  Electrical     1459 non-null   object \n",
      " 43  1stFlrSF       1460 non-null   int64  \n",
      " 44  2ndFlrSF       1460 non-null   int64  \n",
      " 45  LowQualFinSF   1460 non-null   int64  \n",
      " 46  GrLivArea      1460 non-null   int64  \n",
      " 47  BsmtFullBath   1460 non-null   int64  \n",
      " 48  BsmtHalfBath   1460 non-null   int64  \n",
      " 49  FullBath       1460 non-null   int64  \n",
      " 50  HalfBath       1460 non-null   int64  \n",
      " 51  BedroomAbvGr   1460 non-null   int64  \n",
      " 52  KitchenAbvGr   1460 non-null   int64  \n",
      " 53  KitchenQual    1460 non-null   object \n",
      " 54  TotRmsAbvGrd   1460 non-null   int64  \n",
      " 55  Functional     1460 non-null   object \n",
      " 56  Fireplaces     1460 non-null   int64  \n",
      " 57  FireplaceQu    770 non-null    object \n",
      " 58  GarageType     1379 non-null   object \n",
      " 59  GarageYrBlt    1379 non-null   float64\n",
      " 60  GarageFinish   1379 non-null   object \n",
      " 61  GarageCars     1460 non-null   int64  \n",
      " 62  GarageArea     1460 non-null   int64  \n",
      " 63  GarageQual     1379 non-null   object \n",
      " 64  GarageCond     1379 non-null   object \n",
      " 65  PavedDrive     1460 non-null   object \n",
      " 66  WoodDeckSF     1460 non-null   int64  \n",
      " 67  OpenPorchSF    1460 non-null   int64  \n",
      " 68  EnclosedPorch  1460 non-null   int64  \n",
      " 69  3SsnPorch      1460 non-null   int64  \n",
      " 70  ScreenPorch    1460 non-null   int64  \n",
      " 71  PoolArea       1460 non-null   int64  \n",
      " 72  PoolQC         7 non-null      object \n",
      " 73  Fence          281 non-null    object \n",
      " 74  MiscFeature    54 non-null     object \n",
      " 75  MiscVal        1460 non-null   int64  \n",
      " 76  MoSold         1460 non-null   int64  \n",
      " 77  YrSold         1460 non-null   int64  \n",
      " 78  SaleType       1460 non-null   object \n",
      " 79  SaleCondition  1460 non-null   object \n",
      " 80  SalePrice      1460 non-null   int64  \n",
      "dtypes: float64(3), int64(35), object(43)\n",
      "memory usage: 924.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill Missing Values\n",
    "\n",
    "df['LotFrontage']=df['LotFrontage'].fillna(df['LotFrontage'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Alley'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BsmtCond']=df['BsmtCond'].fillna(df['BsmtCond'].mode()[0])\n",
    "df['BsmtQual']=df['BsmtQual'].fillna(df['BsmtQual'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FireplaceQu']=df['FireplaceQu'].fillna(df['FireplaceQu'].mode()[0])\n",
    "df['GarageType']=df['GarageType'].fillna(df['GarageType'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['GarageYrBlt'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GarageFinish']=df['GarageFinish'].fillna(df['GarageFinish'].mode()[0])\n",
    "df['GarageQual']=df['GarageQual'].fillna(df['GarageQual'].mode()[0])\n",
    "df['GarageCond']=df['GarageCond'].fillna(df['GarageCond'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['PoolQC','Fence','MiscFeature'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 76)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(['Id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSSubClass       0\n",
       "MSZoning         0\n",
       "LotFrontage      0\n",
       "LotArea          0\n",
       "Street           0\n",
       "                ..\n",
       "MoSold           0\n",
       "YrSold           0\n",
       "SaleType         0\n",
       "SaleCondition    0\n",
       "SalePrice        0\n",
       "Length: 75, dtype: int64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MasVnrType']=df['MasVnrType'].fillna(df['MasVnrType'].mode()[0])\n",
    "df['MasVnrArea']=df['MasVnrArea'].fillna(df['MasVnrArea'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV8AAAE5CAYAAAA3GCPGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA+VUlEQVR4nO2dd5hlVZW339U0GZsgSUWCZAYlCArCR9ZBBxSRIMGAKI4itKKiMCqIoyiiA4qgKCAiUQmCkrNkG+hu8oiAYABHAWlAQuP6/lj7dJ26dXJV9enG3/s896m65+599j5pnb1X2ubuCCGEmL1M6LsDQgjxr4iErxBC9ICErxBC9ICErxBC9ICErxBC9MDEpgU33f4auUUIIURLrrtgcyvarpGvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0wMS+OyDmbg66eJ/aModve3xtncEyQrzcMXdvVHDT7a9pVlAIIcQsrrtgcyvaLrWDEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0wMS+OyDmbg66eJ/aModve3xtncEyQrzcMXdvVHDT7a9pVlAIIcQsrrtgcyvaLrWDEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0wMS+OyDmbg66eJ/aModve3xtncEyQrzcMXdvVHDT7a9pVlAIIcQsrrtgcyvaLrWDEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0wMS+OyDmbg66eJ/aModve3xtncEyQrzcMXdvVHDT7a9pVlAIIcQsrrtgcyvaLrWDEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gISvEEL0gbu3+gD7tK3Ttd7sqvNybWtO75/OxdzTP52Lsak3bB8dGp3SsbOt682uOi/Xtub0/ulczD3907kYm3r5j9QOQgjRAxK+QgjRA12E7/Ed2+pSb3bVebm2Naf3b3a2pf7NPW3N6f0bTb1ZWNJfCCGEmI1I7SCEED0g4SuEED0g4SvmWMxs+b77IMR4IeE7TpjZElWfhvuY3GRb35jZwuO06/NybZzdpqKZTTCzt4x5j8SY8q/8gm1kcDOzTYCp7v6Mme0JrA8c7e6/b1B3bWAtYIFsm7v/pKTsBGC6u6/dsP/5uisAq7r75Wa2IDDR3We03c9YYWYPAg4YsDzwRPp/MeBhd1+pwT5uc/f1B7bd7u7r1dT7D+DfGH7OD6upswlwKLACMDH11d39dRV13gL8CFjE3Zc3s3WAj7r7xysPrCH5Y21y3AX1b3T3jRuW3cjdb+rSz9w+3gKsSJw/oPxeT+XnB95TUKfwWpnZHcQ9NeKnqOZvqGhrZeAP7v68mW0BvAH4ibs/2Wf/8ve4mZ3t7u8p609Jm6sBn2Xovs36uFVB2QOq9uXu365ox4A9gNe5+2HppbGsu9/Spr95JtYXAeA4YJ30cB0InAD8BNi8qpKZHQJsQQjfC4G3A9eluiNw93+a2TQzW97dH27YN8zsI8A+wBLAysBywPeBrQvKvh74IfAa4CLgc+7+RPrtFnd/U0kbM6i+sSYNHMtKqd73gfPd/cL0/e3ANjXHsxuwO7CSmZ2f+2kS8Leaut8HFgK2JATjTkCTG+QE4FPArcBLDcoD/A/w78D5AO4+zcw2q+hb23PvJf835VIzew9wjtePMo4lBhWthHaGmZ1C3HtTGTp/Tsm9nvgF8HfinD/foJnt2vRpgLOBDcxsFeJanw+cBryj5/5Z7v/SF30FPyOe9R9Sf9++osP+M44F/glsBRwGzCDO6Yad99gwlO629PdLwN75bTX17iBUG9PS92WAC2rqXJkO7AriBjmfEF5VdaYC8wG359suKXsdsC0xAv0McBewcvrt9qp2unyAWwu2VYYmEm/xLYAbiRdc9lmfGNFX1Z0+8HcR4NIG/by5w7HdPHjesms9FueeeJieSvfDzNz/M4CnGvRvBvHAvJCrW1hv4Bha3wfAPaSZZIs6d471/VbRVvYMfxbYr8lxzo7+5eVIE5lSUH/E8zXO5y9/n5Te600+TUe+M8zsIGBPYDMzmweYt0G9f3iMZmea2STgL9S/3b7csE95nnf3F2JmAGY2kfKR0iLufnH6/0gzuxW42MzeV1FnBGa2NMOn9WUj9b+a2ReAn6b970nN6NVDnfN7M9uGoXO4GrAG8UKr4h/p77Nm9urUVq2KA7jKzL4JnENulOPut1XUeSRNtd3M5gP2J4RQGa3OvbvP06Dfpbh7m5HOBDNbnBgsZP/PGpW5++M19e8ElgX+3KLNG8zs9e5ed02HYWYbAd8F1iQGHfMAz/jA7GuAF9OM6gPA9mlb3TM8O/q3jpk9RZzrBXP/Q8GMMtdGZje5wMw+DpzL8Pu29HqZ2QLA3oxUzX2o4rBeTHLP0z6WIl7snWkqfHclpsF7u/ujSd/xzQb1ppjZYsSU4FbgaWqmwO5+TcM+5bnGzA4mLt5bgY8DF5SUNTNb1N3/ntq7Kk1NzybUFpWY2TuBbwGvJl4mKxAC599KquwGHELcHADXpm1NuBb4f0kQXAFMIa7FHhV1fpnO+TeB24ib5UcN2npz+rtBbpsT06wy/hM4mlAj/AG4FNi3onyrc29mCwEvuvuL6fvqxDT5IXc/d7B8rt4a7n6vma1f9HvJC2VR4h7NHvx8Gadk0GBmF6TfXwHcbWa3MFwIvLOgTqYbnQjsZWYPpDq1utvEMcB7iSn3BsD7gVVq6uxFXK+vuvuDZrYSMSAoOqbZ1r9RvGBvZcimAjGin7Vbqgd5pwD3Eiqzw4jnqWrQAPAd4hlexsy+SqjzvtC+20M0NbgtDDzn7i/lRmAXZQ9Fo4bMVgQmufv0mnJ53ep8xNu58q2eDHV7A28jLsYlwI+84ODMbHfgAR8wrqQXyhfd/SM1/ZtGCKTL3X09M9sS2M3d96mq14XMGGFm+wELuvsRbQxPyWCyQCbsxrhv8wAnu/ueLeq0Ovdmdi3xwv9t0lXeApxK2BB+4+6fL2nneHffx8yuKvjZvcAY0xUzq7R7FA0mLIzDVXUqDdlmNsXdNzCz6ZkgNLMb3L3Su8PCEL28u99XU2629a/rC3Y0ZM9Q1j8zmxe4pO6+MLM1GLIjXenudQK7mob6jlsJI85rgEeIN8CpDeoZMc3+Uvq+PPCmlrqWHYCvNSg3H2HBfT0wX1v9S4v+TEl/pwET0v+3FJS7gJzOevDTsK3bgY2Bm4B/S9sKddm5OgsBXwR+mL6vCmzXoK1FgW8To+spxOh+0Zo6l4zmXAML1/x+R+7/rwDfy13ryvNQsc95S7avkD9ewmB5NGGErD1G4BtNtg38fkqTbQVlrk3n4CfAEamPlfpHQtVwH/Bg+r5u3X1IGBDnT/9vQaiVFhvL/qWyq6b/VwEeJ1QWVwBfb9DWvvk+AYsDH6+pc0uu7bWBJYlBQV1b66dzsB+wfpf7L/9p6udr7v4ssCPwXXd/N+XT7DzHEsIjm2bPAL7XsE0A3P08qqe+mWvV74ipwTHA/cmroKrOamb2QzO71MyuzD4NuvSkmS1CXLhTzexowhg0yJGEAHuQ0MP+MH2eJvSDTZgMHASc6+53mdnrgKLRXJ6TiCliZq3/A/DfDdo6kbg+u6TPU2lfVTwEXG9mXzSzA7JPXUNm9hYzu5s01TOzdczs2IKi+ZnLVsBlAO7+Ai30bRZsZWY/Is5HEWcBC6fy6xJT5ocJIVXUt0HeWrCt8h5k4BlKs4k3NmjrfYQe9RPAM8BrCZewKg4F3gQ8CeDuU6m3BZwNvJTzkFiJ8JAYy/4t7u6/Tf9/ADjd3fcjzt1/NGjrI55zl/PwnqmcvQLHJ1XeF4nB0N3AN6oqmNmXgJMJ9diSwEnJltOdJhKaDiOwVKa1hZAQ8NlnJ+DrwI01de4FVhl4Y99bU2ca8DHihnxj9mlwTAsTN9ZE4mbZH3hl1Sigybax+jA0Mm98zlOZqU22Dfx+SNGnQVs3Ew9kvo8jLOuETvJI4ADgMWChtH2xhsf0ZmL0+jDx0vsA8bAXlZ2e+/9I4Ij0/4T8bwX1PkYYQZ8Bpuc+D1IyOyReqHkPjswT42/A4eN0XxR5ppQeV/o9e34PpKGHRId+5c/79cAOLe/b6eS8TNKzeVdJ2buB/yJ52LTs5z2ECi/7viBwz2iOvanBrcsIDLpZCLfP/T+TGF29q6bOX9z9/tz3BwhjWBUz3f24mjIjcPdncl9PblBlKTN7nbs/AJAMHUu1bTcj02dWFHkh6fayc74yzXw0/2Fmm7r7daneJgx5ThTi7l08U7K6j5jlXTwLfTQ/Qtx7ywNv85h9Qeh8jyzbdzKI7EII3dMJo8oUd6+6XvnObEXc73h4mlQdymmEz/LhQF4HPcNLLO7ufjhwuJkd7u4HVe28sKNDATyD+60yMt2ZdO7zmNmqxKDhhpqmMg+J99PcQ6Jt/6ab2ZHAHwm1w6VpH4vVtZO4FDjLwr/dCaPixSVldyMMgZea2V+Je+NMd2/iofIQ4RnxXPo+PzHb7sy4ppQ0sz0I6/z6hKDaCfiCu/9sjNs5jtDZnUVcgJ0J/db1AO5+TkGdQwkBfS4NXVRSvVYGQTPblsj9+UDatCIRBXZJRRtlXhdGjAaWq6j7VsIKuxZxY24CfNDdry6rk+qtS1yjRVM7j6d60yrqXEXxQ1anJvo5oV8+BtiIEAQbuPt7S8pPdvej67blfvs/4vofBfzS3Z8zsweqhFNSH70KeJQQNKu5+4tm9irCN32DsroD+2nqgpiVX5zQy+frXFtT55W5rwsQ9/sS7v6lijoLEaO+t6VNlwD/7e7PVdRZixBmN7r76WngsKu7f32s+pcGCpOJc39idr9ZuDCu7O6n1LRlwEeJwCUj7vkfuXtlwIWFO9yuhDrkfkLd8cOK8ucRARWXEff8Wwm/9b8AuPv+Ve0V7rOJ8E0j1gMZ6RdX+pAlD4SNiId4a+LEXOE1FkIzW45QuG9CHOR1wGR3L9PVYWZVukn3Av+99HYuKtsqysbMdiCMiAdXlJmf8BCBUJEs5u6PVZR/Cfg9w0djnr6/xt3nK6k3gXjBXUGcewNucve/tjieSQDu/lSDsnn95ALEjTzT3Q+sqbckoQ7YhpjWX0Jc40L/Z2sZZp1mW28jRjpbEbO0bYDXunuRfj57iHclfHV/5u5/TNvXA5auelmmctsTL5RhLojuXmobMbMPE4JnOSJQaCNC0LX2xjCz69x907b1Zhd1/TOzN7r7rQPbtnf3MpfR7H7vlI4gt48tiEjNtdx9/opyH6jaT82sqrRSE33HpYQr1z1EpNWJ1FhyU71KXW1JncsIn8SJ6fNB4LLR6FbG+0MIuLoyiwIfAi4H/lhT9reES1DRb4/U1G2lTwb2TH8PKPp0OBfXjOF53Y3wGnmC4d4iVxGufk32sQDxQjqb0BufVlF2nqb7Lag7DXglSSdKeEscX1PnjtS/qen7GsQ0uK6t9XOfDYjRaZ0t5TJGegVcUlL2rFz/pg9+xql/twGvH7j2tVGXhOth4bNSUWdD4kX5e+AaQm+/ZE2d7UjeTWP1aarzfaW7n5CmetcQQQ1NgiHaxNZnLOXu+ZHsj83sk1UVOo6W5yVOepaL4GrgB17ju2xmO+a+TiBursJjS1OqdxIBKusTjvg7EJ4SVRxFPBxFU9YjaupeZmafAc4kjEBApToly0hWFA1Wec0G1CMTCKPlsjX9I9kMjiZGek6EUX/Kk148xw1ExNiShOdIxgxCENTiMa3+OfBzM3sFYcgtK/uSmT1ruUCQFrzo7n+zyKY2wSOApNKCTvjOP2dmmNn8HoEhqzdoK38uMrvILjV1lvQBr4CkIilicvrbNZdEl/7tRFyjPYBNCT3z26qrAKGuuMsiuCV/vxcFt3yNmN08AZwBbFIlIwZ4L3C0RXa9k3y0Pr40j3DLBNKfLdy6/kRMleo4gHi4Z5rZc1CchGaAv1pkTjs9fd+NmnBcwiXqNEK3BOFbfBLF7j8ZxxH62syN6H1p24dr2mpkEDSzUwnBfimh27wSuN9rdK8A7v699BC/xd1vGPjtuzXVMxVLPtLMKYn4cfcfpH8vd/frB45hk5q28lFGMwkL/941dSCu1feAd6fv7yWu95vzhTyFWZPc5pJKJLtnJxEqrRFYA3e3Cp4D7jCzyxj+MNfp9AZdEP9CsQtinj8kw9J5xEvzCeLZqsTdt6wrU8A/LZewyiKQovDl6skA5Q2yFo5V/9z9ATN7L3EuHiEMrJUG30Qbo+/zwNvd/X879G/PdP/tRriZOSFjTveO2ROb6ny3A35NuAd9l7jxv+zu53dptKat5QlhtTFxc9wA7O8Vhgszm+ru69ZtG/h9mruvU7etKxaRcEY4mp/pYd2vNPoU7KN1dq2S/czn4RtbVaZIrzpi21hgZje7+5sHtt3k7huVlN+HCLL4B+EtU5nu0iKbXile4aVRptvzGp2eRRToP4gZwB6EmulUL9FjF9TfPNW5uOpaJR30pwmDKkRAzBHufr+ZTfRynXZm+M1mrJsB+3i14XdHwv91aeKc1w6e2vbPRqahXJrIpPY80VhdKDNmtgxD2cVucfdKTycz25e4Nk+m74sTUaq1/tzJXrEn8ElCDbsK8J0Gg6KRjKUOo0ZnsjJhba3MlERMBWq3Dfx+eToh86TPnoRxr07HtHLu++uoyapEjHCvJ0ZcjxOj2k3Tb4sWlF+DcHO6j3h5/R+RA7TpOfsyYcRqlS0r1TXC2PQj4LGKchsTD8sjDNf3Hkq9nm5n4BXp/y8QSXlqI38I3+3PE54fKxDG3C8SDuxLFJT/LTU6uTnpQ6hJGl8zIipxA0LlVlUus8x/iIjmXCf9PzVdx7p7fklClbB9k/OZ2lqzxXG07l+6/qWfBm3uQsyOTiYGOg8CO9XUmVqw7faSsjumv9sTnlHTiTwSS+eu3e873Sc1nfwuETVW+GlwYl5FhBbeQkznDiGnVC+pM0IAFm0b+H15whDzf4Sl+by6C0d4YDxM6HqvIdQHW1aU/zjxFt+KGPlPSv/fQOiR6gTVBoSS/2HghoY3c5YS8UVqUiLm6jQOLEjlN0/X5c8MD5Y4gBT2WVE3S1u5KfFyeRfNjCQPVnxGhHkSfpsLtb65w5i1L6FaOjH7lJQtNC5RY2Qi9NZXEy+e9YjoxUfTfbhtSZ13pvvtNiKPwYNEANOjwAeqzjewYsH2FdPzVRmGT9gR3kSMejcDNqspf33L8925f+k8viL3/RXAmxu0OY0kCNP3pRo8i20CM7JAk5+UnS9g67b3prtXqx26uldYJDffjdALn5U+v/CK1RvMbGPgLcRw/n9yP00C3u1jpA4YaHN+YHVilHivu5cGI5jZPcQI/PGB7a8kQlYP8AZBG8mlaTPvlr2tar+DgQXnEoEFTdJJYmYreEsdnw0lKDmciHg8rcoFrCtpKnsSERmX98mu1MOa2c8I177dyWWvcvfJBWVXqNpX2bkxsynAwYTK4HhCp3iTRRKW04vORVJJ7ZzqXAW8wUPnuTQxOnx9SVt3u/taJb/d5+6lxroubm0Wvs/LEoOZ/Hkf4Tc/Bv27nZg1efo+gbh/K9VeZnZH/nyletPKzmEq803ihZAPzHjE3T9dUHZcVG9Qb3A7k3gb/d9Ah5YmRmJlfI+wYO/u7lNSnTrl8nxE4u+JDLe8P0VYQkdgZt+lwiJf9HCa2VbufuWA1wLAymZWemOl/Y0w8HhYuH8/KHjr+saQ7q0SixSWszwy3P2XJUX3IdQbxzEUWFCv0B/i2XRTNvblBv5oZj8gfGi/kV5mE+oaSgLrRMLt68kGffsBYbC8g3Y5VFdx953N7F3ufrKZnUb4FBfxKu+2jNBEd8+isg7L9uHhuVBW55+ejD5m9qAnLw93/4uZVRnpXrSCVV7Si6MuinEyoRe9yd23TC+HOmPVJOBZhnsdODHKH+v+WSZ4YVZkYROHgIvN7BKGDPS7EqvmVPE54nn5GLnAjJKya5hZkWdN0/SapdQd3HeIKd/gyX4rMdX8WEm9VxNv9m8nZfhZ1IQl+pAL249bjMCmNCyXZ3PiQd6+4LeqG+spM1vHByK+LJZWKnJLyvq2CWF8ODN935nwEqjFzL5OPDCnpk2TLUKAi1IpLstQYMFRFtFnC1YZYQY4NfVxO2Ik8AFCjVPFLsTKFEe6+5MW0WCfrakD4d2wF5HveQoxqr00//ANMNPdu3gwZF46T1qsJfgoMeIpousyQvmXwaB1vux48onb/2nDE7dXvbwOAS63cJnKPE02JPTnn6vpZ2u3Nnffq2afY9m/B8xsf2LwAKHmG3Q9LOrjZ9NAalPiHB7vNako3f2fxKj3+xbukst5eUTcgxTLitFToxu5u+K3Qh1JQbnliCVjbiWsg3V6qdWI6dulhJC8ksid2VTvtDgNjB3ASk225X7blFDsH5ouxnbEyOEhktGtpN5V5NIYEi+hqxoey3Ryjt2EbqqJk3vjwIJcnVuzNnPbagMmCKPKJ9JnnabXKdWdQOg//0gY/L5MscHtq8RI5VUko1xRuYJ6H073w2YM5fv4aEnZ24v+b9BG0VJH2fcXS+o8mPrTSOddcL5/kp6n24jE4LXnnVBDLZbu32uJ9dkuLCl7YPpbaPNp2b+fNOzf0oTv7V+ye5acLreg/KrpGO4kRr2vaXHNriZG9UsQarpbgW/X3Rdj/anrZGnWnqrfcmXmH/i+OjVZm2iRbYxYU26NrC1CUD+eLuA2Ne0UGfYq14MiRpeHEULtHML9qdJ7gVAFLJH7vjhwX8ObZPpA3SWoNv5MAHYZ2DaJCiNOrtxN6e8lRCq/9YDf1dSZnG7+w9LnDlL2qwbtvYHQ7d+XHuo3E14XUwvKdhFSI85Fg/tucSJKLfu/saBv82HIQ2aBsdxvi/Y3J156hXmKSfmfidnPiE+LdhYZx2P4NZF4aXVicHdOi7q3p78fJlxmKXuugGPG7RhqOnkNBcnPialEbRhriYCr81xovCAesQBjZjTchxhlzkOsHTUiwXkqtwbhEvM7hqev/CANR/Mtb5K9iBHzj9PnwaY3MKFCyOqenOq+t6ZOp3SVxEh+USK59FXEaOCdNXWmk0uITgTUVL0cLs2uMZF/YndGvqAbP0QNjqnxuSBmMK1Ho3kBXfSpusfrnoWKNhvPDjv278e5/xvdqwP1NybSNz6cvq8DHNvgmK4guaISL+cvVJSfOvC98bkkBgmvSudvw+xerqmzDJHT+KL0fS3SYsJdP3U6388S6dp+zJCeMluTqTADFYCZLUuserFgslRn+qxJhF9cFW0WxHvB05kg1mM6w0N3c0+Fsn51QtAsxnBdzgwqkjAXOIPP+okKxbu7n2RmFxEjOwc+7+6PlrUzUPd0M7uaeNkZsdR6Xd224cXZ75kh7+9EXoImGMNTQb7E8GRAgyyZ/u7sI0OJs36MCP81s52J4IMZFgms1we+4u631/Sv8blw9xVr9lXG4Fpiw3ZLcWThixbJoJYzs+8U9KUumi5bLv1H1C+X3qV/+Xt5Ms1Sp+Y5ingezwdw92lmtllljVho4LOEcRV3n54MpGULASwwIFuGyRqvXvj1MGKGd527/8Yi3P23FeUhBkAnEbEKAP9L3Fcn1NQrpTbCLXk27EuMiCCmmd/ziiiS5KL2QUJQ541iM4i3aqlHgbXINmZmNxFTh8eI6esb3f3B9Nu97r7GYJ1c3Y3d/cay3wvKr1D1u1cYCQc8Fq7xikxNqXyXBSCzuo3PXyrf2mMkV/cAYip6LnHTv4u4vkeVlH+AmCKWtVXmwpSttbUpkTf3SOBgH4iSK6jX5l6qdCeqeZhbkaKktiGix0akWfT6aLpb3b3JihedyLtXdXG1shTBmHc7tJroUTP7jbtvOFBnqpdEqVrx+nwZ7mO4Tl+X/jWh1pUjCdlDLJYGX5Ow7j5ZU+dk4GQze4+7n92mQ97QLzXxSSJpylLA/+QE7zuI1TdGYGYHuvsRwO4WiaIH2y8UNlXCtYoCj4X9LXI2VCXRPoBQo3yr4DenYlmllucPRuGV4e7fTiPzLFXgXjWj0UWJWUfZKKzspZyN7v4DOM7df2GRj7mONX0gX63FsuFFZOd6AWLQkIWHv4HwLy5Mh9hFaHuk+DzDzO7xinzJFbReLj31NfMKcODXHkt0FZGNyI2C0XmDkfkjFvl4PcmN/alfHfivFon/PfV1JyLwpxDvkD8ie/bLBhw1x/WMhU9/1r+NKPZyat6fupFvaugdxHTgd8QFWYmwGl9UU28x4s0+a9QHHOYVGaOsY7axpljKEWotY/hteBL1YT9REe+efATX9XBvwSLX7O1laopcvQnAxj6Q7KYJ6cZfkdzL1d1/UlPnKiKZSbaK7LyEjrbyJk/C5/8RL+Xra0blnRzWzeyXhEfENoQB9h+ETr90JFXWXl0fzOwMYnn1O9L3tYHPuPsHS8q3HoGNZraR6rea3aQ6xxJ5CPL+sL9z930LyhY+G7mG6kbm+XzNmR/tZK/Ic5Gm/scTgVZPELr2PZoMepre712f/VR3fcL7Y21i9r8UEcZc5APciKbC917CAnp/+r4y8KuqaX0qd3bqaHZQ7yPcTkbo9XJ1fkS4Y+XrvOTuI7KNWU32Knf/dtXvs4MkfLfIRiXJr/DqOuGbyrZOrGNmpxB5NKYyNGL0Bg/0fYSwz/q5OOEBURWV9CVihHw28ZDtQCQiL9TTWcfoN4tVGLYlouh+a+FP/HpPwQ0F5TObw08Jo17e5vD9GnXUiKnkaKeXBW2MSrh1bPMuYG1PD3x6ud/hFcnec3UX9uHLZ40bFgmKJnjDTGFd7/eOfZvIUETsfaMdEDZNKdlljTSIxDX5VUu/bGZTa+psODCiudIiHLOILBJudWJqf376vj0lOXPN7AKqRx0j8oCW7KfpcjGHA7enEZIRI/qm63Z1yYe8AZGVv2n5jK/n+gnhjnRoTZ3dgPWyqX1SsdxGuZHkfS37BIDH2m3nmNlCZrYBkcikUPAm/p2wOSxH5NPImEGEAldxTxoA/JS4T/akfsqMmb2/pO8jRmCjFa7pZXQAkUR8H4s12Vb38uhHCJvI8oT3DESGwspRm0XI/wlE5OnyFgFFH3X3j9fUG2FEJKboU9z9FwXlVyfUbNlL8R6LtQqbpH5sfL+b2flVvxc9+zYyEjZjNauJiK2jUvjmGr7LzC5k+Bppv2mw/9aLMhJLVa/s7r9LdV5HiUXXU2pAM7uUiAufkb4fSliEi8gWXtyR8Nv9afq+G+FuVImF8exbDCwXw8Ay4Lk+dvFYyDiAcN96ycz+QY2KI3EncVxNFgXM9zPvlQHNvDIeosWigu5+J8y6r2pTFaZz/R3Cd/sLRNj6Y8CKZva5MiE2GpsD4Rr4MYYSil/LUNRVFRvm/l+ASNyUBRkUYrE81+cIXXvTkG4Iq/utxBQdIrfIz4Aq4ftKQqjdkuvvjZlAKhl0HEV7rwWIY1mDoWfwPYRb6N5mtqW7fzIrmAT8OYRa83jiXlgPuNrMdvT6kO829/vGRDDP6YQev8ozJ6Mquq3KTlFLXWKdk6oa9oK10Qbqr0PcfIumTU8QfoOlb1wz25q4uR4gTs4KhCGnVLeW1CLreEqMY5FjYFrN9PJad9+sbltBvWmEwetyj6QyWxK5QEtXFDazNzBSJ9X5otX07ypgXSKTXN4YUzuiN7PXEOc738/CGUQqfx4dFhU0s/uB7b1+Pb9OCWhy9ecnHvwVB47psKp6Y4GZLQqcUnXe06DhTMIDZFZIt7tXhuKa2RR338DaeRNsXrVPL0j0ZB28FlKZKwn7wcz0fSKh930roepYK1f2ImJJsqsL+vt5d397TVuN7/dkb3krMdB6A/ArIvnRXVVtjBeVI19vH9s9WH8asI7lFmW0WBKoVPi6+xXZNArqs40lTgFuMbNzCSHwbipGHImuS7q3Wi7GzE4kLvRdDOUBaPTGNDMjMnGt5O5fMbPXEglgbqmodmiDYyhq6xuEEWawn6XCl7C2n5v7fnXD5h6rE7yJrgloMn5BTHdvpT6xC6mdTYhzOPgSKjVmlfAsEQJbRdfluV6wWKIq09+uTM3xufs1Fu6Sq7r75an+xBrdahevBQh9+8IMeQMsDLzaY5mmwX6uPCh4c/09vkFbhzYok+3zJSJXzcXpxbwbMcI+zBskQ7dYxWcw8VTnF3kjnW8aARe5ZlSOfHPlnsp9PYCYzgy2sScxEj8lCdvpaftHzOwZdz+tYv9fNbOLae7yBJFn+GoL31NIS7o3OJwnrd1yMRt5SZq9BhxLCMKtiFDmp4mp94aDBc3sGCKHQ6NsaQXsQOgNGwmpxEU+4O9tZqu7+3019aaY2ZnUpyrsmoAmYzl337ZBuTwnEPfGrdQHMMxiwJYwgVAlnFVTrevyXIcQQuS1FstVbULouKv69xFCr7oEYaBajgjU2Lqi2n8SXguvIVQblzJ8eaoyjgCmJnVbZuf4moUx7fKBslXCv4mRbwrwD48saKsR6o5SL6wkdP+DELwrEmqtJgOh7xMBYlsSwS07EaPtzjT1dsgbzRYgRpZ/6mJRNLNH3P21BdtvJ/LczhjYPolIRFPpVJ6mFMswfLRSZgTL6gxb0r2J4LGWy8WY2QnAt9z97rp9F9S9zd3XbzLtM7PJRNThq4ip7OnuPrVFWxcRkWdPt6hzH/BFdz8rff80EXJZ+bIpUWeNUGOZ2UMMLRtUVL5yNJpGTt/15DbWBCtY4qhhvfy0fiZhFKxcnNFGsTyXhc/pRsS5ucnDd7iq/FQiX8rNuXtpWC7cscTCI+VNqX+3uHvh2nRp8HJG0U9Ebo5latq5lXB1XJxISD8FeNbd9ygoezLhKnYREQ17Z4vjyQJ9sr+LEIbwJot8Fu+zifAt6MgEQufZOorEzB529+ULtk/3Everqt/S7/sRo4HHGApx9ao6qV4rf9gk4C9x922q9jtQZzNi+fNHiVFeo76lujcTRpXfJCG8FOF7W+qulaaW702fBQjjwhleYzm2cAtch4ivb5SwPD1gxxMGt2WIKemn2wjw8cTM7iZ8Wx+k4bm38NiYhxgN5c9Dof+yRQrUD6b/P+Dj4CZW0m5b/fww/W3Sw95Wcy5aeS0M1F2cULvkp+gj+mej9ynOBij7AQt6BFEUugaa2T8ZGk3nBV+tITt3/m4ijPWPE/rrOtVSKU1dzQZZlXBbKcSqAxIWLKk2rxX4E1os9z1fTX8mE1PmRosVpv0W+gdSoSv2bkuLn0i4WLVNBA4xJToXWNpipYqdiLXOSvFwSv8Gkdx8vdT+IYRAqeJ8hlz1GuHuf07qnoOIYzuoSvBaywgjG33Ib6WxpoRs1LtBvinKowrzs5BGeRDKjn9WY/U+2V3089eY2cFEDoS3EvlyK8PcaeG1MNC/wlUzKDiHY/CyMguPiT0YWjm78F539yaqqjJ+aRE0dgRDkZ9lCdgb0VTnmwlTS38fpSI5sru/ouy3Ck4Afm5mH3P3h1K7KxI6zrrkFY/QPtSvqz9s26XFH24yjSzC3U9N06qtiXO/Q52hyiIybVti5Ls1EVX45QZttX4I0jn4MzGVWw440cJjpCx/Q6Z6aZoEvyi8OqMyzBriRWSRD2JVD1e6pQif1UIsVnf4b2Jq/nRue5UQbz91HH78XyZejm3Ygfb6+c8ReVDuIGwbF1IvPFYBtvIhr4XjyHktVNSbTMNVM2z0fveTiZf/ue5+l4VralXUYSvMbENiiaGvpO+LEMd+L8OXO2u/7y5qh/HCzP6TOJHZA/I08HWvWRst6VVXJ1xH8lPF0gg3i/W99nf3Vv6wZdOkMuFlEda5GDHKqF0Ha6DuKe7+vrptaXvmQrMd4cN4BnDe4Eyioq0HKR6NVoWs7uC5/ABpKntQdqMWlJ+tU3SLJeQ3IATVamb2aiICb5OCsvsTxqR7CNelydnU2ipCknM6SyNGo8P0lw1GsbP0+S2Oq5V+PqkJp7v72rWFh9e7j0gp+/f0fVHixbRGVb9tKAnNVGIRzOcrVAGZrrzQ797d64Ji8vtaHHiyw4Cqap+3EbnBH08qxDOA/Yh7ZE13L1zirAl1QRYrEAeTnfwtibfuQ0Rmsxe6NlyEu2dLeyxCvBgahRgS2egfJtQTdSqKjCWBuy2czhv7w3qsBbYgEV1UZ9WHULM8T/N1sPIMC9xIOucyw+PBRPb/z3hNgpUS8tPsBQj/2iWKClrKuubu51ksSfM8gLvPTKPhMjqnKrTIsTAYjFDnTvhuwmH/tlT+T0mNVcRHiKx4T6cZ18/NbEV3P5pig19GftmkLstaNRYUOXXFs4Q3QSP9vIcnwDQrWF+thjZeC3n+kKbo5xFpPZ8gPDmK+nZNOrav+HAf+wvMrEqH/SXgLI/sf/MTRrR1gZlmtru7V/WvDfPknqddiWWKzgbOtvpo3Urq1A5nETfw381sXUL3czhxkMcS05gxwQryNFhuAcKqUaynSLeWHNqhDma2PRElNx+wUjovh5UJbe/gK21mBxHCdEEzy9z0DHiByHta1M6Wqe7KFq55z5vZFoTA+4nXLFRZoC8/ysyuoyDlISHks5Hgjbn/IbcW2liRRrBbEML3QkKXex31vtwvuLtbWkg0CY0y5slGku7+UDp3P08DkFLhm43ezWxndx8WVWmRh3gsyYT7rbTUzxNeMHelwUZeXVY62PDwQb6QIa+Fg33Ia6F0rT53f3f691CLIIhFCde4Ktr63e9KuF9CBKdMSOVXI17qYyZ8bWgdxK0Jd72MrjazRpUXzJ3sPYET3f1baRozdTQNF9A6T0NG0uUdSIuVdz2cuJdhyGf2Fq/IUZzjUOJmvDrtZ2q6Ucr6thoRnrqMu69tEe32Ti9JPpP2eThwuJkd7tWpJ4s4G9jAzFYhdOXnE8LyHVWVBoxbE4iRcNko0Ur+L/qep2uqwp0Iw9bt7r5Xum5NjB1nWayuvJiFn+uHKHl5AY+a2bqe3PPSCHg7wmDZxB3rIEaGtBdtGzRILzTwgnUvsbrnBP3CxIKYL6Xv8xCh3VV0GaBA2Dj+TDxXq5jZKl7tVTFMxeHN/c6L/O5Lo0YZuZDC6V6/kEIXTieMlX8lXEx/DZCer7Z2pmHUdTL/IG1FSgiTpjGjaXcE3i1PQ0brlXfNbBfgm4QQNeC7ZvZZd/95TVsz3f3vA8dfNXVsm6E/Tz6ZUfaQfaFmpP/PNP1/N3CUu3/Xwoe6jrxxayahWtqlpKyX/F/0PU/XKXrmRD/Twu/7LxSvwADMejCWcfcjky78KeKlfhHly4q/n4FgmTTaeX8S4GVtvZ14sb1m4GUyaXB/uf12MUjnuYJI15jpfBckDGFvKavQQgjOwlp4LeTa6aTicPeLLSJbm/rdP59UUY8BWzI8SX/dajmN8QjguoK07FBO4E8gdL+dqRO+V5rZWcSbb3FirajMv3NM9b05lh/Y9wuUL/ed0SVM87+IDGp/gVmj58uJ5OxV3GlmuxPTkVWJkMsbKsov5O63DAjrJqGxAFtbBLjsTeioTyS8F6p40SJJ/AcYSgoyb11D3i45ddkI1ohoqLI2uk7RpyQd4g+JKffTVEcXHUXKXubulxG5J7DIiHYUBclSvCIgwqtzKv+JeJG8k+HJ52cQo7nxYIG8sS2N0gsFjpld5+6b2kj3z1rfVlp4LQzQWsVh4aXzUXJ5vM2sKo/3J2m5kEJXvCC5jzfLuFZJnfD9JKFbeRWx4mp2IpZlaC2jsaZLnoYuYZoTBtQMf6NZyOp+xLE/T0znL6F6FNsqQ38ed9/dzHYlXFueJRL41CVX34sY/X/V3R9MKpGf1tTJLNmH0CzxfdUItsmItvEUHcCHUhh+38KveJJXJ7Feseh3d59iYUwbMzzyl0wzs9MqBMVY84yZre/Jz9nMsgTzReyR+tlltP2cuz9nZlgYVu+1SP9YRxcVx3HEIOHY9P19aVuhXcndb7LwPPqnxzpsaxEulve6+4gVauZEWrmaWYQ0bkb4rlYuMTOqTsXNlOVpuNZr8jRYcZjmoV6xVpqZfZMwRuUz+0/3+oxS69X1Z6D8aDL0r0oYD+4glnC6GzjAI7/tmGLdEt8XjmAHt+V+y6bouzC0XBHE9VrL3d9UUu8Kd9+6blvut/vdfZW2v42GdA9+haGosyYjy65tbUi4PGX2mFcRq1qPePHZ8PXYzvbh+bXr2jmXeJl/klA1PAHM6+6V9oMuWEHYfNG23G+HEIbXicTM5s2ECnEbIgr1q2PdxzHHq5dL/iWR/R7iAv+Z8Fe9G/hkVd3RfIgIlVcTKojlCbeutvso7B/hOL5J+n9HItn2/xBW/ZUb7PcqwsH6K8C/tejPwoQBayIhfJvUuRfYOv1vwKepWd6eiD78ebpGD2SfBm1NbbJt4PcRy3UXbcv9tg6hDvl9+pt9dgQWLyi/AOHuNo1Qey2RPisC91S0czrwkYLtewNnjtM9ez/xMrfx2P9AW/MTo8S1CWPgvMD8JWVvL/q/Q5ubE6qV+RqU3YjI9/00oTZ8CXiq7l7KP3+ETr/qXrojyYmFCJ3+pLR9QWqWgZ9TPnUn8a7c/wcTLkskITIuB0hM6/9KhDFOTye5dVvE6Lxo+y+JvLCD2zcALmi472UJXe/1qX9fKCgziZhKH0NEBBnwCcKQ9YuG7Uwq2LZqTZ3rCJeY6cQo7FAiWUtdWzcSqqXs+ybAjSVl307MMB4jQqCzz48Jr5G6tuZtePyTGcrL8GDuMw34REW9ZQg9/NWEIfFbhBrlRmDZcbpvryJUWWO+74K2Gr/08turhFlBvQnAnR37N4UY5NyeBORewNdq6mxN+Opfna7VQ8CWFeVvL/o/fZ86O67DqK9jzQmZmvv/CmJqM64HSIwgXjkG+3mkZHvpDUUkymjTxusJHfULBb/9IgmjjxL+0pelm2rdBvs9MPf/zgO/1d3Etw4eC7FSbV2b6ySh9lD63E7BSypXtvEItqD+dmn/jxOjlhlUjIyA/TreA1sSL/P9iDDZMb9fc21tSPiyHkSkTT2AUBGNZRvLEkE29xDBI+unzxaErrOozku5czwz/V97zlPdU+k265yS/k7PbbuhQb35idnDOpSM5HNlbyaM2ZB76RE+xY1fMn1+6gxuj1hkC/pDusgXA1hEeNVa0DvSJU9DEWXK7LKlw6E86c8szGxNQj+8E2GkO5NQBwzyOk/p+izWBPsrcSM3idp7LxFdBCMNUdtSvQ7Zc8nX8rdm9gli1d+lK45neXd/2AsS35fV8dEbmY4iBPUdnp6YGn5gEf47yxJOgxWtPVY/uapD/7rwVWKavQDNoyzb0nptOnevS6hURWuvhcSzFsnXp5nZEYS6sirApYu3w2Y+FFmZT1g1LzEQmOOpW0ZoaeAw4iJ8z9OihRZhxm909yNLK3ftUIs8DQXuM7N+IgJERrxczOx04Ep3/+HA9r2JpU92renfzYTq4moi1eNzJeWG5QMY/F7Txu0+lHN11v9F3wvqbkiMjBYj9NKLAkd4yVpYozTIdDIyWUQ9bT3w0FSVb7yidV9YWtpnNrXVZW26Lu1sXrTda3yGLaICHyNeQp8iVHDH+fBFeAfrzPHXeKyZoxLrwCwr5gi8Wwhx0f6XIdI0vsCQX+YGxI3ybi9ZNNIiauZrRJTUwyQfV2K9uf8afEOb2UvEaCFz8F2QcBdrkjs0LxA7C/EmVAn6BnXvp90INqu3ISG0r6HiBWsprLOtJbwPLPIAX+nVqyqPto093f2nFknrR5zvogHK7MTM3kWsHvK99P1mYtblhCqt1Id+brjGY01dYp3K+PEG04/WjJWQrdj/Y8Bb0ug9y/L0K3e/sqbqNwlD40o+FH03icjzcCRDq91m7YxmureORcipMTK/Q6HaZBTXykv+b8IjhA69bb2mU/RbCHVX4xWte2Rf4ECLNcpeZHxczbKpe1FazDEfRZnZRoRhdU3iOs0DPFNxTAcSKrOM+Qkd9SLEIKUqgGluuMZjSp3Ot8tSy6PCOuRp6EIHfeB2wGp5QeOxIOjHCJewyYMVrGMqv46Cu+u1qhL0dcLjQOBCi2jCRqk8E0t4s+VXsmP4DHCVDY/7b52waDzx0YcMN+FXqa0RAxSLhE9jzTGEMP0ZMTt8P9WLgs7n7o/kvl/nkRHscStJamSxoO71wOeJiNoH008rErPMly11wndZhpZa3p3Zs9Ry6zwNswkvGuF5rG5ROOrw7qn8utDpWo1yhN7VyHS5mb2twRR9KRvKdvcD0sgrtbces8+YVovFqsdT3f0Zi8Vg1ydya4zldb/CzP7d02IDubb3Ar5A/coUrXH3+81sHo+kNSeZWVUo/eIDdT+R+1qWoWw5YpHONYH/JTxgbgVO8pJ1314u1C0dP6qlljvSdTnt8eZuM3u/D+SQTQ/avRX1ulqMW9HTtWo6gh2k6RR9HmLKmh/FZ1Pu2THSbMNxxCxiHWJGcALhhlhotOrIp4j8uO9w998CWKQf3X2M28lo67Vws5l9pMCY/VFKcnF4WvUktbMBEQm6MbCvmT3p3Vf+nuOpTb1mHZdaHgVdl9Meb/YFzjGzDxFvZid8Oxck8k+UMa467Dw9XKumI9hhtJii/9ndD+vQrz6Y6e6ejE5HpwHEmLo8ufuF6YV1kZntQOQ92JBwu3piLNtKvI8IttiXEPzLEeu4lfEp4DyLxFPZ+npvJHS/O9S0tSDhFbFo+vyJ6qWK5nrqXM1OpuNSy5071CFPw+zEzLYi9NFGRABe0XOXgN6u1QxiJNTKyNR0it7W+6JP0uzsYkIXvRmhKpvq47A0u8W6dOcRUXy7eIm74yj239lrIZXPnhGIZ6TUmG1mx6eyMwhbxU1EFrXxeJnMUdQJ385LLY8lZvZJdz9qdrQ11nSwGHdtZ464Vk0ws+lEFNMbiKn5CcCO7r75QLklvNuSSLMdM1uWmP7/xt1/bWbLA1sMqqlG2UZ+Idv5iRfeS4zxNTaz64lo1kfS96lEYp1FCF1sYUKjjm1dTKRLvZN4mdxINw+auY45zs+3CDN72N1Ll6qfkzGzKRRYjL3FwoBzKl2NTJmvssU6XH9MU/Qx9V/uEzNbEvjb3CpALC2Amft+TGY8M7Ob3H2jMW7PiNHvW9JnbcLwdqO7F/r9vxwYzTr2s5Nxd3EbTzwie+Zx95fc/SQiFv/lwHGEUSYzMv2eGMnWMSMZivYEfmWxQsd4hauPK2a2kZldbWbnmNl6ZnYnMYp7zMy27bt/HenitdAZD+4kVhm5iHA9W5kC982XE3OL8J0rRxCJzGI81cyOMLNPURPnPhcxM43uMiPT0TTzQtiV0BPv7RFR+BoiiGVu5Bgi8vF0YqWXD7v7soTe9/A+OzYKbrZY824YVV4LXTGz/c3sDDN7hFircTvgPiJysnD17JcLc4zawTrkaZgbsJFx7osCx3pFnPvcwlgYmV4GU/Sp7r5u+v8ed18z99tcYzDMY5HT5TziBTnCa8EjSnSs2vo2oeu93t0brfDycmGOEb4vZyyywC3v7vf13ZexpK2RKRkfv07o875CqCiWJGZg73f3uuXF5zhsNubhmN208VoQ7ZHwHWdS2OeRROjlSma2LrE22pjnxeiTJiPYZHw8mBj9Hw+83WMtrjWIaLy5cZSYT6CUJU8ifV/A3edKXbYYf+YWne/czKHAm4AnAdx9KvWrMc/RjMLINNHdL/VY4+1RT2ku3b0qQnCOxt3ncfdJ7v4Kd5+Y/s++S/CKUuZKPepcxkx3/7vZXO2wMcgxDI1gr2RgBEtKul9APn/v4Gq7moKJfykkfMcJM7uQCMu8M4VbzmOxGvH+hIFhbmaiDyXWPyw/gq15ybROlSnEyxWpHcaPHwOXEOuhrU1Yjk8jlkia2/0XO41gNUUXYggZ3MYRixymXyLWXTuFIcHk3vOqA6NBRiYhRo/UDuPLi4SQmp+Ii39ZvOl8dDmAhRBI+I4byer/beB8YH13f7amihDiXwipHcYJM/s18J8+vqt+CCHmUiR8hRCiB+TtIIQQPSDhK4QQPSDhK4QQPSDhK4QQPSDhK4QQPfD/AdgK4DVYKbfDAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BsmtExposure']=df['BsmtExposure'].fillna(df['BsmtExposure'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV8AAAE5CAYAAAA3GCPGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA+N0lEQVR4nO2dd7heVZX/PysJJYChSFMxgFQdlCIo7UdVRx1QVIogFkRwFCGKisKgFEdRRAcUQVFAREVQBFFpUpVugCR0RUCwgKOARHpg/f5Y++Se+97T7705Seb7eZ73ufc9795n79PW2Xu1be6OEEKIecuEvjsghBD/F5HwFUKIHpDwFUKIHpDwFUKIHpDwFUKIHpjUvOjv5BYhhBCtWduKtmrkK4QQPSDhK4QQPSDhK4QQPSDhK4QQPSDhK4QQPSDhK4QQPSDhK4QQPSDhK4QQPSDhK4QQPSDhK4QQPSDhK4QQPSDhK4QQPSDhK4QQPSDhK4QQPSDhK4QQPSDhK4QQPSDhK4QQPSDhK4QQPSDhK4QQPSDhK4QQPSDhK4QQPSDhK4QQPSDhK4QQPSDhK4QQPSDhK4QQPSDhK4QQPTCp7w6IhYvJUw8bse3J+49oXUaIhR1z94ZFf9e0oBBCiLmsbUVbpXYQQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogekPAVQogemNR3B8TCxeSph43Y9uT9R7QuI8TCjrl7w6K/a1pQCCHEXNa2oq1SOwghRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9I+AohRA9M6rsDYuFi8tTDRmx78v4jWpcRYmHH3L1h0d81LSiEEGIua1vRVqkdhBCiByR8hRCiByR8hRCiByR8hRCiByR8hRCiByR8hRCiByR8hRCiByR8hRCiByR8hRCiByR8hRCiByR8hRCiByR8hRCiByR8hRCiByR8hRCiByR8hRCiByR8hRCiByR8hRCiByR8hRCiByR8hRCiByR8hRCiByR8hRCiByR8hRCiByR8hRCiByb13QGxcDF56mEjtj15/xGtywixsGPu3rDo75oWFEIIMZe1rWir1A5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEDEr5CCNEH7t7qA+zbtk7XevOqzsLa1vzeP52LBad/OhdjU2/YPjo0Or1jZ1vXm1d1Fta25vf+6VwsOP3TuRibevmP1A5CCNEDEr5CCNEDXYTvSR3b6lJvXtVZWNua3/s3L9tS/xactub3/o2m3lws6S+EEELMQ6R2EEKIHpDwFUKIHpDwFfMtZja17z4IMV5I+I4TZrZc1afhPqY12dY3ZrbkOO363FwbZ7epaGYTzGzzMe+RGFP+L79gGxnczGwLYIa7P25mewIbAce5+x8b1F0PeAWweLbN3b9XUnYCMMvd12vY/3zdVYG13P0SM5sMTHL32W33M1aY2b2AAwZMBR5J/y8D3O/uqzfYx03uvtHAtpvdfcOaev8B/BvDz/mRNXW2AA4HVgUmpb66u7+sos7mwHeApdx9qpmtD3zQ3T9ceWANyR9rk+MuqH+tu2/WsOym7n5dl37m9rE5sBpx/oDyez2VXwx4R0GdwmtlZrcQ99SIn6Kav6qirTWAP7n702a2DfAq4Hvu/mif/cvf42Z2tru/o6w/JW2uDXySofs26+N2BWUPrNqXu3+1oh0D3gW8zN2PTC+Nld39hjb9zTOpvggAJwLrp4frIOBk4HvA1lWVzOwwYBtC+J4PvAm4KtUdgbs/b2YzzWyqu9/fsG+Y2T7AvsBywBrAKsA3ge0Lyr4S+DbwEuAC4FPu/kj67QZ3f01JG7OpvrGmDBzL6qneN4Hz3P389P1NwOtqjmd3YA9gdTM7L/fTFOAfNXW/CSwBbEsIxp2BJjfIycDHgBuB5xqUB/gf4N+B8wDcfaaZbVXRt7bn3kv+b8rFZvYO4KdeP8o4gRhUtBLaGWZ2OnHvzWDo/Dkl93riZ8A/iXP+dINmdmjTpwHOBjY2szWJa30e8EPgzT33z3L/l77oK/gx8ax/m/r79gUd9p9xAvA8sB1wJDCbOKebdN5jw1C6m9LfzwJ757fV1LuFUG3MTN9XAn5eU+eydGCXEjfIeYTwqqozA1gUuDnfdknZq4A3EiPQTwC3AWuk326uaqfLB7ixYFtlaCLxFt8GuJZ4wWWfjYgRfVXdWQN/lwIubtDP6zsc2/WD5y271mNx7omH6bF0P8zJ/T8beKxB/2YTD8wzubqF9QaOofV9ANxBmkm2qHPrWN9vFW1lz/Angf2bHOe86F9ejjSRKQX1Rzxf43z+8vdJ6b3e5NN05DvbzA4G9gS2MrOJwCIN6j3pMZqdY2ZTgL9R/3Y7omGf8jzt7s/EzADMbBLlI6Wl3P3C9P8xZnYjcKGZvbuizgjMbEWGT+vLRup/N7NDge+n/e9JzejVQ53zRzN7HUPncG1gXeKFVsWT6e8TZvbi1FatigO43My+DPyU3CjH3W+qqPNAmmq7mS0KHEAIoTJanXt3n9ig36W4e5uRzgQzW5YYLGT/zx2VufvDNfVvBVYG/tqizWvM7JXuXndNh2FmmwJfB15ODDomAo/7wOxrgGfTjOq9wI5pW90zPC/6t76ZPUac68m5/6FgRplrI7Ob/NzMPgycw/D7tvR6mdniwN6MVM29v+Kwnk1yz9M+ViBe7J1pKnx3I6bBe7v7g0nf8eUG9aab2TLElOBG4F/UTIHd/cqGfcpzpZkdQly81wMfBn5eUtbMbGl3/2dq7/I0NT2bUFtUYmZvAb4CvJh4maxKCJx/K6myO3AYcXMA/Dpta8Kvgf+XBMGlwHTiWryros4v0jn/MnATcbN8p0Fbr01/N85tc2KaVcZ/AscRaoQ/ARcD+1WUb3XuzWwJ4Fl3fzZ9X4eYJt/n7ucMls/VW9fd7zSzjYp+L3mhLE3co9mDny/jlAwazOzn6fcXALeb2Q0MFwJvKaiT6UYnAXuZ2T2pTq3uNnE88E5iyr0x8B5gzZo6exHX6/Pufq+ZrU4MCIqOaZ71bxQv2BsZsqlAjOjn7pbqQd7pwJ2EyuxI4nmqGjQAfI14hlcys88T6rxD23d7iKYGtyWBp9z9udwI7ILsoWjUkNlqwBR3n1VTLq9bXZR4O1e+1ZOhbm/gDcTFuAj4jhccnJntAdzjA8aV9EL5jLvvU9O/mYRAusTdNzSzbYHd3X3fqnpdyIwRZrY/MNndj25jeEoGk8UzYTfGfZsInObue7ao0+rcm9mviRf+75Ou8gbgB4QN4bfu/umSdk5y933N7PKCn90LjDFdMbNKu0fRYMLCOFxVp9KQbWbT3X1jM5uVCUIzu8bdK707LAzRU939rppy86x/XV+woyF7hrL+mdkiwEV194WZrcuQHekyd68T2NU01HfcSBhxXgI8QLwBftCgnhHT7M+m71OB17TUtewEfKFBuUUJC+4rgUXb6l9a9Gd6+jsTmJD+v6Gg3M/J6awHPw3buhnYDLgO+Le0rVCXnauzBPAZ4Nvp+1rADg3aWhr4KjG6nk6M7peuqXPRaM41sGTN77fk/v8c8I3cta48DxX7XKRk+6r54yUMlscRRsjaYwS+1GTbwO+nN9lWUObX6Rx8Dzg69bFS/0ioGu4C7k3fN6i7DwkD4mLp/20ItdIyY9m/VHat9P+awMOEyuJS4IsN2tov3ydgWeDDNXVuyLW9HrA8MSioa2ujdA72Bzbqcv/lP039fM3dnwDeDnzd3d9G+TQ7zwmE8Mim2bOBbzRsEwB3P5fqqW/mWvUHYmpwPHB38iqoqrO2mX3bzC42s8uyT4MuPWpmSxEX7gdmdhxhDBrkGEKA3UvoYb+dPv8i9INNmAYcDJzj7reZ2cuAotFcnlOJKWJmrf8T8N8N2jqFuD67ps9jaV9V3AdcbWafMbMDs09dQ2a2uZndTprqmdn6ZnZCQdH8zGU74FcA7v4MLfRtFmxnZt8hzkcRZwFLpvIbEFPm+wkhVdS3QV5fsK3yHmTgGUqziVc3aOvdhB71I8DjwEsJl7AqDgdeAzwK4O4zqLcFnA08l/OQWJ3wkBjL/i3r7r9P/78XOMPd9yfO3X80aGsfz7nLeXjPVM5egZOSKu8zxGDoduBLVRXM7LPAaYR6bHng1GTL6U4TCU2HEVgq09pCSAj47LMz8EXg2po6dwJrDryx76ypMxP4EHFDvjr7NDimJYkbaxJxsxwAvLBqFNBk21h9GBqZNz7nqcyMJtsGfj+s6NOgreuJBzLfxxGWdUIneQxwIPAQsETavkzDY3otMXq9n3jpvZd42IvKzsr9fwxwdPp/Qv63gnofIoygjwOzcp97KZkdEi/UvAdH5onxD+CocbovijxTSo8r/Z49vwfR0EOiQ7/y5/1qYKeW9+0scl4m6dm8raTs7cB/kTxsWvbzDkKFl32fDNwxmmNvanDrMgKDbhbCHXP/zyFGV2+tqfM3d7879/0ewhhWxRx3P7GmzAjc/fHc19MaVFnBzF7m7vcAJEPHCm3bzcj0mRVFnkm6veycr0EzH80nzWxLd78q1duCIc+JQty9i2dKVvcBs7yLZ6GP5j7EvTcVeIPH7AtC53tM2b6TQWRXQuieQRhVprt71fXKd2Y74n7Hw9Ok6lB+SPgsHwXkddCzvcTi7u5HAUeZ2VHufnDVzgs7OhTAM7jfKiPTrUnnPtHM1iIGDdfUNJV5SLyH5h4Sbfs3y8yOAf5MqB0uTvtYpq6dxMXAWRb+7U4YFS8sKbs7YQi82Mz+TtwbZ7p7Ew+V+wjPiKfS98WI2XZnxjWlpJm9i7DOb0QIqp2BQ939x2PczomEzu4s4gLsQui3rgZw958W1DmcENDn0NBFJdVrZRA0szcSuT/vSZtWI6LALqpoo8zrwojRwCoVdV9PWGFfQdyYWwDvc/cryuqkehsQ12jp1M7Dqd7MijqXU/yQ1amJfkLol48HNiUEwcbu/s6S8tPc/bi6bbnf/pe4/scCv3D3p8zsnirhlNRHLwIeJATN2u7+rJm9iPBN37is7sB+mrogZuWXJfTy+Tq/rqnzwtzXxYn7fTl3/2xFnSWIUd8b0qaLgP9296cq6ryCEGbXuvsZaeCwm7t/caz6lwYK04hzf0p2v1m4MK7h7qfXtGXAB4nAJSPu+e+4e2XAhYU73G6EOuRuQt3x7Yry5xIBFb8i7vnXE37rfwNw9wOq2ivcZxPhm0asBzHSL670IUseCJsSD/H2xIm51GsshGa2CqFw34I4yKuAae5epqvDzKp0k+4F/nvp7VxUtlWUjZntRBgRD6kosxjhIQKhIlnG3R+qKP8c8EeGj8Y8fX+Juy9aUm8C8YK7lDj3Blzn7n9vcTxTANz9sQZl8/rJxYkbeY67H1RTb3lCHfA6Ylp/EXGNC/2frWWYdZptvYEY6WxHzNJeB7zU3Yv089lDvBvhq/tjd/9z2r4hsGLVyzKV25F4oQxzQXT3UtuImX2AEDyrEIFCmxKCrrU3hpld5e5btq03r6jrn5m92t1vHNi2o7uXuYxm93undAS5fWxDRGq+wt0Xqyj33qr91MyqSis10XdcTLhy3UFEWp1CjSU31avU1ZbU+RXhkzgpfd4H/Go0upXx/hACrq7M0sD7gUuAP9eU/T3hElT02wM1dVvpk4E9098Diz4dzsWVY3hedye8Rh5huLfI5YSrX5N9LE68kM4m9MY/rCg7sel+C+rOBF5I0okS3hIn1dS5JfVvRvq+LjENrmtro9xnY2J0WmdL+RUjvQIuKil7Vq5/swY/49S/m4BXDlz72qhLwvWw8FmpqLMJ8aL8I3AlobdfvqbODiTvprH6NNX5vtDdT05TvSuJoIYmwRBtYuszVnD3/Ej2u2b20aoKHUfLixAnPctFcAXwLa/xXTazt+e+TiBursJjS1OqtxABKhsRjvg7EZ4SVRxLPBxFU9aja+r+ysw+AZxJGIGASnVKlpGsKBqs8poNqEcmEEbLlWv6R7IZHEeM9JwIo/6YJ714jmuIiLHlCc+RjNmEIKjFY1r9E+AnZvYCwpBbVvY5M3vCcoEgLXjW3f9hkU1tgkcASaUFnfCdf8rMMLPFPAJD1mnQVv5cZHaRXWvqLO8DXgFJRVLEtPS3ay6JLv3bmbhG7wK2JPTMb6iuAoS64jaL4Jb8/V4U3PIFYnbzCPAjYIsqGTHAO4HjLLLrneqj9fGleYRbJpD+auHW9RdiqlTHgcTDPcfMnoLiJDQD/N0ic9oZ6fvu1ITjEi5RPyR0SxC+xadS7P6TcSKhr83ciN6dtn2gpq1GBkEz+wEh2C8mdJuXAXd7je4VwN2/kR7izd39moHfvl5TPVOx5CPNnJKIH3f/Vvr3Ene/euAYtqhpKx9lNIew8O9dUwfiWn0DeFv6/k7ier82X8hTmDXJbS6pRLJ7dgqh0hqBNXB3q+Ap4BYz+xXDH+Y6nd6gC+LfKHZBzPOnZFg6l3hpPkI8W5W4+7Z1ZQp43nIJqywCKQpfrp4MUN4ga+FY9c/d7zGzdxLn4gHCwFpp8E20Mfo+DbzJ3X/XoX97pvtvd8LNzAkZc4Z3zJ7YVOe7A/Abwj3o68SNf4S7n9el0Zq2phLCajPi5rgGOMArDBdmNsPdN6jbNvD7THdfv25bVywi4YxwND/Tw7pfafQp2Efr7Fol+1nUwze2qkyRXnXEtrHAzK5399cObLvO3TctKb8vEWTxJOEtU5nu0iKbXile4aVRptvzGp2eRRTok8QM4F2EmukHXqLHLqi/dapzYdW1SjrojxMGVYiAmKPd/W4zm+TlOu3M8JvNWLcC9vVqw+/bCf/XFYlzXjt4ats/G5mGckUik9rTRGN1ocyY2UoMZRe7wd0rPZ3MbD/i2jyavi9LRKnW+nMne8WewEcJNeyawNcaDIpGMpY6jBqdyRqEtbUyUxIxFajdNvD7JemETEyfPQnjXp2OaY3c95dRk1WJGOFeTYy4HiZGtVum35YuKL8u4eZ0F/Hy+l8iB2jTc3YEYcRqlS0r1TXC2PQd4KGKcpsRD8sDDNf3Hk69nm4X4AXp/0OJpDy1kT+E7/anCc+PVQlj7mcIB/blCsr/nhqd3Pz0IdQkja8ZEZW4MaFyqyqXWebfT0Rzrp/+n5GuY909vzyhStixyflMbb28xXG07l+6/qWfBm3uSsyOTiMGOvcCO9fUmVGw7eaSsm9Pf3ckPKNmEXkkVsxduz92uk9qOvl1Imqs8NPgxLyICC28gZjOHUZOqV5SZ4QALNo28PtUwhDzv4Sl+dy6C0d4YNxP6HqvJNQH21aU/zDxFt+OGPlPSf9fQ+iR6gTVxoSS/37gmoY3c5YS8VlqUiLm6jQOLEjlt07X5a8MD5Y4kBT2WVE3S1u5JfFyeSvNjCT3VnxGhHkSfptLtL65w5i1H6FaOiX7lJQtNC5RY2Qi9NZXEC+eDYnoxQfTffjGkjpvSffbTUQeg3uJAKYHgfdWnW9gtYLtq6XnqzIMn7AjvIYY9W4FbFVT/uqW57tz/9J5fEHu+wuA1zZocyZJEKbvKzR4FtsEZmSBJt8rO1/A9m3vTXevVjt0da+wSG6+O6EXPit9fuYVqzeY2WbA5sRw/n9yP00B3uZjpA4YaHMxYB1ilHinu5cGI5jZHcQI/OGB7S8kQlYP9AZBG8mlaSvvlr2tar+DgQXnEIEFTdJJYmareksdnw0lKDmKiHj8YZULWFfSVPZUIjIu75NdqYc1sx8Trn17kMte5e7TCsquWrWvsnNjZtOBQwiVwUmETvE6iyQsZxSdi6SS2iXVuRx4lYfOc0VidPjKkrZud/dXlPx2l7uXGuu6uLVZ+D6vTAxm8ud9hN/8GPTvZmLW5On7BOL+rVR7mdkt+fOV6s0sO4epzJeJF0I+MOMBd/94QdlxUb1BvcHtTOJt9L8DHVqRGImV8Q3Cgr2Hu09PdeqUy4sSib8nMdzy/hhhCR2BmX2dCot80cNpZtu5+2UDXgsAa5hZ6Y2V9jfCwONh4f7joOCt6xtDurdKLFJYzvXIcPdflBTdl1BvnMhQYEG9Qn+IJ9JN2diXG/izmX2L8KH9UnqZTahrKAmsUwi3r0cb9O1bhMHyFtrlUF3T3Xcxs7e6+2lm9kPCp7iIF3m3ZYQmuXsWlXVktg8Pz4WyOs97MvqY2b2evDzc/W9mVmWke9YKVnlJL466KMZphF70OnffNr0c6oxVU4AnGO514MQof6z7Z5nghbmRhU0cAi40s4sYMtDvRqyaU8WniOflQ+QCM0rKrmtmRZ41TdNrllJ3cF8jpnyDJ/v1xFTzQyX1Xky82b+alOFnUROW6EMubN9tMQKb3rBcnq2JB3nHgt+qbqzHzGx9H4j4slhaqcgtKevbFoTx4cz0fRfCS6AWM/si8cD8IG2aZhECXJRKcWWGAguOtYg+m1xlhBngB6mPOxAjgfcSapwqdiVWpjjG3R+1iAb7ZE0dCO+GvYh8z9OJUe3F+YdvgDnu3sWDIfPSedRiLcEHiRFPEV2XEcq/DAat82XHk0/c/rwNT9xe9fI6DLjEwmUq8zTZhNCff6qmn63d2tx9r5p9jmX/7jGzA4jBA4Sab9D1sKiPn0wDqS2Jc3iS16SidPfniVHvNy3cJVfx8oi4eymWFaOnRjdye8VvhTqSgnKrEEvG3EhYB+v0UmsT07eLCSF5GZE7s6neaVkaGDuA1Ztsy/22JaHYPzxdjB2IkcN9JKNbSb3LyaUxJF5Clzc8llnkHLsJ3VQTJ/fGgQW5Ojdmbea21QZMEEaVj6TP+k2vU6o7gdB//pkw+B1BscHt88RI5UUko1xRuYJ6H0j3w1YM5fv4YEnZm4v+b9BG0VJH2fdnS+rcm/rTSOddcL6/l56nm4jE4LXnnVBDLZPu318T67OdX1L2oPS30ObTsn/fa9i/FQnf279l9yw5XW5B+bXSMdxKjHpf0uKaXUGM6pcj1HQ3Al+tuy/G+lPXydKsPVW/5cosNvB9HWqyNtEi2xixpty6WVuEoH44XcDX1bRTZNirXA+KGF0eSQi1nxLuT5XeC4QqYLnc92WBuxreJLMG6i5HtfFnArDrwLYpVBhxcuWuS38vIlL5bQj8oabOtHTzH5k+t5CyXzVo71WEbv+u9FC/lvC6mFFQtouQGnEuGtx3yxJRatn/jQV9mw9DHjKLj+V+W7S/NfHSK8xTTMr/TMx+RnxatLPUOB7Db4jES+sQg7uftqh7c/r7AcJllrLnCjh+3I6hppNXUpD8nJhK1Iaxlgi4Os+FxgviEQswZkbDfYlR5kRi7agRCc5TuXUJl5g/MDx95ftoOJpveZPsRYyYv5s+9za9gQkVQlb3tFT3nTV1OqWrJEbySxPJpS8nRgNvqakzi1xCdCKgpurlcHF2jYn8E3sw8gXd+CFqcEyNzwUxg2k9Gs0L6KJP1T1e9yxUtNl4dtixf9/N/d/oXh2ovxmRvvH+9H194IQGx3QpyRWVeDkfWlF+xsD3xueSGCS8KJ2/TbJ7uabOSkRO4wvS91eQFhPu+qnT+X6SSNf2XYb0lNmaTIUZqADMbGVi1YvJyVKd6bOmEH5xVbRZEO8ZT2eCWI/pRx66mzsqlPXrEIJmGYbrcmZTkYS5wBl87k9UKN7d/VQzu4AY2TnwaXd/sKydgbpnmNkVxMvOiKXW6+q2DS/Ofs8Mef8k8hI0wRieCvI5hicDGmT59HcXHxlKnPVjRPivme1CBB/MtkhgvRHwOXe/uaZ/jc+Fu69Ws68yBtcSG7ZbiiMLn7VIBrWKmX2toC910XTZcunfoX659C79y9/L02iWOjXPscTzeB6Au880s60qa8RCA58kjKu4+6xkIC1bCGDxAdkyTNZ49cKvRxIzvKvc/bcW4e6/rygPMQA6lYhVAPgdcV+dXFOvlNoIt+TZsB8xIoKYZn7DK6JIkova+whBnTeKzSbeqqUeBdYi25iZXUdMHR4ipq+vdvd70293uvu6g3VydTdz92vLfi8ov2rV715hJBzwWLjSKzI1pfJdFoDM6jY+f6l8a4+RXN0DianoOcRN/1bi+h5bUv4eYopY1laZC1O21taWRN7cY4BDfCBKrqBem3up0p2o5mFuRYqSeh0RPTYizaLXR9Pd6O5NVrzoRN69qourlaUIxrzbodVEj5rZb919k4E6M7wkStWK1+fLcB/Ddfq69K8Jta4cScgeZrE0+MsJ6+6jNXVOA04zs3e4+9ltOuQN/VITHyWSpqwA/E9O8L6ZWH1jBGZ2kLsfDexhkSh6sP1CYVMlXKso8Fg4wCJnQ1US7QMJNcpXCn5zKpZVann+YBReGe7+1TQyz1IF7lUzGl2amHWUjcLKXsrZ6O4/gBPd/WcW+ZjreLkP5Ku1WDa8iOxcL04MGrLw8FcR/sWF6RC7CG2PFJ8/MrM7vCJfcgWtl0tPfc28Ahz4jccSXUVkI3KjYHTeYGT+gEU+Xk9y4wDqVwf+u0Xif0993ZkI/CnEO+SPyJ79sgFHzXE9buHTn/VvU4q9nJr3p27kmxp6MzEd+ANxQVYnrMYX1NRbhnizzx31AUd6RcYo65htrCmWcoRayxh+G55EfdhPVMS7Jx/BDTzcW7DINXtzmZoiV28CsJkPJLtpQrrxVyP3cnX379XUuZxIZpKtIrsIoaOtvMmT8Pl/xEv56ppReSeHdTP7BeER8TrCAPskodMvHUmVtVfXBzP7EbG8+i3p+3rAJ9z9fSXlW4/ARjPbSPVbzW5SnROIPAR5f9g/uPt+BWULn41cQ3Uj83y+5syPdppX5LlIU/+TiECrRwhd+7uaDHqa3u9dn/1UdyPC+2M9Yva/AhHGXOQD3IimwvdOwgJ6d/q+BvDLqml9Knd26mh2UO8m3E5G6PVydb5DuGPl6zzn7iOyjVlN9ip3/2rV7/OCJHy3yUYlya/wijrhm8q2TqxjZqcTeTRmMDRi9AYP9F2EsM/6uSzhAVEVlfRZYoR8NvGQ7UQkIi/U01nH6DeLVRjeSETR/d7Cn/iVnoIbCspnNofvE0a9vM3hmzXqqBFTydFOLwvaGJVw69jmbcB6nh749HK/xSuSvefqLunDl88aNywSFE3whpnCut7vHfs2iaGI2LtGOyBsmlKyyxppEIlr8quWHmFmM2rqbDIwornMIhyziCwSbh1ian9e+r4jJTlzzeznVI86RuQBLdlP0+VijgJuTiMkI0b0Tdft6pIPeWMiK3/T8hlfzPUTwh3p8Jo6uwMbZlP7pGK5iXIjybtb9gkAj7XbfmpmS5jZxkQik0LBm/h3wuawCpFPI2M2EQpcxR1pAPB94j7Zk/opM2b2npK+jxiBjVa4ppfRgUQS8X0t1mRbx8ujHyFsIlMJ7xmIDIWVozaLkP+TicjTqRYBRR909w/X1BthRCSm6NPd/WcF5dch1GzZS/EOi7UKm6R+bHy/m9l5Vb8XPfs2MhI2Y22riYito1L45hq+zczOZ/gaab9tsP/WizISS1Wv4e5/SHVeRolF11NqQDO7mIgLn52+H05YhIvIFl58O+G3+/30fXfC3agSC+PZVxhYLoaBZcBzfezisZBxIOG+9ZyZPUmNiiNxK3FcTRYFzPcz75UBzbwy7qPFooLufivMva9qUxWmc/01wnf7UCJs/SFgNTP7VJkQG43NgXAN/BBDCcV/zVDUVRWb5P5fnEjclAUZFGKxPNenCF1705BuCKv7jcQUHSK3yI+BKuH7QkKo3ZDr77WZQCoZdBxLe68FiGNZl6Fn8B2EW+jeZratu380K5gE/E8JteZJxL2wIXCFmb3d60O+29zvmxHBPGcQevwqz5yMqui2KjtFLXWJdU6tatgL1kYbqL8+cfMtnTY9QvgNlr5xzWx74ua6hzg5qxKGnFLdWlKLrO8pMY5FjoGZNdPLX7v7VnXbCurNJAxel3gkldmWyAVauqKwmb2KkTqpzhetpn+XAxsQmeTyxpjaEb2ZvYQ43/l+Fs4gUvlz6bCooJndDezo9ev5dUpAk6u/GPHgrzZwTEdW1RsLzGxp4PSq854GDWcSHiBzQ7rdvTIU18ymu/vG1s6bYOuqfXpBoifr4LWQylxG2A/mpO+TCL3v6wlVxytyZS8gliS7oqC/n3b3N9W01fh+T/aW1xMDrVcBvySSH91W1cZ4UTny9fax3YP1ZwLrW25RRoslgUqFr7tfmk2joD7bWOJ04AYzO4cQAm+jYsSR6Lqke6vlYszsFOJC38ZQHoBGb0wzMyIT1+ru/jkzeymRAOaGimqHNziGora+RBhhBvtZKnwJa/s5ue9XNGzuoTrBm+iagCbjZ8R090bqE7uQ2tmCOIeDL6FSY1YJTxAhsFV0XZ7rGYslqjL97RrUHJ+7X2nhLrmWu1+S6k+q0a128VqA0LcvyZA3wJLAiz2WaRrs5xqDgjfX35MatHV4gzLZPp8jctVcmF7MuxMj7CO9QTJ0i1V8BhNPdX6RN9L5phFwkWtG5cg3V+6x3NcDienMYBt7EiPx05OwnZW272Nmj7v7Dyv2/3kzu5DmLk8QeYavsPA9hbSke4PDedTaLRezqZek2WvACYQg3I4IZf4XMfXeZLCgmR1P5HBolC2tgJ0IvWEjIZW4wAf8vc1sHXe/q6bedDM7k/pUhV0T0GSs4u5vbFAuz8nEvXEj9QEMcxmwJUwgVAln1VTrujzXYYQQeanFclVbEDruqv7tQ+hVlyMMVKsQgRrbV1T7T8Jr4SWEauNihi9PVcbRwIykbsvsHF+wMKZdMlC2Svg3MfJNB570yIK2NqHuKPXCSkL3PwjBuxqh1moyEPomESC2LRHcsjMx2u5MU2+HvNFscWJk+ZcuFkUze8DdX1qw/WYiz+3sge1TiEQ0lU7laUqxEsNHK2VGsKzOsCXdmwgea7lcjJmdDHzF3W+v23dB3ZvcfaMm0z4zm0ZEHb6ImMqe4e4zWrR1ARF59q8Wde4CPuPuZ6XvHydCLitfNiXqrBFqLDO7j6Flg4rKV45G08jp657cxppgBUscNayXn9bPIYyClYsz2iiW57LwOd2UODfXefgOV5WfQeRLuT53Lw3LhTuWWHikvCb17wZ3L1ybLg1eflT0E5GbY6Wadm4kXB2XJRLSTweecPd3FZQ9jXAVu4CIhr21xfFkgT7Z36UIQ3iTRT6L99lE+BZ0ZAKh82wdRWJm97v71ILts7zE/arqt/T7/sRo4CGGQly9qk6q18ofNgn4i9z9dVX7HaizFbH8+YPEKK9R31Ld6wmjym+TEF6B8L0tdddKU8t3ps/ihHHhR15jObZwC1yfiK9vlLA8PWAnEQa3lYgp6cfbCPDxxMxuJ3xb76Xhubfw2JhIjIby56HQf9kiBer70v/v9XFwEytpt61+fpj+Nulhb6o5F628FgbqLkuoXfJT9BH9s9H7FGcDlP2ByR5BFIWugWb2PEOj6bzgqzVk587fdYSx/mFCf12nWiqlqavZIGsRbiuFWHVAwuSSaotYgT+hxXLfi9b0ZxoxZW60WGHab6F/IBW6Yu+2tPgphItV20TgEFOic4AVLVaq2JlY66wUD6f0LxHJzTdM7R9GCJQqzmPIVa8R7v7XpO45mDi2g6sEr7WMMLLRh/xWGmtKyEa9G+ebojyqMD8LaZQHoez45zZW75PdRT9/pZkdQuRAeD2RL7cyzJ0WXgsD/StcNYOCczgGLyuz8Jh4F0MrZxfe6+7eRFVVxi8sgsaOZijysywBeyOa6nwzYWrp74NUJEd29xeU/VbBycBPzOxD7n5fanc1QsdZl7ziAdqH+nX1h227tPj9TaaRRbj7D9K0anvi3O9UZ6iyiEx7IzHy3Z6IKjyiQVutH4J0Dv5KTOVWAU6x8Bgpy9+QqV6aJsEvCq/OqAyzhngRWeSDWMvDlW4Fwme1EIvVHf6bmJr/K7e9Soi3nzoOP/4jiJdjG3aivX7+U0QelFsI28b51AuPNYHtfMhr4URyXgsV9abRcNUMG73f/TTi5X+Ou99m4ZpaFXXYCjPbhFhi6HPp+1LEsd/J8OXO2u+7i9phvDCz/yROZPaA/Av4otesjZb0qusQriP5qWJphJvF+l4HuHsrf9iyaVKZ8LII61yGGGXUroM1UPd0d3933ba0PXOh2YHwYfwRcO7gTKKirXspHo1Whazu5Ln8AGkqe3B2oxaUn6dTdIsl5DcmBNXaZvZiIgJvi4KyBxDGpDsI16Vp2dTaKkKSczpLI0ajw/SXDUaxc/X5LY6rlX4+qQlnuft6tYWH17uLSCn7z/R9aeLFtG5Vv20oCc0MYhHMpytUAZmuvNDv3t3rgmLy+1oWeLTDgKpqnzcRucEfTirEHwH7E/fIy929cImzJtQFWaxKHEx28rcl3rr3EZnNnunacBHuni3tsRTxYmgUYkhko7+fUE/UqSgylgdut3A6b+wP67EW2GQiuqjOqg+hZnma5utg5RkWuJF0zmWGx0OI7P+f8JoEKyXkp9mLE/61yxUVtJR1zd3PtViS5mkAd5+TRsNldE5VaJFjYTAYoc6d8G2Ew/5NqfxfkhqriH2IrHj/SjOun5jZau5+HMUGv4z8skldlrVqLChy6oonCG+CRvp5D0+AmVawvloNbbwW8vwpTdHPJdJ6PkJ4chT17cp0bJ/z4T72PzezKh32Z4GzPLL/LUYY0TYA5pjZHu5e1b82TMw9T7sRyxSdDZxt9dG6ldSpHc4ibuB/mtkGhO7nKOIgTyCmMWOCFeRpsNwChFWjWE+Rbi05vEMdzGxHIkpuUWD1dF6OLBPa3sFX2swOJoTpZDPL3PQMeIbIe1rUzrap7hoWrnlPm9k2hMD7ntcsVFmgLz/WzK6iIOUhIeSzkeC1uf8htxbaWJFGsNsQwvd8Qpd7FfW+3M+4u1taSDQJjTImZiNJd78vnbufpAFIqfDNRu9mtou7D4uqtMhDPJZkwv1GWurnCS+Y29JgI68uKx1sePggn8+Q18IhPuS1ULpWn7u/Lf17uEUQxNKEa1wVbf3udyPcLyGCUyak8msTL/UxE742tA7i9oS7XkZXm1mjypNzJ3tP4BR3/0qaxswYTcMFtM7TkJF0eQfRYuVdDyfulRjymb3BK3IU5zicuBmvSPuZkW6Usr6tTYSnruTu61lEu73FS5LPpH0eBRxlZkd5derJIs4GNjazNQld+XmEsHxzVaUB49YEYiRcNkq0kv+LvufpmqpwZ8KwdbO775WuWxNjx1kWqysvY+Hn+n5KXl7Ag2a2gSf3vDQC3oEwWDZxxzqYkSHtRdsGDdJLDLxg3Uus7jlBvySxIOZz6ftEIrS7ii4DFAgbx1+J52pNM1vTq70qhqk4vLnfeZHffWnUKCMXUjjD6xdS6MIZhLHy74SL6W8A0vPV1s40jLpO5h+k7UgJYdI0ZjTtjsC75WnIaL3yrpntCnyZEKIGfN3MPunuP6lpa467/3Pg+Kumjm0z9OfJJzPKHrJDa0b6z6fp/9uAY9396xY+1HXkjVtzCNXSriVlveT/ou95uk7RMyf6ORZ+33+jeAUGYO6DsZK7H5N04Y8RL/ULKF9W/D0MBMuk0c57kgAva+tNxIvtJQMvkymD+8vtt4tBOs+lRLrGTOc7mTCEbV5WoYUQnIu18FrItdNJxeHuF1pEtjb1u386qaIeArZleJL+utVyGuMRwHUpadmhnMCfQOh+O1MnfC8zs7OIN9+yxFpRmX/nmOp7c0wd2PczlC/3ndElTPO/iAxqf4O5o+dLiOTsVdxqZnsQ05G1iJDLayrKL+HuNwwI6yahsQDbWwS47E3oqE8hvBeqeNYiSfx7GUoKskhdQ94uOXXZCNaIaKiyNrpO0acnHeK3iSn3v6iOLjqWlL3M3X9F5J7AIiPasRQkS/GKgAivzqn8F+JF8haGJ5+fTYzmxoPF88a2NEovFDhmdpW7b2kj3T9rfVtp4bUwQGsVh4WXzgfJ5fE2s6o83h+l5UIKXfGC5D7eLONaJXXC96OEbuVFxIqr2YlYmaG1jMaaLnkauoRpThhQM/yDZiGr+xPH/jQxnb+I6lFsqwz9edx9DzPbjXBteYJI4FOXXH0vYvT/eXe/N6lEvl9TJ7NkH0azxPdVI9gmI9rGU3QAH0ph+E0Lv+IpXp3EerWi3919uoUxbczwyF8y08x+WCEoxprHzWwjT37OZpYlmC/iXamfXUbbT7n7U2aGhWH1Tov0j3V0UXGcSAwSTkjf3522FdqV3P06C8+j5z3WYXsF4WJ5p7uPWKFmfqSVq5lFSONWhO9q5RIzo+pU3ExZnoZfe02eBisO0zzcK9ZKM7MvE8aofGb/WV6fUWrDuv4MlB9Nhv61COPBLcQSTrcDB3rktx1TrFvi+8IR7OC23G/ZFH1XhpYrgrher3D315TUu9Tdt6/blvvtbndfs+1voyHdg59jKOqsyciya1ubEC5PmT3mRcSq1iNefDZ8PbazfXh+7bp2ziFe5h8lVA2PAIu4e6X9oAtWEDZftC3322GE4XUSMbN5LaFCfB0Rhfr5se7jmOPVyyX/gsh+D3GB/0r4q94OfLSq7mg+RITKiwkVxFTCravtPgr7RziOb5H+fzuRbPt/CKv+Gg32eznhYP054N9a9GdJwoA1iRC+TercCWyf/jfg49Qsb09EH/4kXaN7sk+DtmY02Tbw+4jluou25X5bn1CH/DH9zT5vB5YtKL844e42k1B7LZc+qwF3VLRzBrBPwfa9gTPH6Z69m3iZ23jsf6CtxYhR4nqEMXARYLGSsjcX/d+hza0J1cqiDcpuSuT7/hehNnwOeKzuXso/f4ROv+peuiXJiSUInf6UtH0yNcvAzy+fupN4W+7/QwiXJZIQGZcDJKb1fyfCGGelk9y6LWJ0XrT9F0Re2MHtGwM/b7jvlQld79Wpf4cWlJlCTKWPJyKCDPgIYcj6WcN2phRsW6umzlWES8wsYhR2OJGspa6tawnVUvZ9C+DakrJvImYYDxEh0Nnnu4TXSF1bizQ8/mkM5WW4N/eZCXykot5KhB7+CsKQ+BVCjXItsPI43beXE6qsMd93QVuNX3r57VXCrKDeBODWjv2bTgxybk4Cci/gCzV1tid89a9I1+o+YNuK8jcX/Z++z5gX12HU17HmhMzI/X8pMbUZ1wMkRhAvHIP9PFCyvfSGIhJltGnjlYSO+pmC336WhNEHCX/pX6WbaoMG+z0o9/8uA7/V3cQ3Dh4LsVJtXZvrJ6F2X/rcTMFLKle28Qi2oP4Oaf8PE6OW2VSMjID9O94D2xIv8/2JMNkxv19zbW1C+LIeTKRNPZBQEY1lGysTQTZ3EMEjG6XPNoSus6jOc7lzPCf9X3vOU90f0G3WOT39nZXbdk2DeosRs4f1KRnJ58peTxizIffSI3yKG79k+vzUGdwesMgW9Kd0kS8EsIjwqrWgd6RLnoYiypTZZUuHQ3nSn7mY2csJ/fDOhJHuTEIdMMjLPKXrs1gT7O/Ejdwkau+dRHQRjDREvZHqdcieSr6WvzezjxCr/q5YcTxT3f1+L0h8X1bHR29kOpYQ1Ld4emJq+JZF+O9cSzgNVrT2WP3k8g7968LniWn24jSPsmxL67Xp3L0uoVIVrb0WEk9YJF+faWZHE+rKqgCXLt4OW/lQZGU+YdUixEBgvqduGaEVgSOJi/ANT4sWWoQZv9rdjymt3LVDLfI0FLjPzP2JCBAZ8XIxszOAy9z92wPb9yaWPtmtpn/XE6qLK4hUj0+VlBuWD2Dwe00bN/tQztW5/xd9L6i7CTEyWobQSy8NHO0la2GN0iDTychkEfW0/cBDU1W+8YrWfWFpaZ951FaXtem6tLN10Xav8Rm2iAp8iHgJfYxQwZ3owxfhHawz31/jsWa+SqwDc62YI/BuIcRF+1+JSNP4DEN+mRsTN8rbvGTRSIuomS8QUVL3k3xcifXm/mvwDW1mzxGjhczBdzLhLtYkd2heIHYW4k2oEvQN6t5NuxFsVm8TQmhfScUL1lJYZ1tLeB9Y5AG+zKtXVR5tG3u6+/ctktaPON9FA5R5iZm9lVg95Bvp+/XErMsJVVqpD/2CcI3HmrrEOpXx4w2mH60ZKyFbsf+HgM3T6D3L8vRLd7+spuqXCUPj6j4UfTeFyPNwDEOr3WbtjGa6t75FyKkxMr9DodpkFNfKS/5vwgOEDr1tvaZT9BsIdVfjFa17ZD/gIIs1yp5lfFzNsql7UVrMMR9FmdmmhGH15cR1mgg8XnFMBxEqs4zFCB31UsQgpSqAaUG4xmNKnc63y1LLo8I65GnoQgd94A7A2nlB47Eg6IcIl7BpgxWsYyq/joK767WqEvR1wuMg4HyLaMJGqTwTy3mz5VeyY/gEcLkNj/tvnbBoPPHRhww34ZeprREDFIuET2PN8YQw/TExO3wP1YuCLuruD+S+X+WREexhK0lqZLGg7tXAp4mI2nvTT6sRs8yFljrhuzJDSy3vwbxZarl1noZ5hBeN8DxWtygcdXj3VH5d6HStRjlC72pkusTM3tBgir6CDWW7+xZp5JXa25B5Z0yrxWLV4xnu/rjFYrAbEbk1xvK6X2pm/+5psYFc23sBh1K/MkVr3P1uM5vokbTmVDOrCqVfdqDuR3JfyzKUrUIs0vly4HeEB8yNwKlesu7bwkLd0vGjWmq5I12X0x5vbjez9/hADtn0oN1ZUa+rxbgVPV2rpiPYQZpO0ScSU9b8KD6bcs+LkWYbTiRmEesTM4KTCTfEQqNVRz5G5Md9s7v/HsAi/egeY9xORluvhevNbJ8CY/YHKcnF4WnVk9TOxkQk6GbAfmb2qHdf+Xu+pzb1mnVcankUdF1Oe7zZD/ipmb2feDM74ds5mcg/Uca46rDz9HCtmo5gh9Fiiv5Xdz+yQ7/6YI67ezI6HZcGEGPq8uTu56cX1gVmthOR92ATwu3qkbFsK/FuIthiP0Lwr0Ks41bGx4BzLRJPZevrvZrQ/e5U09Zkwiti6fT5C9VLFS3w1LmanUbHpZY7d6hDnoZ5iZltR+ijjYgAvLTnLgG9XavZxEiolZGp6RS9rfdFn6TZ2YWELnorQlU2w8dhaXaLdenOJaL4dvUSd8dR7L+z10Iqnz0jEM9IqTHbzE5KZWcTtorriCxq4/Eyma+oE76dl1oeS8zso+5+7Lxoa6zpYDHu2s58ca2aYGaziCimVxFT85OBt7v71gPllvNuSyLNc8xsZWL6/1t3/42ZTQW2GVRTjbKN/EK2ixEvvOcY42tsZlcT0awPpO8ziMQ6SxG62MKERh3bupBIl3or8TK5lm4eNAsc852fbxFmdr+7ly5VPz9jZtMpsBh7i4UB51e6GpkyX2WLdbj+nKboY+q/3CdmtjzwjwVVgFhaADP3/fjMeGZm17n7pmPcnhGj383TZz3C8Hatuxf6/S8MjGYd+3nJuLu4jScekT0T3f05dz+ViMVfGDiRMMpkRqY/EiPZOmYnQ9GewC8tVugYr3D1ccXMNjWzK8zsp2a2oZndSoziHjKzN/bdv4508VrojAe3EquMXEC4nq1BgfvmwsSCInwXyBFEIrMYzzCzo83sY9TEuS9AzEmju8zIdBzNvBB2I/TEe3tEFL6ECGJZEDmeiHw8g1jp5QPuvjKh9z2qz46Ngust1rwbRpXXQlfM7AAz+5GZPUCs1bgDcBcROVm4evbCwnyjdrAOeRoWBGxknPvSwAleEee+oDAWRqaFYIo+w903SP/f4e4vz/22wBgM81jkdDmXeEGO8FrwiBIdq7a+Suh6r3b3Riu8LCzMN8J3YcYiC9xUd7+r776MJW2NTMn4+EVCn/c5QkWxPDEDe4+71y0vPt9h8zAPx7ymjdeCaI+E7ziTwj6PIUIvVzezDYi10cY8L0afNBnBJuPjIcTo/yTgTR5rca1LROMtiKPEfAKlLHkS6fvi7r5A6rLF+LOg6HwXZA4HXgM8CuDuM6hfjXm+ZhRGpknufrHHGm8Pekpz6e5VEYLzNe4+0d2nuPsL3H1S+j/7LsErSlkg9agLGHPc/Z9mC7TDxiDHMzSCvYyBESwp6X4B+fy9g6vtagom/k8h4TtOmNn5RFjmrSnccqLFasQHEAaGBZlJPpRY/8j8CLbmJdM6VaYQCytSO4wf3wUuItZDW4+wHP+QWCJpQfdf7DSC1RRdiCFkcBtHLHKYfpZYd+10hgSTe8+rDowGGZmEGD1SO4wvzxJCajEiLn6heNP56HIACyGQ8B03ktX/q8B5wEbu/kRNFSHE/yGkdhgnzOw3wH/6+K76IYRYQJHwFUKIHpC3gxBC9ICErxBC9ICErxBC9ICErxBC9ICErxBC9MD/B5qs1jVeCzpgAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BsmtFinType2']=df['BsmtFinType2'].fillna(df['BsmtFinType2'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1422, 75)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>...</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street LotShape LandContour  \\\n",
       "0          60       RL         65.0     8450   Pave      Reg         Lvl   \n",
       "1          20       RL         80.0     9600   Pave      Reg         Lvl   \n",
       "2          60       RL         68.0    11250   Pave      IR1         Lvl   \n",
       "3          70       RL         60.0     9550   Pave      IR1         Lvl   \n",
       "4          60       RL         84.0    14260   Pave      IR1         Lvl   \n",
       "\n",
       "  Utilities LotConfig LandSlope  ... EnclosedPorch 3SsnPorch ScreenPorch  \\\n",
       "0    AllPub    Inside       Gtl  ...             0         0           0   \n",
       "1    AllPub       FR2       Gtl  ...             0         0           0   \n",
       "2    AllPub    Inside       Gtl  ...             0         0           0   \n",
       "3    AllPub    Corner       Gtl  ...           272         0           0   \n",
       "4    AllPub       FR2       Gtl  ...             0         0           0   \n",
       "\n",
       "  PoolArea MiscVal  MoSold  YrSold  SaleType  SaleCondition SalePrice  \n",
       "0        0       0       2    2008        WD         Normal    208500  \n",
       "1        0       0       5    2007        WD         Normal    181500  \n",
       "2        0       0       9    2008        WD         Normal    223500  \n",
       "3        0       0       2    2006        WD        Abnorml    140000  \n",
       "4        0       0      12    2008        WD         Normal    250000  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "##HAndle Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['MSZoning','Street','LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood',\n",
    "         'Condition2','BldgType','Condition1','HouseStyle','SaleType',\n",
    "        'SaleCondition','ExterCond',\n",
    "         'ExterQual','Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n",
    "        'RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Heating','HeatingQC',\n",
    "         'CentralAir',\n",
    "         'Electrical','KitchenQual','Functional',\n",
    "         'FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å°‡categorical featuresçš„å€¼åš one-hot encoding\n",
    "def category_onehot_multcols(multcolumns):\n",
    "    df_final=final_df\n",
    "    i=0  \n",
    "    for fields in multcolumns:\n",
    "        print(fields)\n",
    "        #å°‡ final_df ä¸­çš„æ¬„ä½ fields çš„å€¼åš one-hot encodingï¼Œä¸¦ä¸”ç§»é™¤æŽ‰æ¯å€‹æ¬„ä½çš„ç¬¬ä¸€å€‹å€¼ï¼Œä»¥é¿å…å¤šé‡å…±ç·šæ€§çš„å•é¡Œã€‚\n",
    "        df1=pd.get_dummies(final_df[fields],drop_first=True) \n",
    "        final_df.drop([fields],axis=1,inplace=True)\n",
    "        if i==0:\n",
    "            df_final=df1.copy()\n",
    "        else:\n",
    "            df_final=pd.concat([df_final,df1],axis=1)\n",
    "        i=i+1\n",
    "    df_final=pd.concat([final_df,df_final],axis=1) \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine Test Data \n",
    "\n",
    "test_df=pd.read_csv('formulatedtest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 74)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>...</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>RH</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12500</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120</td>\n",
       "      <td>RL</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5005</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>HLS</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street LotShape LandContour  \\\n",
       "0          20       RH         80.0    11622   Pave      Reg         Lvl   \n",
       "1          20       RL         81.0    14267   Pave      IR1         Lvl   \n",
       "2          60       RL         74.0    13830   Pave      IR1         Lvl   \n",
       "3          60       RL         78.0     9978   Pave      IR1         Lvl   \n",
       "4         120       RL         43.0     5005   Pave      IR1         HLS   \n",
       "\n",
       "  Utilities LotConfig LandSlope  ... OpenPorchSF EnclosedPorch 3SsnPorch  \\\n",
       "0    AllPub    Inside       Gtl  ...           0             0         0   \n",
       "1    AllPub    Corner       Gtl  ...          36             0         0   \n",
       "2    AllPub    Inside       Gtl  ...          34             0         0   \n",
       "3    AllPub    Inside       Gtl  ...          36             0         0   \n",
       "4    AllPub    Inside       Gtl  ...          82             0         0   \n",
       "\n",
       "  ScreenPorch PoolArea  MiscVal  MoSold  YrSold  SaleType SaleCondition  \n",
       "0         120        0        0       6    2010        WD        Normal  \n",
       "1           0        0    12500       6    2010        WD        Normal  \n",
       "2           0        0        0       3    2010        WD        Normal  \n",
       "3           0        0        0       6    2010        WD        Normal  \n",
       "4         144        0        0       1    2010        WD        Normal  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=pd.concat([df,test_df],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainå‡ºç­”æ¡ˆä»¥å¾Œæ–°åŠ çš„codeï¼Œtrain æ™‚åŽ»é™¤ID column\n",
    "final_df.drop(['Id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       208500.0\n",
       "1       181500.0\n",
       "2       223500.0\n",
       "3       140000.0\n",
       "4       250000.0\n",
       "          ...   \n",
       "1454         NaN\n",
       "1455         NaN\n",
       "1456         NaN\n",
       "1457         NaN\n",
       "1458         NaN\n",
       "Name: SalePrice, Length: 2881, dtype: float64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2881, 75)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSZoning\n",
      "Street\n",
      "LotShape\n",
      "LandContour\n",
      "Utilities\n",
      "LotConfig\n",
      "LandSlope\n",
      "Neighborhood\n",
      "Condition2\n",
      "BldgType\n",
      "Condition1\n",
      "HouseStyle\n",
      "SaleType\n",
      "SaleCondition\n",
      "ExterCond\n",
      "ExterQual\n",
      "Foundation\n",
      "BsmtQual\n",
      "BsmtCond\n",
      "BsmtExposure\n",
      "BsmtFinType1\n",
      "BsmtFinType2\n",
      "RoofStyle\n",
      "RoofMatl\n",
      "Exterior1st\n",
      "Exterior2nd\n",
      "MasVnrType\n",
      "Heating\n",
      "HeatingQC\n",
      "CentralAir\n",
      "Electrical\n",
      "KitchenQual\n",
      "Functional\n",
      "FireplaceQu\n",
      "GarageType\n",
      "GarageFinish\n",
      "GarageQual\n",
      "GarageCond\n",
      "PavedDrive\n"
     ]
    }
   ],
   "source": [
    "final_df=category_onehot_multcols(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2881, 235)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df =final_df.loc[:,~final_df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2881, 175)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>Min1</th>\n",
       "      <th>Min2</th>\n",
       "      <th>Typ</th>\n",
       "      <th>Attchd</th>\n",
       "      <th>Basment</th>\n",
       "      <th>BuiltIn</th>\n",
       "      <th>CarPort</th>\n",
       "      <th>Detchd</th>\n",
       "      <th>RFn</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>160</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1936</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>160</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1894</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>20</td>\n",
       "      <td>160.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1960</td>\n",
       "      <td>1996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1224.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>85</td>\n",
       "      <td>62.0</td>\n",
       "      <td>10441</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>60</td>\n",
       "      <td>74.0</td>\n",
       "      <td>9627</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1993</td>\n",
       "      <td>1994</td>\n",
       "      <td>94.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2881 rows Ã— 175 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0             60         65.0     8450            7            5       2003   \n",
       "1             20         80.0     9600            6            8       1976   \n",
       "2             60         68.0    11250            7            5       2001   \n",
       "3             70         60.0     9550            7            5       1915   \n",
       "4             60         84.0    14260            8            5       2000   \n",
       "...          ...          ...      ...          ...          ...        ...   \n",
       "1454         160         21.0     1936            4            7       1970   \n",
       "1455         160         21.0     1894            4            5       1970   \n",
       "1456          20        160.0    20000            5            7       1960   \n",
       "1457          85         62.0    10441            5            5       1992   \n",
       "1458          60         74.0     9627            7            5       1993   \n",
       "\n",
       "      YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  Min1  Min2  Typ  \\\n",
       "0             2003       196.0       706.0         0.0  ...     0     0    1   \n",
       "1             1976         0.0       978.0         0.0  ...     0     0    1   \n",
       "2             2002       162.0       486.0         0.0  ...     0     0    1   \n",
       "3             1970         0.0       216.0         0.0  ...     0     0    1   \n",
       "4             2000       350.0       655.0         0.0  ...     0     0    1   \n",
       "...            ...         ...         ...         ...  ...   ...   ...  ...   \n",
       "1454          1970         0.0         0.0         0.0  ...     0     0    1   \n",
       "1455          1970         0.0       252.0         0.0  ...     0     0    1   \n",
       "1456          1996         0.0      1224.0         0.0  ...     0     0    1   \n",
       "1457          1992         0.0       337.0         0.0  ...     0     0    1   \n",
       "1458          1994        94.0       758.0         0.0  ...     0     0    1   \n",
       "\n",
       "      Attchd  Basment  BuiltIn  CarPort  Detchd  RFn  P  \n",
       "0          1        0        0        0       0    1  0  \n",
       "1          1        0        0        0       0    1  0  \n",
       "2          1        0        0        0       0    1  0  \n",
       "3          0        0        0        0       1    0  0  \n",
       "4          1        0        0        0       0    1  0  \n",
       "...      ...      ...      ...      ...     ...  ... ..  \n",
       "1454       1        0        0        0       0    0  0  \n",
       "1455       0        0        0        1       0    0  0  \n",
       "1456       0        0        0        0       1    0  0  \n",
       "1457       1        0        0        0       0    0  0  \n",
       "1458       1        0        0        0       0    0  0  \n",
       "\n",
       "[2881 rows x 175 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train=final_df.iloc[:1422,:]\n",
    "df_Test=final_df.iloc[1422:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>Min1</th>\n",
       "      <th>Min2</th>\n",
       "      <th>Typ</th>\n",
       "      <th>Attchd</th>\n",
       "      <th>Basment</th>\n",
       "      <th>BuiltIn</th>\n",
       "      <th>CarPort</th>\n",
       "      <th>Detchd</th>\n",
       "      <th>RFn</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 175 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0          60         65.0     8450            7            5       2003   \n",
       "1          20         80.0     9600            6            8       1976   \n",
       "2          60         68.0    11250            7            5       2001   \n",
       "3          70         60.0     9550            7            5       1915   \n",
       "4          60         84.0    14260            8            5       2000   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  Min1  Min2  Typ  \\\n",
       "0          2003       196.0       706.0         0.0  ...     0     0    1   \n",
       "1          1976         0.0       978.0         0.0  ...     0     0    1   \n",
       "2          2002       162.0       486.0         0.0  ...     0     0    1   \n",
       "3          1970         0.0       216.0         0.0  ...     0     0    1   \n",
       "4          2000       350.0       655.0         0.0  ...     0     0    1   \n",
       "\n",
       "   Attchd  Basment  BuiltIn  CarPort  Detchd  RFn  P  \n",
       "0       1        0        0        0       0    1  0  \n",
       "1       1        0        0        0       0    1  0  \n",
       "2       1        0        0        0       0    1  0  \n",
       "3       0        0        0        0       1    0  0  \n",
       "4       1        0        0        0       0    1  0  \n",
       "\n",
       "[5 rows x 175 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>Min1</th>\n",
       "      <th>Min2</th>\n",
       "      <th>Typ</th>\n",
       "      <th>Attchd</th>\n",
       "      <th>Basment</th>\n",
       "      <th>BuiltIn</th>\n",
       "      <th>CarPort</th>\n",
       "      <th>Detchd</th>\n",
       "      <th>RFn</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1961</td>\n",
       "      <td>1961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1958</td>\n",
       "      <td>1958</td>\n",
       "      <td>108.0</td>\n",
       "      <td>923.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1997</td>\n",
       "      <td>1998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>791.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "      <td>20.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5005</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 175 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0          20         80.0    11622            5            6       1961   \n",
       "1          20         81.0    14267            6            6       1958   \n",
       "2          60         74.0    13830            5            5       1997   \n",
       "3          60         78.0     9978            6            6       1998   \n",
       "4         120         43.0     5005            8            5       1992   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  Min1  Min2  Typ  \\\n",
       "0          1961         0.0       468.0       144.0  ...     0     0    1   \n",
       "1          1958       108.0       923.0         0.0  ...     0     0    1   \n",
       "2          1998         0.0       791.0         0.0  ...     0     0    1   \n",
       "3          1998        20.0       602.0         0.0  ...     0     0    1   \n",
       "4          1992         0.0       263.0         0.0  ...     0     0    1   \n",
       "\n",
       "   Attchd  Basment  BuiltIn  CarPort  Detchd  RFn  P  \n",
       "0       1        0        0        0       0    0  0  \n",
       "1       1        0        0        0       0    0  0  \n",
       "2       1        0        0        0       0    0  0  \n",
       "3       1        0        0        0       0    0  0  \n",
       "4       1        0        0        0       0    1  0  \n",
       "\n",
       "[5 rows x 175 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1422, 175)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peggy/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "df_Test.drop(['SalePrice'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=df_Train.drop(['SalePrice'],axis=1)\n",
    "y_train=df_Train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSSubClass       0\n",
       "LotFrontage      0\n",
       "LotArea          0\n",
       "OverallQual      0\n",
       "OverallCond      0\n",
       "YearBuilt        0\n",
       "YearRemodAdd     0\n",
       "MasVnrArea       0\n",
       "BsmtFinSF1       0\n",
       "BsmtFinSF2       0\n",
       "BsmtUnfSF        0\n",
       "TotalBsmtSF      0\n",
       "1stFlrSF         0\n",
       "2ndFlrSF         0\n",
       "LowQualFinSF     0\n",
       "GrLivArea        0\n",
       "BsmtFullBath     0\n",
       "BsmtHalfBath     0\n",
       "FullBath         0\n",
       "HalfBath         0\n",
       "BedroomAbvGr     0\n",
       "KitchenAbvGr     0\n",
       "TotRmsAbvGrd     0\n",
       "Fireplaces       0\n",
       "GarageCars       0\n",
       "GarageArea       0\n",
       "WoodDeckSF       0\n",
       "OpenPorchSF      0\n",
       "EnclosedPorch    0\n",
       "3SsnPorch        0\n",
       "ScreenPorch      0\n",
       "PoolArea         0\n",
       "MiscVal          0\n",
       "MoSold           0\n",
       "YrSold           0\n",
       "FV               0\n",
       "RH               0\n",
       "RL               0\n",
       "RM               0\n",
       "Pave             0\n",
       "IR2              0\n",
       "IR3              0\n",
       "Reg              0\n",
       "HLS              0\n",
       "Low              0\n",
       "Lvl              0\n",
       "NoSeWa           0\n",
       "CulDSac          0\n",
       "FR2              0\n",
       "FR3              0\n",
       "Inside           0\n",
       "Mod              0\n",
       "Sev              0\n",
       "Blueste          0\n",
       "BrDale           0\n",
       "BrkSide          0\n",
       "ClearCr          0\n",
       "CollgCr          0\n",
       "Crawfor          0\n",
       "Edwards          0\n",
       "Gilbert          0\n",
       "IDOTRR           0\n",
       "MeadowV          0\n",
       "Mitchel          0\n",
       "NAmes            0\n",
       "NPkVill          0\n",
       "NWAmes           0\n",
       "NoRidge          0\n",
       "NridgHt          0\n",
       "OldTown          0\n",
       "SWISU            0\n",
       "Sawyer           0\n",
       "SawyerW          0\n",
       "Somerst          0\n",
       "StoneBr          0\n",
       "Timber           0\n",
       "Veenker          0\n",
       "Feedr            0\n",
       "Norm             0\n",
       "PosA             0\n",
       "PosN             0\n",
       "RRAe             0\n",
       "RRAn             0\n",
       "RRNn             0\n",
       "2fmCon           0\n",
       "Duplex           0\n",
       "Twnhs            0\n",
       "TwnhsE           0\n",
       "RRNe             0\n",
       "1.5Unf           0\n",
       "1Story           0\n",
       "2.5Fin           0\n",
       "2.5Unf           0\n",
       "2Story           0\n",
       "SFoyer           0\n",
       "SLvl             0\n",
       "CWD              0\n",
       "Con              0\n",
       "ConLD            0\n",
       "ConLI            0\n",
       "ConLw            0\n",
       "New              0\n",
       "Oth              0\n",
       "WD               0\n",
       "AdjLand          0\n",
       "Alloca           0\n",
       "Family           0\n",
       "Normal           0\n",
       "Partial          0\n",
       "Fa               0\n",
       "Gd               0\n",
       "Po               0\n",
       "TA               0\n",
       "CBlock           0\n",
       "PConc            0\n",
       "Slab             0\n",
       "Stone            0\n",
       "Wood             0\n",
       "Mn               0\n",
       "No               0\n",
       "BLQ              0\n",
       "GLQ              0\n",
       "LwQ              0\n",
       "Rec              0\n",
       "Unf              0\n",
       "Gable            0\n",
       "Gambrel          0\n",
       "Hip              0\n",
       "Mansard          0\n",
       "Shed             0\n",
       "CompShg          0\n",
       "Membran          0\n",
       "Metal            0\n",
       "Roll             0\n",
       "Tar&Grv          0\n",
       "WdShake          0\n",
       "WdShngl          0\n",
       "AsphShn          0\n",
       "BrkComm          0\n",
       "BrkFace          0\n",
       "CemntBd          0\n",
       "HdBoard          0\n",
       "ImStucc          0\n",
       "MetalSd          0\n",
       "Plywood          0\n",
       "Stucco           0\n",
       "VinylSd          0\n",
       "Wd Sdng          0\n",
       "WdShing          0\n",
       "Brk Cmn          0\n",
       "CmentBd          0\n",
       "Other            0\n",
       "Wd Shng          0\n",
       "None             0\n",
       "GasW             0\n",
       "Grav             0\n",
       "OthW             0\n",
       "Wall             0\n",
       "Y                0\n",
       "FuseF            0\n",
       "FuseP            0\n",
       "Mix              0\n",
       "SBrkr            0\n",
       "Maj2             0\n",
       "Min1             0\n",
       "Min2             0\n",
       "Typ              0\n",
       "Attchd           0\n",
       "Basment          0\n",
       "BuiltIn          0\n",
       "CarPort          0\n",
       "Detchd           0\n",
       "RFn              0\n",
       "P                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediciton and selecting the Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "             gamma=0, gpu_id=-1, importance_type=None,\n",
       "             interaction_constraints='', learning_rate=0.300000012,\n",
       "             max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "             monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "classifier = xgboost.XGBRegressor()\n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = xgboost.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyper Parameter Optimization\n",
    "n_estimators = [100, 500, 900, 1100, 1500]\n",
    "max_depth = [2, 3, 5, 10]\n",
    "learning_rate = [0.05, 0.1, 0.15, 0.20]\n",
    "min_child_weight = [1, 2, 3, 4]\n",
    "#booster = ['gbtree', 'gblinear']\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "hyperparameter_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_depth':max_depth,\n",
    "    'learning_rate':learning_rate,\n",
    "    'min_child_weight':min_child_weight,\n",
    "    #'booster':booster,\n",
    "    #'base_score':base_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the random search with 4-fold cross validation\n",
    "#regressorè¨­å®šè¨“ç·´çš„å­¸ç¿’å™¨\n",
    "#hyperparameter_gridå­—å…¸åž‹åˆ¥ï¼Œæ”¾å…¥åƒæ•¸æœå°‹ç¯„åœ\n",
    "#n_iter=300ï¼Œè¨“ç·´300æ¬¡ï¼Œæ•¸å€¼è¶Šå¤§ï¼Œç²å¾—çš„åƒæ•¸ç²¾åº¦è¶Šå¤§ï¼Œä½†æ˜¯æœå°‹æ™‚é–“è¶Šé•·\n",
    "#n_jobs = -1ï¼Œä½¿ç”¨æ‰€æœ‰çš„CPUé€²è¡Œè¨“ç·´ï¼Œé è¨­çˆ²1ï¼Œä½¿ç”¨1å€‹CPU\n",
    "#scoring = 'neg_log_loss'ï¼Œç²¾åº¦è©•åƒ¹æ–¹å¼è¨­å®šçˆ²ã€Œneg_log_lossã€\n",
    "#cv : In this we have to pass a interger value, as it signifies the number of splits that is needed for cross validation. By default is set as five.\n",
    "#verbose æ˜¯ Scikit-learn ä¸­è¨±å¤šæ¨¡åž‹çš„ä¸€å€‹åƒæ•¸ï¼Œç”¨æ–¼æŽ§åˆ¶æ¨¡åž‹è¨“ç·´æ™‚çš„è¼¸å‡ºä¿¡æ¯ã€‚ç•¶ verbose çš„å€¼è¶Šå¤§æ™‚ï¼Œæ¨¡åž‹çš„è¨“ç·´éŽç¨‹ä¸­è¼¸å‡ºçš„ä¿¡æ¯è¶Šè©³ç´°ï¼Œå¯ä»¥ç”¨ä¾†è§€å¯Ÿæ¨¡åž‹è¨“ç·´çš„é€²ç¨‹å’Œæ•ˆæžœã€‚\n",
    "#random_stateæ˜¯åœ¨æ©Ÿå™¨å­¸ç¿’ä¸­çš„ä¸€å€‹åƒæ•¸ï¼Œé€šå¸¸ç”¨æ–¼åœ¨æ¨¡åž‹è¨“ç·´éŽç¨‹ä¸­ç”Ÿæˆéš¨æ©Ÿæ•¸ã€‚åœ¨æ¯æ¬¡é‹è¡Œæ¨¡åž‹æ™‚ï¼Œå¦‚æžœæ²’æœ‰æŒ‡å®š random_stateï¼Œå‰‡æ¯æ¬¡ç”Ÿæˆçš„éš¨æ©Ÿæ•¸éƒ½æœƒä¸åŒï¼Œé€™å°‡å°Žè‡´æ¯æ¬¡é‹è¡Œçµæžœçš„å·®ç•°ï¼Œä¹Ÿå°±æ˜¯ä¸ç©©å®šçš„çµæžœã€‚\n",
    "random_cv = RandomizedSearchCV(\n",
    "            estimator = regressor,\n",
    "            param_distributions = hyperparameter_grid,\n",
    "            n_iter=50,\n",
    "            n_jobs = -1,        \n",
    "            scoring = 'neg_mean_absolute_error',\n",
    "            cv=5, \n",
    "            verbose = 5, \n",
    "            return_train_score = True,\n",
    "            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                          colsample_bylevel=None,\n",
       "                                          colsample_bynode=None,\n",
       "                                          colsample_bytree=None,\n",
       "                                          enable_categorical=False, gamma=None,\n",
       "                                          gpu_id=None, importance_type=None,\n",
       "                                          interaction_constraints=None,\n",
       "                                          learning_rate=None,\n",
       "                                          max_delta_step=None, max_depth=None,\n",
       "                                          min_child_weight=None, missing=nan,\n",
       "                                          monotone_constraints=...\n",
       "                                          reg_alpha=None, reg_lambda=None,\n",
       "                                          scale_pos_weight=None, subsample=None,\n",
       "                                          tree_method=None,\n",
       "                                          validate_parameters=None,\n",
       "                                          verbosity=None),\n",
       "                   n_iter=50, n_jobs=-1,\n",
       "                   param_distributions={'learning_rate': [0.05, 0.1, 0.15, 0.2],\n",
       "                                        'max_depth': [2, 3, 5, 10],\n",
       "                                        'min_child_weight': [1, 2, 3, 4],\n",
       "                                        'n_estimators': [100, 500, 900, 1100,\n",
       "                                                         1500]},\n",
       "                   random_state=42, return_train_score=True,\n",
       "                   scoring='neg_mean_absolute_error', verbose=5)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "             gamma=0, gpu_id=-1, importance_type=None,\n",
       "             interaction_constraints='', learning_rate=0.05, max_delta_step=0,\n",
       "             max_depth=2, min_child_weight=1, missing=nan,\n",
       "             monotone_constraints='()', n_estimators=1100, n_jobs=8,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END learning_rate=0.15, max_depth=10, min_child_weight=3, n_estimators=100;, score=(train=-1202.035, test=-17522.930) total time=   7.5s\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=10, min_child_weight=3, n_estimators=100;, score=(train=-1001.237, test=-14941.810) total time=   7.3s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=2, min_child_weight=2, n_estimators=1500;, score=(train=-7532.314, test=-17140.161) total time=  16.0s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=2, min_child_weight=2, n_estimators=1500;, score=(train=-7122.422, test=-16684.830) total time=  16.3s\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=3, min_child_weight=2, n_estimators=500;, score=(train=-3500.867, test=-14104.974) total time=   7.7s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=10, min_child_weight=1, n_estimators=1100;, score=(train=-7.411, test=-17157.291) total time= 1.4min\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=10, min_child_weight=1, n_estimators=1500;, score=(train=-0.055, test=-18279.564) total time= 2.0min\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=5, min_child_weight=1, n_estimators=900;, score=(train=-21.476, test=-17280.329) total time=  29.7s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=5, min_child_weight=4, n_estimators=100;, score=(train=-8564.147, test=-16093.812) total time=   3.2s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=5, min_child_weight=4, n_estimators=100;, score=(train=-8471.700, test=-16961.328) total time=   3.2s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=5, min_child_weight=4, n_estimators=100;, score=(train=-8497.181, test=-16900.876) total time=   3.3s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=5, min_child_weight=4, n_estimators=100;, score=(train=-9134.698, test=-14511.696) total time=   3.2s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=5, min_child_weight=4, n_estimators=100;, score=(train=-8805.831, test=-18466.467) total time=   3.1s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=1500;, score=(train=-1475.928, test=-15954.732) total time=  25.5s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=1500;, score=(train=-1484.808, test=-14588.847) total time=  26.5s\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=5, min_child_weight=3, n_estimators=500;, score=(train=-311.351, test=-16895.135) total time=  18.7s\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=3, min_child_weight=4, n_estimators=1100;, score=(train=-1048.799, test=-17263.562) total time=  19.5s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=5, min_child_weight=4, n_estimators=1500;, score=(train=-29.826, test=-17415.293) total time=  45.3s\n",
      "[CV 1/5] END learning_rate=0.2, max_depth=5, min_child_weight=4, n_estimators=900;, score=(train=-14.569, test=-16786.362) total time=  27.7s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=3, min_child_weight=3, n_estimators=1100;, score=(train=-4297.250, test=-16572.840) total time=  19.7s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=10, min_child_weight=3, n_estimators=1100;, score=(train=-8.677, test=-15255.533) total time= 1.3min\n",
      "[CV 2/5] END learning_rate=0.2, max_depth=10, min_child_weight=1, n_estimators=100;, score=(train=-55.151, test=-18314.080) total time=   9.5s\n",
      "[CV 5/5] END learning_rate=0.2, max_depth=10, min_child_weight=1, n_estimators=100;, score=(train=-82.118, test=-18087.509) total time=   9.1s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=10, min_child_weight=4, n_estimators=1100;, score=(train=-50.022, test=-17947.989) total time= 1.1min\n",
      "[CV 4/5] END learning_rate=0.2, max_depth=5, min_child_weight=1, n_estimators=1500;, score=(train=-7.361, test=-15235.741) total time=  45.1s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=2, min_child_weight=1, n_estimators=1500;, score=(train=-5009.677, test=-17396.792) total time=  16.7s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=2, min_child_weight=1, n_estimators=1500;, score=(train=-4822.784, test=-16894.681) total time=  18.3s\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=3, min_child_weight=3, n_estimators=1100;, score=(train=-1204.872, test=-16220.230) total time=  19.2s\n",
      "[CV 4/5] END learning_rate=0.2, max_depth=5, min_child_weight=2, n_estimators=100;, score=(train=-3715.032, test=-14627.530) total time=   3.3s\n",
      "[CV 2/5] END learning_rate=0.2, max_depth=5, min_child_weight=1, n_estimators=500;, score=(train=-90.163, test=-17685.414) total time=  16.4s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=5, min_child_weight=2, n_estimators=500;, score=(train=-3008.656, test=-17526.433) total time=  16.9s\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=3, min_child_weight=3, n_estimators=1500;, score=(train=-579.593, test=-17391.597) total time=  25.7s\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=2, min_child_weight=4, n_estimators=900;, score=(train=-5192.056, test=-16878.419) total time=   9.8s\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=2, min_child_weight=2, n_estimators=900;, score=(train=-5049.976, test=-17068.412) total time=  12.0s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=10, min_child_weight=1, n_estimators=1100;, score=(train=-7.600, test=-16736.564) total time= 1.4min\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=10, min_child_weight=1, n_estimators=1100;, score=(train=-7.412, test=-16253.729) total time= 1.5min\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=10, min_child_weight=4, n_estimators=500;, score=(train=-557.991, test=-17926.243) total time=  31.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=2, min_child_weight=1, n_estimators=900;, score=(train=-7041.351, test=-15054.526) total time=  10.6s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=2, min_child_weight=1, n_estimators=900;, score=(train=-7281.860, test=-14395.361) total time=  11.0s\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=2, min_child_weight=3, n_estimators=900;, score=(train=-5189.843, test=-15340.547) total time=  11.2s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=3, min_child_weight=4, n_estimators=1500;, score=(train=-1362.120, test=-17022.764) total time=  26.4s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=10, min_child_weight=4, n_estimators=900;, score=(train=-26.588, test=-16026.292) total time= 1.0min\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=5, min_child_weight=1, n_estimators=1100;, score=(train=-16.862, test=-14402.110) total time=  40.0s\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=3, min_child_weight=4, n_estimators=1100;, score=(train=-1135.033, test=-18143.986) total time=  19.5s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=5, min_child_weight=1, n_estimators=900;, score=(train=-1117.787, test=-16755.168) total time=  29.2s\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=5, min_child_weight=3, n_estimators=1100;, score=(train=-17.989, test=-16902.439) total time=  34.0s\n",
      "[CV 5/5] END learning_rate=0.2, max_depth=5, min_child_weight=4, n_estimators=900;, score=(train=-10.361, test=-18203.033) total time=  27.4s\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=3, min_child_weight=1, n_estimators=1100;, score=(train=-1228.794, test=-16494.549) total time=  19.0s\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=3, min_child_weight=1, n_estimators=1100;, score=(train=-1203.414, test=-14559.874) total time=  17.9s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=10, min_child_weight=4, n_estimators=100;, score=(train=-3506.302, test=-17917.525) total time=   6.8s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=2, min_child_weight=2, n_estimators=100;, score=(train=-16426.261, test=-19649.071) total time=   1.1s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=2, min_child_weight=2, n_estimators=100;, score=(train=-17294.144, test=-18695.843) total time=   1.1s\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=2, min_child_weight=2, n_estimators=1100;, score=(train=-4516.961, test=-15424.781) total time=  12.2s\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=2, min_child_weight=2, n_estimators=1100;, score=(train=-4669.488, test=-14247.936) total time=  12.2s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=10, min_child_weight=4, n_estimators=900;, score=(train=-0.924, test=-17922.058) total time=  59.0s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=10, min_child_weight=4, n_estimators=1100;, score=(train=-27.948, test=-17984.028) total time= 1.1min\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=2, min_child_weight=1, n_estimators=1500;, score=(train=-5149.733, test=-15397.398) total time=  15.6s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=2, min_child_weight=1, n_estimators=1500;, score=(train=-5382.154, test=-14325.533) total time=  16.9s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=2, min_child_weight=2, n_estimators=900;, score=(train=-9180.794, test=-15032.036) total time=  11.2s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=2, min_child_weight=2, n_estimators=900;, score=(train=-9096.743, test=-15720.530) total time=  10.9s\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=3, min_child_weight=3, n_estimators=1100;, score=(train=-1081.471, test=-17079.024) total time=  19.6s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=5, min_child_weight=2, n_estimators=500;, score=(train=-2668.699, test=-16390.734) total time=  16.0s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=2, min_child_weight=1, n_estimators=1100;, score=(train=-8490.123, test=-16526.790) total time=  14.3s\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=10, min_child_weight=2, n_estimators=500;, score=(train=-0.045, test=-18021.018) total time=  39.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END learning_rate=0.15, max_depth=2, min_child_weight=2, n_estimators=900;, score=(train=-5198.341, test=-17319.022) total time=  11.8s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=3, min_child_weight=2, n_estimators=100;, score=(train=-13474.963, test=-16390.807) total time=   1.7s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=10, min_child_weight=1, n_estimators=1100;, score=(train=-0.250, test=-18348.557) total time= 1.5min\n",
      "[CV 3/5] END learning_rate=0.2, max_depth=2, min_child_weight=1, n_estimators=1500;, score=(train=-2474.527, test=-16827.484) total time=  17.8s\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=10, min_child_weight=1, n_estimators=1500;, score=(train=-7.364, test=-16973.006) total time= 1.8min\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=2, min_child_weight=1, n_estimators=900;, score=(train=-6694.153, test=-15348.923) total time=  11.0s\n",
      "[CV 2/5] END learning_rate=0.2, max_depth=2, min_child_weight=1, n_estimators=100;, score=(train=-11906.987, test=-18309.308) total time=   1.2s\n",
      "[CV 4/5] END learning_rate=0.2, max_depth=2, min_child_weight=1, n_estimators=100;, score=(train=-12370.715, test=-15372.725) total time=   1.2s\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=2, min_child_weight=3, n_estimators=900;, score=(train=-5200.215, test=-17310.655) total time=  11.3s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=3, min_child_weight=4, n_estimators=1500;, score=(train=-1445.939, test=-14486.935) total time=  26.2s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=10, min_child_weight=4, n_estimators=900;, score=(train=-16.688, test=-17608.241) total time= 1.1min\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=5, min_child_weight=1, n_estimators=1100;, score=(train=-11.375, test=-17353.865) total time=  39.9s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=5, min_child_weight=4, n_estimators=1500;, score=(train=-32.635, test=-16591.696) total time=  47.1s\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=5, min_child_weight=3, n_estimators=1100;, score=(train=-18.855, test=-14027.298) total time=  33.9s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=3, min_child_weight=3, n_estimators=1100;, score=(train=-4842.054, test=-15304.968) total time=  19.6s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=10, min_child_weight=3, n_estimators=1100;, score=(train=-8.275, test=-17294.454) total time= 1.4min\n",
      "[CV 3/5] END learning_rate=0.2, max_depth=10, min_child_weight=1, n_estimators=100;, score=(train=-71.993, test=-18350.123) total time=   8.8s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=10, min_child_weight=4, n_estimators=1100;, score=(train=-12.902, test=-16027.715) total time= 1.3min\n",
      "[CV 5/5] END learning_rate=0.2, max_depth=5, min_child_weight=1, n_estimators=1500;, score=(train=-0.047, test=-17430.260) total time=  45.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=2, min_child_weight=1, n_estimators=1500;, score=(train=-4823.130, test=-15564.742) total time=  17.0s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=2, min_child_weight=2, n_estimators=900;, score=(train=-9266.330, test=-17334.178) total time=  11.2s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=2, min_child_weight=2, n_estimators=900;, score=(train=-9593.061, test=-14287.290) total time=  10.7s\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=3, min_child_weight=3, n_estimators=1100;, score=(train=-1092.117, test=-17029.936) total time=  19.4s\n",
      "[CV 5/5] END learning_rate=0.2, max_depth=5, min_child_weight=1, n_estimators=500;, score=(train=-116.047, test=-17423.160) total time=  15.9s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=2, min_child_weight=1, n_estimators=1100;, score=(train=-8995.661, test=-14172.536) total time=  14.5s\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=10, min_child_weight=2, n_estimators=500;, score=(train=-7.347, test=-17209.718) total time=  40.6s\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=2, min_child_weight=2, n_estimators=900;, score=(train=-5079.194, test=-15972.787) total time=  11.9s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=3, min_child_weight=2, n_estimators=100;, score=(train=-12954.946, test=-18502.505) total time=   1.8s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=2, min_child_weight=2, n_estimators=1500;, score=(train=-7410.742, test=-14962.788) total time=  15.7s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=2, min_child_weight=2, n_estimators=1500;, score=(train=-7825.950, test=-14222.412) total time=  16.3s\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=3, min_child_weight=2, n_estimators=500;, score=(train=-3266.969, test=-16867.464) total time=   7.8s\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=3, min_child_weight=2, n_estimators=500;, score=(train=-3482.195, test=-17430.245) total time=   7.1s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=10, min_child_weight=1, n_estimators=1100;, score=(train=-0.100, test=-18035.010) total time= 1.3min\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=2, min_child_weight=3, n_estimators=100;, score=(train=-13897.080, test=-16765.742) total time=   1.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=2, min_child_weight=3, n_estimators=100;, score=(train=-13538.654, test=-18092.208) total time=   1.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=2, min_child_weight=3, n_estimators=100;, score=(train=-14205.885, test=-17229.424) total time=   1.1s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=2, min_child_weight=3, n_estimators=100;, score=(train=-14604.168, test=-16357.027) total time=   1.1s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=2, min_child_weight=3, n_estimators=100;, score=(train=-13785.441, test=-19538.312) total time=   1.4s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=10, min_child_weight=4, n_estimators=500;, score=(train=-331.929, test=-16004.746) total time=  35.9s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=10, min_child_weight=4, n_estimators=500;, score=(train=-318.201, test=-17602.050) total time=  33.1s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=10, min_child_weight=4, n_estimators=500;, score=(train=-863.413, test=-17991.629) total time=  29.4s\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=2, min_child_weight=3, n_estimators=900;, score=(train=-5080.105, test=-18083.690) total time=  11.2s\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=5, min_child_weight=1, n_estimators=900;, score=(train=-34.717, test=-17427.892) total time=  29.9s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=10, min_child_weight=4, n_estimators=900;, score=(train=-135.306, test=-17977.840) total time=  51.4s\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=5, min_child_weight=1, n_estimators=1100;, score=(train=-6.682, test=-17280.969) total time=  40.4s\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=3, min_child_weight=4, n_estimators=1100;, score=(train=-1062.758, test=-14726.117) total time=  19.5s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=5, min_child_weight=1, n_estimators=900;, score=(train=-1121.443, test=-16497.316) total time=  29.4s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=5, min_child_weight=1, n_estimators=900;, score=(train=-1398.444, test=-17044.651) total time=  28.1s\n",
      "[CV 3/5] END learning_rate=0.2, max_depth=5, min_child_weight=4, n_estimators=900;, score=(train=-8.814, test=-18010.304) total time=  27.9s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=10, min_child_weight=3, n_estimators=1100;, score=(train=-9.087, test=-16298.747) total time= 1.3min\n",
      "[CV 1/5] END learning_rate=0.2, max_depth=10, min_child_weight=1, n_estimators=100;, score=(train=-126.034, test=-17869.254) total time=   8.3s\n",
      "[CV 4/5] END learning_rate=0.2, max_depth=10, min_child_weight=1, n_estimators=100;, score=(train=-80.350, test=-15985.549) total time=   9.0s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=10, min_child_weight=4, n_estimators=1100;, score=(train=-9.753, test=-17608.656) total time= 1.4min\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=10, min_child_weight=1, n_estimators=900;, score=(train=-7.364, test=-16973.006) total time= 1.1min\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=2, min_child_weight=2, n_estimators=900;, score=(train=-8838.091, test=-16812.946) total time=  10.8s\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=3, min_child_weight=3, n_estimators=1100;, score=(train=-1096.948, test=-14543.583) total time=  19.5s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=5, min_child_weight=2, n_estimators=500;, score=(train=-2898.164, test=-15828.798) total time=  15.3s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=2, min_child_weight=1, n_estimators=1100;, score=(train=-8495.477, test=-15602.740) total time=  14.5s\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=10, min_child_weight=2, n_estimators=500;, score=(train=-7.354, test=-16238.445) total time=  41.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END learning_rate=0.15, max_depth=2, min_child_weight=2, n_estimators=900;, score=(train=-5239.100, test=-15299.845) total time=  11.7s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=3, min_child_weight=2, n_estimators=100;, score=(train=-13072.859, test=-17715.790) total time=   1.8s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=10, min_child_weight=1, n_estimators=1100;, score=(train=-7.600, test=-16417.812) total time= 1.4min\n",
      "[CV 2/5] END learning_rate=0.2, max_depth=2, min_child_weight=1, n_estimators=1500;, score=(train=-2508.543, test=-17245.287) total time=  18.1s\n",
      "[CV 5/5] END learning_rate=0.2, max_depth=2, min_child_weight=1, n_estimators=1500;, score=(train=-2632.245, test=-17143.874) total time=  17.0s\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=10, min_child_weight=1, n_estimators=1500;, score=(train=-7.363, test=-15817.981) total time= 1.7min\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=2, min_child_weight=3, n_estimators=900;, score=(train=-5315.497, test=-14338.805) total time=  11.3s\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=5, min_child_weight=1, n_estimators=900;, score=(train=-35.367, test=-16086.531) total time=  30.0s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=10, min_child_weight=4, n_estimators=900;, score=(train=-22.187, test=-14854.628) total time= 1.0min\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=5, min_child_weight=3, n_estimators=500;, score=(train=-275.810, test=-16224.813) total time=  18.5s\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=5, min_child_weight=3, n_estimators=500;, score=(train=-309.939, test=-14027.622) total time=  16.9s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=5, min_child_weight=4, n_estimators=1500;, score=(train=-33.433, test=-16440.302) total time=  46.1s\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=5, min_child_weight=3, n_estimators=1100;, score=(train=-17.901, test=-16250.455) total time=  34.1s\n",
      "[CV 4/5] END learning_rate=0.2, max_depth=5, min_child_weight=4, n_estimators=900;, score=(train=-13.470, test=-14671.667) total time=  28.5s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=10, min_child_weight=3, n_estimators=1100;, score=(train=-4.708, test=-16962.821) total time= 1.2min\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=10, min_child_weight=4, n_estimators=900;, score=(train=-0.671, test=-17194.549) total time=  58.0s\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=2, min_child_weight=4, n_estimators=500;, score=(train=-7300.519, test=-15469.799) total time=   5.9s\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=2, min_child_weight=4, n_estimators=500;, score=(train=-7642.387, test=-13965.278) total time=   6.1s\n",
      "[CV 2/5] END learning_rate=0.2, max_depth=5, min_child_weight=1, n_estimators=1500;, score=(train=-7.361, test=-17686.170) total time=  46.5s\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=10, min_child_weight=1, n_estimators=900;, score=(train=-0.067, test=-18299.950) total time= 1.0min\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=3, min_child_weight=3, n_estimators=1100;, score=(train=-1092.231, test=-17387.188) total time=  19.3s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=5, min_child_weight=2, n_estimators=500;, score=(train=-2543.983, test=-16709.752) total time=  16.0s\n",
      "[CV 1/5] END learning_rate=0.2, max_depth=3, min_child_weight=1, n_estimators=100;, score=(train=-8646.797, test=-15915.272) total time=   2.1s\n",
      "[CV 2/5] END learning_rate=0.2, max_depth=3, min_child_weight=1, n_estimators=100;, score=(train=-8465.404, test=-17371.364) total time=   1.9s\n",
      "[CV 3/5] END learning_rate=0.2, max_depth=3, min_child_weight=1, n_estimators=100;, score=(train=-8442.947, test=-17011.502) total time=   1.9s\n",
      "[CV 4/5] END learning_rate=0.2, max_depth=3, min_child_weight=1, n_estimators=100;, score=(train=-8829.005, test=-14669.194) total time=   1.9s\n",
      "[CV 5/5] END learning_rate=0.2, max_depth=3, min_child_weight=1, n_estimators=100;, score=(train=-8779.436, test=-17619.546) total time=   1.8s\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=3, min_child_weight=3, n_estimators=1500;, score=(train=-591.839, test=-14604.486) total time=  25.8s\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=2, min_child_weight=4, n_estimators=900;, score=(train=-5276.311, test=-17573.428) total time=   9.7s\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=2, min_child_weight=4, n_estimators=900;, score=(train=-5409.978, test=-14153.057) total time=   8.3s\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=10, min_child_weight=3, n_estimators=100;, score=(train=-816.223, test=-17405.221) total time=   8.4s\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=10, min_child_weight=3, n_estimators=100;, score=(train=-1066.597, test=-18010.344) total time=   6.9s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=2, min_child_weight=2, n_estimators=1500;, score=(train=-7306.924, test=-15762.316) total time=  16.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=100;, score=(train=-10745.923, test=-15708.762) total time=   1.6s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=100;, score=(train=-10559.641, test=-17103.057) total time=   1.5s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=100;, score=(train=-10944.740, test=-16632.790) total time=   1.6s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=100;, score=(train=-11172.423, test=-14863.073) total time=   1.6s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=100;, score=(train=-10733.105, test=-17843.572) total time=   1.6s\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=3, min_child_weight=2, n_estimators=500;, score=(train=-3506.345, test=-16304.533) total time=   7.6s\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=3, min_child_weight=2, n_estimators=500;, score=(train=-3374.729, test=-16693.877) total time=   7.7s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=10, min_child_weight=1, n_estimators=1100;, score=(train=-7.389, test=-17129.995) total time= 1.3min\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=10, min_child_weight=1, n_estimators=1500;, score=(train=-7.362, test=-17987.176) total time= 2.0min\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=3, min_child_weight=4, n_estimators=1500;, score=(train=-1293.501, test=-17030.067) total time=  26.7s\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=5, min_child_weight=1, n_estimators=900;, score=(train=-36.797, test=-17351.599) total time=  28.4s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=1500;, score=(train=-1419.559, test=-16224.462) total time=  25.3s\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=5, min_child_weight=1, n_estimators=1100;, score=(train=-15.991, test=-16087.787) total time=  39.2s\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=5, min_child_weight=3, n_estimators=500;, score=(train=-358.821, test=-17880.156) total time=  16.1s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=5, min_child_weight=4, n_estimators=1500;, score=(train=-32.983, test=-17201.111) total time=  45.1s\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=5, min_child_weight=3, n_estimators=1100;, score=(train=-10.128, test=-17199.714) total time=  34.2s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=3, min_child_weight=3, n_estimators=1100;, score=(train=-4550.672, test=-16415.027) total time=  19.6s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=10, min_child_weight=3, n_estimators=1100;, score=(train=-6.615, test=-17671.118) total time= 1.2min\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=10, min_child_weight=4, n_estimators=900;, score=(train=-7.377, test=-17600.608) total time= 1.1min\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=2, min_child_weight=4, n_estimators=500;, score=(train=-7446.238, test=-17729.197) total time=   6.0s\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=2, min_child_weight=4, n_estimators=500;, score=(train=-7101.386, test=-17670.075) total time=   6.0s\n",
      "[CV 3/5] END learning_rate=0.2, max_depth=5, min_child_weight=1, n_estimators=1500;, score=(train=-0.089, test=-17672.863) total time=  45.6s\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=10, min_child_weight=1, n_estimators=900;, score=(train=-7.363, test=-15817.948) total time= 1.1min\n",
      "[CV 1/5] END learning_rate=0.2, max_depth=5, min_child_weight=2, n_estimators=100;, score=(train=-3646.749, test=-16373.658) total time=   3.3s\n",
      "[CV 3/5] END learning_rate=0.2, max_depth=5, min_child_weight=2, n_estimators=100;, score=(train=-3325.291, test=-17204.087) total time=   3.3s\n",
      "[CV 1/5] END learning_rate=0.2, max_depth=5, min_child_weight=1, n_estimators=500;, score=(train=-100.729, test=-16699.963) total time=  16.2s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=5, min_child_weight=2, n_estimators=500;, score=(train=-2879.358, test=-14312.885) total time=  16.9s\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=3, min_child_weight=3, n_estimators=1500;, score=(train=-562.150, test=-17164.800) total time=  25.8s\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=2, min_child_weight=4, n_estimators=900;, score=(train=-5248.308, test=-15756.373) total time=  10.0s\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=2, min_child_weight=4, n_estimators=900;, score=(train=-5110.609, test=-17902.398) total time=   8.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END learning_rate=0.15, max_depth=2, min_child_weight=2, n_estimators=900;, score=(train=-5488.352, test=-14121.847) total time=  11.9s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=10, min_child_weight=1, n_estimators=1100;, score=(train=-7.661, test=-16702.500) total time= 1.5min\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=10, min_child_weight=1, n_estimators=1100;, score=(train=-0.098, test=-18300.986) total time= 1.6min\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=10, min_child_weight=4, n_estimators=500;, score=(train=-317.533, test=-14861.298) total time=  33.3s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=2, min_child_weight=1, n_estimators=900;, score=(train=-6918.504, test=-17386.043) total time=  11.0s\n",
      "[CV 1/5] END learning_rate=0.2, max_depth=2, min_child_weight=1, n_estimators=100;, score=(train=-12020.745, test=-15815.231) total time=   1.2s\n",
      "[CV 3/5] END learning_rate=0.2, max_depth=2, min_child_weight=1, n_estimators=100;, score=(train=-12107.839, test=-16038.627) total time=   1.2s\n",
      "[CV 5/5] END learning_rate=0.2, max_depth=2, min_child_weight=1, n_estimators=100;, score=(train=-11930.557, test=-17075.159) total time=   1.2s\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=2, min_child_weight=3, n_estimators=900;, score=(train=-5036.208, test=-16255.129) total time=  11.2s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=3, min_child_weight=4, n_estimators=1500;, score=(train=-1407.211, test=-18264.501) total time=  26.5s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=10, min_child_weight=4, n_estimators=900;, score=(train=-126.640, test=-17949.182) total time=  52.0s\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=5, min_child_weight=1, n_estimators=1100;, score=(train=-16.565, test=-17427.977) total time=  39.9s\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=3, min_child_weight=4, n_estimators=1100;, score=(train=-1183.240, test=-16751.434) total time=  19.3s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=5, min_child_weight=4, n_estimators=1500;, score=(train=-33.082, test=-14315.747) total time=  45.8s\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=5, min_child_weight=3, n_estimators=1100;, score=(train=-12.327, test=-17877.237) total time=  34.5s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=3, min_child_weight=3, n_estimators=1100;, score=(train=-4906.680, test=-13814.718) total time=  18.9s\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=3, min_child_weight=1, n_estimators=1100;, score=(train=-1110.401, test=-16966.991) total time=  19.1s\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=3, min_child_weight=1, n_estimators=1100;, score=(train=-1186.643, test=-16726.044) total time=  18.0s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=10, min_child_weight=4, n_estimators=100;, score=(train=-3933.891, test=-14933.313) total time=   6.5s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=2, min_child_weight=2, n_estimators=100;, score=(train=-16928.655, test=-18674.184) total time=   1.1s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=2, min_child_weight=2, n_estimators=100;, score=(train=-16774.308, test=-19038.218) total time=   1.1s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=2, min_child_weight=2, n_estimators=100;, score=(train=-16295.360, test=-20546.195) total time=   1.1s\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=2, min_child_weight=2, n_estimators=1100;, score=(train=-4523.979, test=-17380.217) total time=  12.2s\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=2, min_child_weight=2, n_estimators=1100;, score=(train=-4364.606, test=-17177.248) total time=  12.3s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=10, min_child_weight=4, n_estimators=900;, score=(train=-7.472, test=-15221.221) total time= 1.0min\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=2, min_child_weight=4, n_estimators=500;, score=(train=-7379.837, test=-16729.689) total time=   5.9s\n",
      "[CV 1/5] END learning_rate=0.2, max_depth=5, min_child_weight=1, n_estimators=1500;, score=(train=-7.374, test=-16699.581) total time=  46.6s\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=10, min_child_weight=1, n_estimators=900;, score=(train=-7.362, test=-17987.225) total time= 1.2min\n",
      "[CV 2/5] END learning_rate=0.2, max_depth=5, min_child_weight=2, n_estimators=100;, score=(train=-3189.887, test=-17261.884) total time=   3.4s\n",
      "[CV 5/5] END learning_rate=0.2, max_depth=5, min_child_weight=2, n_estimators=100;, score=(train=-3692.572, test=-17901.644) total time=   3.3s\n",
      "[CV 3/5] END learning_rate=0.2, max_depth=5, min_child_weight=1, n_estimators=500;, score=(train=-90.321, test=-17668.520) total time=  16.2s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=2, min_child_weight=1, n_estimators=1100;, score=(train=-8640.312, test=-17275.414) total time=  14.0s\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=3, min_child_weight=3, n_estimators=1500;, score=(train=-580.117, test=-17078.527) total time=  26.1s\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=10, min_child_weight=2, n_estimators=500;, score=(train=-0.049, test=-18065.191) total time=  23.3s\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=10, min_child_weight=3, n_estimators=100;, score=(train=-713.561, test=-16371.223) total time=   8.5s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=3, min_child_weight=2, n_estimators=100;, score=(train=-13181.554, test=-16611.921) total time=   1.8s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=3, min_child_weight=2, n_estimators=100;, score=(train=-12754.216, test=-17610.883) total time=   2.0s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=10, min_child_weight=1, n_estimators=1100;, score=(train=-0.266, test=-17844.514) total time= 1.5min\n",
      "[CV 1/5] END learning_rate=0.2, max_depth=2, min_child_weight=1, n_estimators=1500;, score=(train=-2612.597, test=-15510.958) total time=  18.5s\n",
      "[CV 4/5] END learning_rate=0.2, max_depth=2, min_child_weight=1, n_estimators=1500;, score=(train=-2687.045, test=-15346.499) total time=  17.0s\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=10, min_child_weight=1, n_estimators=1500;, score=(train=-0.067, test=-18299.950) total time= 1.7min\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=2, min_child_weight=1, n_estimators=900;, score=(train=-6490.532, test=-16666.430) total time=  11.1s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=3, min_child_weight=4, n_estimators=1500;, score=(train=-1441.374, test=-16020.050) total time=  26.6s\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=5, min_child_weight=1, n_estimators=900;, score=(train=-39.330, test=-14402.041) total time=  28.4s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=1500;, score=(train=-1393.138, test=-16625.128) total time=  25.5s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=1500;, score=(train=-1495.792, test=-16172.876) total time=  26.6s\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=5, min_child_weight=3, n_estimators=500;, score=(train=-298.392, test=-17176.123) total time=  18.7s\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=3, min_child_weight=4, n_estimators=1100;, score=(train=-1141.497, test=-17535.593) total time=  19.4s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=5, min_child_weight=1, n_estimators=900;, score=(train=-1191.347, test=-15508.722) total time=  29.6s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=5, min_child_weight=1, n_estimators=900;, score=(train=-1244.436, test=-14353.616) total time=  29.1s\n",
      "[CV 2/5] END learning_rate=0.2, max_depth=5, min_child_weight=4, n_estimators=900;, score=(train=-14.010, test=-17396.091) total time=  28.3s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=3, min_child_weight=3, n_estimators=1100;, score=(train=-4496.537, test=-17253.276) total time=  18.7s\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=3, min_child_weight=1, n_estimators=1100;, score=(train=-1090.845, test=-16911.968) total time=  18.8s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=10, min_child_weight=4, n_estimators=100;, score=(train=-3750.480, test=-16200.303) total time=   7.2s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=10, min_child_weight=4, n_estimators=100;, score=(train=-3811.217, test=-17809.948) total time=   6.8s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=10, min_child_weight=4, n_estimators=100;, score=(train=-4026.900, test=-18597.180) total time=   6.5s\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=2, min_child_weight=2, n_estimators=1100;, score=(train=-4362.242, test=-16040.754) total time=  12.3s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=10, min_child_weight=4, n_estimators=900;, score=(train=-7.698, test=-16476.390) total time= 1.0min\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=10, min_child_weight=4, n_estimators=1100;, score=(train=-12.051, test=-14854.276) total time= 1.2min\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=10, min_child_weight=1, n_estimators=900;, score=(train=-0.055, test=-18279.531) total time= 1.2min\n",
      "[CV 4/5] END learning_rate=0.2, max_depth=5, min_child_weight=1, n_estimators=500;, score=(train=-111.449, test=-15229.787) total time=  16.0s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=2, min_child_weight=1, n_estimators=1100;, score=(train=-8678.981, test=-14920.133) total time=  14.0s\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=3, min_child_weight=3, n_estimators=1500;, score=(train=-641.843, test=-16233.083) total time=  25.9s\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=10, min_child_weight=2, n_estimators=500;, score=(train=-7.340, test=-16473.372) total time=  23.7s\n"
     ]
    }
   ],
   "source": [
    "random_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=xgboost.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
    "             gamma=0, gpu_id=-1, importance_type=None,\n",
    "             interaction_constraints='', learning_rate=0.05, max_delta_step=0,\n",
    "             max_depth=2, min_child_weight=1, missing=np.nan,\n",
    "             monotone_constraints='()', n_estimators=1100, n_jobs=8,\n",
    "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
    "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
    "             validate_parameters=1, verbosity=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "             gamma=0, gpu_id=-1, importance_type=None,\n",
       "             interaction_constraints='', learning_rate=0.05, max_delta_step=0,\n",
       "             max_depth=2, min_child_weight=1, missing=nan,\n",
       "             monotone_constraints='()', n_estimators=1100, n_jobs=8,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'finalized_model.pkl'\n",
    "pickle.dump(regressor, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(df_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([117127.42, 163534.92, 186550.88, ..., 175709.97, 123794.81,\n",
       "       236662.38], dtype=float32)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Create Sample Submission file and Submit using ANN\n",
    "pred=pd.DataFrame(y_pred)\n",
    "sub_df=pd.read_csv('formulatedtest.csv')\n",
    "datasets=pd.concat([sub_df['Id'],pred],axis=1)\n",
    "datasets.columns=['Id','SalePrice']\n",
    "datasets.to_csv('formulatedtest.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.columns=['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df=df_Train['SalePrice'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.column=['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train.drop(['SalePrice'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train=pd.concat([df_Train,temp_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>...</th>\n",
       "      <th>Min2</th>\n",
       "      <th>Typ</th>\n",
       "      <th>Attchd</th>\n",
       "      <th>Basment</th>\n",
       "      <th>BuiltIn</th>\n",
       "      <th>CarPort</th>\n",
       "      <th>Detchd</th>\n",
       "      <th>RFn</th>\n",
       "      <th>P</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>468.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>121033.398438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1329</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>923.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>406.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>155717.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>928</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>791.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>185616.859375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>926</td>\n",
       "      <td>678</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>602.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>189161.546875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1280</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1017.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>175323.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 175 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1stFlrSF  2ndFlrSF  3SsnPorch  BedroomAbvGr  BsmtFinSF1  BsmtFinSF2  \\\n",
       "0       896         0          0             2       468.0       144.0   \n",
       "1      1329         0          0             3       923.0         0.0   \n",
       "2       928       701          0             3       791.0         0.0   \n",
       "3       926       678          0             3       602.0         0.0   \n",
       "4      1280         0          0             2       263.0         0.0   \n",
       "\n",
       "   BsmtFullBath  BsmtHalfBath  BsmtUnfSF  EnclosedPorch      ...        Min2  \\\n",
       "0           0.0           0.0      270.0              0      ...           0   \n",
       "1           0.0           0.0      406.0              0      ...           0   \n",
       "2           0.0           0.0      137.0              0      ...           0   \n",
       "3           0.0           0.0      324.0              0      ...           0   \n",
       "4           0.0           0.0     1017.0              0      ...           0   \n",
       "\n",
       "   Typ  Attchd  Basment  BuiltIn  CarPort  Detchd  RFn  P      SalePrice  \n",
       "0    1       1        0        0        0       0    0  0  121033.398438  \n",
       "1    1       1        0        0        0       0    0  0  155717.390625  \n",
       "2    1       1        0        0        0       0    0  0  185616.859375  \n",
       "3    1       1        0        0        0       0    0  0  189161.546875  \n",
       "4    1       1        0        0        0       0    1  0  175323.750000  \n",
       "\n",
       "[5 rows x 175 columns]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Test=pd.concat([df_Test,pred],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 175)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train=pd.concat([df_Train,df_Test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2881, 175)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=df_Train.drop(['SalePrice'],axis=1)\n",
    "y_train=df_Train['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish.naik\\AppData\\Local\\Continuum\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=174, units=50, kernel_initializer=\"he_uniform\")`\n",
      "  del sys.path[0]\n",
      "C:\\Users\\krish.naik\\AppData\\Local\\Continuum\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=25, kernel_initializer=\"he_uniform\")`\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\krish.naik\\AppData\\Local\\Continuum\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=50, kernel_initializer=\"he_uniform\")`\n",
      "C:\\Users\\krish.naik\\AppData\\Local\\Continuum\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1, kernel_initializer=\"he_uniform\")`\n",
      "C:\\Users\\krish.naik\\AppData\\Local\\Continuum\\anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2304 samples, validate on 577 samples\n",
      "Epoch 1/1000\n",
      "2304/2304 [==============================] - 2s 1ms/step - loss: 113530.5093 - val_loss: 56624.8765\n",
      "Epoch 2/1000\n",
      "2304/2304 [==============================] - 1s 447us/step - loss: 62615.1298 - val_loss: 50444.1900\n",
      "Epoch 3/1000\n",
      "2304/2304 [==============================] - 1s 464us/step - loss: 56279.5739 - val_loss: 44504.8296\n",
      "Epoch 4/1000\n",
      "2304/2304 [==============================] - 1s 455us/step - loss: 50261.0310 - val_loss: 39335.5723\n",
      "Epoch 5/1000\n",
      "2304/2304 [==============================] - 1s 440us/step - loss: 44449.5163 - val_loss: 35396.5539\n",
      "Epoch 6/1000\n",
      "2304/2304 [==============================] - 1s 461us/step - loss: 40090.4315 - val_loss: 35178.2237\n",
      "Epoch 7/1000\n",
      "2304/2304 [==============================] - 1s 461us/step - loss: 37493.4126 - val_loss: 31983.3301\n",
      "Epoch 8/1000\n",
      "2304/2304 [==============================] - 1s 433us/step - loss: 36462.1401 - val_loss: 34241.1639\n",
      "Epoch 9/1000\n",
      "2304/2304 [==============================] - 1s 451us/step - loss: 35635.4539 - val_loss: 32087.6040\n",
      "Epoch 10/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 35851.5885 - val_loss: 32125.1113\n",
      "Epoch 11/1000\n",
      "2304/2304 [==============================] - 1s 463us/step - loss: 35622.6530 - val_loss: 31914.6602\n",
      "Epoch 12/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 35187.7144 - val_loss: 31882.3983\n",
      "Epoch 13/1000\n",
      "2304/2304 [==============================] - 1s 427us/step - loss: 35196.1213 - val_loss: 32342.4395\n",
      "Epoch 14/1000\n",
      "2304/2304 [==============================] - 1s 424us/step - loss: 34929.6660 - val_loss: 31565.0875\n",
      "Epoch 15/1000\n",
      "2304/2304 [==============================] - 1s 429us/step - loss: 34672.9052 - val_loss: 32177.3833\n",
      "Epoch 16/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 34501.2563 - val_loss: 31293.5959\n",
      "Epoch 17/1000\n",
      "2304/2304 [==============================] - 1s 459us/step - loss: 34804.2316 - val_loss: 31182.3204\n",
      "Epoch 18/1000\n",
      "2304/2304 [==============================] - 2s 778us/step - loss: 34455.4584 - val_loss: 31376.9020\n",
      "Epoch 19/1000\n",
      "2304/2304 [==============================] - 1s 467us/step - loss: 34377.8071 - val_loss: 31301.4047\n",
      "Epoch 20/1000\n",
      "2304/2304 [==============================] - 1s 477us/step - loss: 34164.2543 - val_loss: 31142.9020\n",
      "Epoch 21/1000\n",
      "2304/2304 [==============================] - 1s 469us/step - loss: 34009.0316 - val_loss: 31243.3016\n",
      "Epoch 22/1000\n",
      "2304/2304 [==============================] - 1s 422us/step - loss: 33660.4401 - val_loss: 31494.8445\n",
      "Epoch 23/1000\n",
      "2304/2304 [==============================] - 1s 413us/step - loss: 33812.4372 - val_loss: 31299.1932\n",
      "Epoch 24/1000\n",
      "2304/2304 [==============================] - 1s 419us/step - loss: 33589.9288 - val_loss: 31848.2731\n",
      "Epoch 25/1000\n",
      "2304/2304 [==============================] - 1s 432us/step - loss: 33607.1809 - val_loss: 30390.6748\n",
      "Epoch 26/1000\n",
      "2304/2304 [==============================] - 1s 426us/step - loss: 33473.9579 - val_loss: 30910.1593\n",
      "Epoch 27/1000\n",
      "2304/2304 [==============================] - 1s 433us/step - loss: 33236.2349 - val_loss: 30219.5441\n",
      "Epoch 28/1000\n",
      "2304/2304 [==============================] - 1s 420us/step - loss: 33243.2710 - val_loss: 30179.0993\n",
      "Epoch 29/1000\n",
      "2304/2304 [==============================] - 1s 430us/step - loss: 32929.2295 - val_loss: 30128.3545\n",
      "Epoch 30/1000\n",
      "2304/2304 [==============================] - 1s 420us/step - loss: 32963.0511 - val_loss: 30510.0357\n",
      "Epoch 31/1000\n",
      "2304/2304 [==============================] - 1s 418us/step - loss: 32787.1626 - val_loss: 29853.5188\n",
      "Epoch 32/1000\n",
      "2304/2304 [==============================] - 1s 427us/step - loss: 32766.8486 - val_loss: 30704.6890\n",
      "Epoch 33/1000\n",
      "2304/2304 [==============================] - 1s 425us/step - loss: 32790.5330 - val_loss: 30381.5674\n",
      "Epoch 34/1000\n",
      "2304/2304 [==============================] - 1s 436us/step - loss: 32740.6885 - val_loss: 29614.6660\n",
      "Epoch 35/1000\n",
      "2304/2304 [==============================] - 1s 420us/step - loss: 32658.3221 - val_loss: 30323.1913\n",
      "Epoch 36/1000\n",
      "2304/2304 [==============================] - 1s 416us/step - loss: 32570.0888 - val_loss: 30407.4534\n",
      "Epoch 37/1000\n",
      "2304/2304 [==============================] - 1s 423us/step - loss: 32189.7531 - val_loss: 30379.3509\n",
      "Epoch 38/1000\n",
      "2304/2304 [==============================] - 1s 432us/step - loss: 32140.4911 - val_loss: 29347.5356\n",
      "Epoch 39/1000\n",
      "2304/2304 [==============================] - 1s 427us/step - loss: 31913.3835 - val_loss: 29861.8741\n",
      "Epoch 40/1000\n",
      "2304/2304 [==============================] - 1s 421us/step - loss: 32135.0634 - val_loss: 29108.2475\n",
      "Epoch 41/1000\n",
      "2304/2304 [==============================] - 1s 423us/step - loss: 32026.9856 - val_loss: 29142.3169\n",
      "Epoch 42/1000\n",
      "2304/2304 [==============================] - 1s 425us/step - loss: 31792.7488 - val_loss: 29323.2182\n",
      "Epoch 43/1000\n",
      "2304/2304 [==============================] - 1s 422us/step - loss: 31622.7127 - val_loss: 29028.4757\n",
      "Epoch 44/1000\n",
      "2304/2304 [==============================] - 1s 426us/step - loss: 31769.7568 - val_loss: 29493.1864\n",
      "Epoch 45/1000\n",
      "2304/2304 [==============================] - 1s 433us/step - loss: 31732.7656 - val_loss: 29812.4935\n",
      "Epoch 46/1000\n",
      "2304/2304 [==============================] - 1s 416us/step - loss: 31434.8387 - val_loss: 28903.6488\n",
      "Epoch 47/1000\n",
      "2304/2304 [==============================] - 1s 414us/step - loss: 31234.3539 - val_loss: 28895.0965\n",
      "Epoch 48/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 31151.4855 - val_loss: 28861.3638\n",
      "Epoch 49/1000\n",
      "2304/2304 [==============================] - 1s 444us/step - loss: 31274.5573 - val_loss: 29427.3137\n",
      "Epoch 50/1000\n",
      "2304/2304 [==============================] - 1s 436us/step - loss: 31510.8220 - val_loss: 28567.3384\n",
      "Epoch 51/1000\n",
      "2304/2304 [==============================] - 1s 423us/step - loss: 31460.1977 - val_loss: 28814.4977\n",
      "Epoch 52/1000\n",
      "2304/2304 [==============================] - 1s 415us/step - loss: 31085.8429 - val_loss: 28524.2395\n",
      "Epoch 53/1000\n",
      "2304/2304 [==============================] - 1s 426us/step - loss: 30989.9977 - val_loss: 28568.5788\n",
      "Epoch 54/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 31092.4584 - val_loss: 28378.7455\n",
      "Epoch 55/1000\n",
      "2304/2304 [==============================] - 1s 433us/step - loss: 30932.9268 - val_loss: 30094.5473\n",
      "Epoch 56/1000\n",
      "2304/2304 [==============================] - 1s 430us/step - loss: 30684.4676 - val_loss: 28414.4087\n",
      "Epoch 57/1000\n",
      "2304/2304 [==============================] - 1s 422us/step - loss: 30493.6564 - val_loss: 29508.0253\n",
      "Epoch 58/1000\n",
      "2304/2304 [==============================] - 1s 429us/step - loss: 30652.6024 - val_loss: 28326.5143\n",
      "Epoch 59/1000\n",
      "2304/2304 [==============================] - 1s 426us/step - loss: 30778.1669 - val_loss: 28056.4234\n",
      "Epoch 60/1000\n",
      "2304/2304 [==============================] - 1s 424us/step - loss: 30424.7307 - val_loss: 28010.0378\n",
      "Epoch 61/1000\n",
      "2304/2304 [==============================] - 1s 425us/step - loss: 30072.8579 - val_loss: 28027.6187\n",
      "Epoch 62/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 30334.3219 - val_loss: 28459.9857\n",
      "Epoch 63/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 30427.8750 - val_loss: 29863.5684\n",
      "Epoch 64/1000\n",
      "2304/2304 [==============================] - 1s 430us/step - loss: 30285.2018 - val_loss: 28957.1549\n",
      "Epoch 65/1000\n",
      "2304/2304 [==============================] - 1s 417us/step - loss: 30310.4877 - val_loss: 28183.0357\n",
      "Epoch 66/1000\n",
      "2304/2304 [==============================] - 1s 405us/step - loss: 30276.2501 - val_loss: 27665.3018\n",
      "Epoch 67/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 29832.1718 - val_loss: 27712.5372\n",
      "Epoch 68/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 30272.4041 - val_loss: 27718.7824\n",
      "Epoch 69/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 30113.7119 - val_loss: 28000.3839\n",
      "Epoch 70/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 29942.6214 - val_loss: 27786.2428\n",
      "Epoch 71/1000\n",
      "2304/2304 [==============================] - 1s 449us/step - loss: 29859.0674 - val_loss: 27215.5401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/1000\n",
      "2304/2304 [==============================] - 1s 471us/step - loss: 29209.5491 - val_loss: 27173.5760\n",
      "Epoch 73/1000\n",
      "2304/2304 [==============================] - 1s 457us/step - loss: 29919.4843 - val_loss: 27220.3691\n",
      "Epoch 74/1000\n",
      "2304/2304 [==============================] - 1s 450us/step - loss: 29509.6134 - val_loss: 27340.0882\n",
      "Epoch 75/1000\n",
      "2304/2304 [==============================] - 1s 434us/step - loss: 29708.4845 - val_loss: 27312.6990\n",
      "Epoch 76/1000\n",
      "2304/2304 [==============================] - 1s 473us/step - loss: 29519.9725 - val_loss: 27508.6494\n",
      "Epoch 77/1000\n",
      "2304/2304 [==============================] - 1s 471us/step - loss: 29357.4566 - val_loss: 26867.6287\n",
      "Epoch 78/1000\n",
      "2304/2304 [==============================] - 1s 461us/step - loss: 29159.6736 - val_loss: 26893.8640\n",
      "Epoch 79/1000\n",
      "2304/2304 [==============================] - 1s 452us/step - loss: 29366.9112 - val_loss: 26603.1912\n",
      "Epoch 80/1000\n",
      "2304/2304 [==============================] - 1s 451us/step - loss: 29120.4931 - val_loss: 26661.9235\n",
      "Epoch 81/1000\n",
      "2304/2304 [==============================] - 1s 466us/step - loss: 28946.7935 - val_loss: 27871.8099\n",
      "Epoch 82/1000\n",
      "2304/2304 [==============================] - 1s 453us/step - loss: 29138.3855 - val_loss: 26531.3914\n",
      "Epoch 83/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 28846.1910 - val_loss: 28481.5947\n",
      "Epoch 84/1000\n",
      "2304/2304 [==============================] - 1s 461us/step - loss: 29017.1691 - val_loss: 26508.0993\n",
      "Epoch 85/1000\n",
      "2304/2304 [==============================] - 1s 458us/step - loss: 28775.6919 - val_loss: 26779.5843\n",
      "Epoch 86/1000\n",
      "2304/2304 [==============================] - 1s 459us/step - loss: 29089.2547 - val_loss: 27172.3217\n",
      "Epoch 87/1000\n",
      "2304/2304 [==============================] - 1s 471us/step - loss: 28686.1871 - val_loss: 26091.4350\n",
      "Epoch 88/1000\n",
      "2304/2304 [==============================] - 1s 461us/step - loss: 28698.4660 - val_loss: 26102.4028\n",
      "Epoch 89/1000\n",
      "2304/2304 [==============================] - 1s 447us/step - loss: 28699.3697 - val_loss: 26289.4353\n",
      "Epoch 90/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 28489.5871 - val_loss: 27897.5505\n",
      "Epoch 91/1000\n",
      "2304/2304 [==============================] - 1s 467us/step - loss: 28665.1914 - val_loss: 25854.9589\n",
      "Epoch 92/1000\n",
      "2304/2304 [==============================] - 1s 465us/step - loss: 28285.7090 - val_loss: 25977.9663\n",
      "Epoch 93/1000\n",
      "2304/2304 [==============================] - 1s 462us/step - loss: 28285.7983 - val_loss: 25438.4628\n",
      "Epoch 94/1000\n",
      "2304/2304 [==============================] - 1s 461us/step - loss: 28321.3720 - val_loss: 25585.6818\n",
      "Epoch 95/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 28190.3902 - val_loss: 25334.2817\n",
      "Epoch 96/1000\n",
      "2304/2304 [==============================] - 1s 449us/step - loss: 28360.6425 - val_loss: 25095.3392\n",
      "Epoch 97/1000\n",
      "2304/2304 [==============================] - 1s 468us/step - loss: 28196.4608 - val_loss: 24977.6499\n",
      "Epoch 98/1000\n",
      "2304/2304 [==============================] - 1s 460us/step - loss: 28143.1103 - val_loss: 24957.5144\n",
      "Epoch 99/1000\n",
      "2304/2304 [==============================] - ETA: 0s - loss: 28541.725 - 1s 465us/step - loss: 28404.1788 - val_loss: 25377.6330\n",
      "Epoch 100/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 27923.1122 - val_loss: 25035.9833\n",
      "Epoch 101/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 28224.6294 - val_loss: 24864.9644\n",
      "Epoch 102/1000\n",
      "2304/2304 [==============================] - 1s 450us/step - loss: 27838.1132 - val_loss: 26435.8536\n",
      "Epoch 103/1000\n",
      "2304/2304 [==============================] - 1s 457us/step - loss: 27936.2079 - val_loss: 24885.9012\n",
      "Epoch 104/1000\n",
      "2304/2304 [==============================] - ETA: 0s - loss: 27583.212 - 1s 450us/step - loss: 27619.0787 - val_loss: 24613.4044\n",
      "Epoch 105/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 27628.1560 - val_loss: 24228.4823\n",
      "Epoch 106/1000\n",
      "2304/2304 [==============================] - 1s 463us/step - loss: 27878.6943 - val_loss: 24845.3175\n",
      "Epoch 107/1000\n",
      "2304/2304 [==============================] - 1s 465us/step - loss: 27493.7438 - val_loss: 24612.2324\n",
      "Epoch 108/1000\n",
      "2304/2304 [==============================] - 1s 449us/step - loss: 27134.2120 - val_loss: 25914.3079\n",
      "Epoch 109/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 27448.0619 - val_loss: 23640.7174\n",
      "Epoch 110/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 27225.6602 - val_loss: 24300.4790\n",
      "Epoch 111/1000\n",
      "2304/2304 [==============================] - 1s 452us/step - loss: 27457.5783 - val_loss: 23591.0836\n",
      "Epoch 112/1000\n",
      "2304/2304 [==============================] - 1s 450us/step - loss: 27562.5854 - val_loss: 23601.4916\n",
      "Epoch 113/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 26970.8130 - val_loss: 23496.5879\n",
      "Epoch 114/1000\n",
      "2304/2304 [==============================] - 1s 456us/step - loss: 27141.8372 - val_loss: 24716.6597\n",
      "Epoch 115/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 26687.5030 - val_loss: 25936.5065\n",
      "Epoch 116/1000\n",
      "2304/2304 [==============================] - 1s 458us/step - loss: 26811.0217 - val_loss: 23067.7963\n",
      "Epoch 117/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 26862.9685 - val_loss: 23789.6916\n",
      "Epoch 118/1000\n",
      "2304/2304 [==============================] - 1s 459us/step - loss: 26478.2679 - val_loss: 24595.6749\n",
      "Epoch 119/1000\n",
      "2304/2304 [==============================] - 1s 464us/step - loss: 26766.5188 - val_loss: 23377.8840\n",
      "Epoch 120/1000\n",
      "2304/2304 [==============================] - 1s 461us/step - loss: 26545.6570 - val_loss: 22453.2940\n",
      "Epoch 121/1000\n",
      "2304/2304 [==============================] - 1s 467us/step - loss: 26442.3213 - val_loss: 22567.1628\n",
      "Epoch 122/1000\n",
      "2304/2304 [==============================] - 1s 465us/step - loss: 26011.6722 - val_loss: 24087.4123\n",
      "Epoch 123/1000\n",
      "2304/2304 [==============================] - 1s 451us/step - loss: 26506.6031 - val_loss: 22453.8540\n",
      "Epoch 124/1000\n",
      "2304/2304 [==============================] - 1s 454us/step - loss: 26042.1024 - val_loss: 22348.5490\n",
      "Epoch 125/1000\n",
      "2304/2304 [==============================] - 1s 466us/step - loss: 26685.7815 - val_loss: 21968.0753\n",
      "Epoch 126/1000\n",
      "2304/2304 [==============================] - 1s 454us/step - loss: 25654.7254 - val_loss: 22398.2175\n",
      "Epoch 127/1000\n",
      "2304/2304 [==============================] - 1s 468us/step - loss: 25909.7507 - val_loss: 21961.1548\n",
      "Epoch 128/1000\n",
      "2304/2304 [==============================] - 1s 480us/step - loss: 25938.8809 - val_loss: 21437.6150\n",
      "Epoch 129/1000\n",
      "2304/2304 [==============================] - 1s 476us/step - loss: 25268.5747 - val_loss: 21907.7049\n",
      "Epoch 130/1000\n",
      "2304/2304 [==============================] - 1s 470us/step - loss: 25609.2470 - val_loss: 21246.5780\n",
      "Epoch 131/1000\n",
      "2304/2304 [==============================] - 1s 465us/step - loss: 25023.4624 - val_loss: 24637.6923\n",
      "Epoch 132/1000\n",
      "2304/2304 [==============================] - 1s 520us/step - loss: 25366.8912 - val_loss: 21211.1705\n",
      "Epoch 133/1000\n",
      "2304/2304 [==============================] - 1s 479us/step - loss: 25424.9557 - val_loss: 21021.8191\n",
      "Epoch 134/1000\n",
      "2304/2304 [==============================] - 1s 464us/step - loss: 25343.5929 - val_loss: 21586.6650\n",
      "Epoch 135/1000\n",
      "2304/2304 [==============================] - 1s 462us/step - loss: 24995.2175 - val_loss: 21105.6569\n",
      "Epoch 136/1000\n",
      "2304/2304 [==============================] - 1s 465us/step - loss: 24986.7363 - val_loss: 20605.5033\n",
      "Epoch 137/1000\n",
      "2304/2304 [==============================] - 1s 449us/step - loss: 25225.4030 - val_loss: 20552.5941\n",
      "Epoch 138/1000\n",
      "2304/2304 [==============================] - 1s 452us/step - loss: 25014.9195 - val_loss: 21813.1205\n",
      "Epoch 139/1000\n",
      "2304/2304 [==============================] - 1s 452us/step - loss: 24983.0982 - val_loss: 20684.3204\n",
      "Epoch 140/1000\n",
      "2304/2304 [==============================] - 1s 464us/step - loss: 24864.2334 - val_loss: 20245.8430\n",
      "Epoch 141/1000\n",
      "2304/2304 [==============================] - 1s 466us/step - loss: 24915.6213 - val_loss: 19954.3808\n",
      "Epoch 142/1000\n",
      "2304/2304 [==============================] - 1s 451us/step - loss: 24832.0423 - val_loss: 22799.6597\n",
      "Epoch 143/1000\n",
      "2304/2304 [==============================] - 1s 434us/step - loss: 24758.2035 - val_loss: 20148.5544\n",
      "Epoch 144/1000\n",
      "2304/2304 [==============================] - 1s 461us/step - loss: 24726.4656 - val_loss: 19816.7254\n",
      "Epoch 145/1000\n",
      "2304/2304 [==============================] - 1s 449us/step - loss: 24342.4105 - val_loss: 21506.8392\n",
      "Epoch 146/1000\n",
      "2304/2304 [==============================] - 1s 494us/step - loss: 24704.4509 - val_loss: 19672.9874\n",
      "Epoch 147/1000\n",
      "2304/2304 [==============================] - 1s 483us/step - loss: 24873.8309 - val_loss: 19765.3019\n",
      "Epoch 148/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 24126.7666 - val_loss: 21504.0878\n",
      "Epoch 149/1000\n",
      "2304/2304 [==============================] - 1s 449us/step - loss: 24424.1095 - val_loss: 21462.0763\n",
      "Epoch 150/1000\n",
      "2304/2304 [==============================] - 1s 450us/step - loss: 24470.7238 - val_loss: 19617.8413\n",
      "Epoch 151/1000\n",
      "2304/2304 [==============================] - 1s 468us/step - loss: 24235.2767 - val_loss: 20478.6877\n",
      "Epoch 152/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 23886.3026 - val_loss: 19679.8546\n",
      "Epoch 153/1000\n",
      "2304/2304 [==============================] - 1s 447us/step - loss: 24203.4083 - val_loss: 19225.7126\n",
      "Epoch 154/1000\n",
      "2304/2304 [==============================] - 1s 459us/step - loss: 24125.6937 - val_loss: 18712.8627\n",
      "Epoch 155/1000\n",
      "2304/2304 [==============================] - 1s 457us/step - loss: 23699.3783 - val_loss: 19005.6716\n",
      "Epoch 156/1000\n",
      "2304/2304 [==============================] - 1s 464us/step - loss: 23628.8058 - val_loss: 20574.2034\n",
      "Epoch 157/1000\n",
      "2304/2304 [==============================] - 1s 466us/step - loss: 23527.5995 - val_loss: 20440.9397\n",
      "Epoch 158/1000\n",
      "2304/2304 [==============================] - 1s 462us/step - loss: 23367.5987 - val_loss: 20027.8654\n",
      "Epoch 159/1000\n",
      "2304/2304 [==============================] - 1s 447us/step - loss: 23393.3923 - val_loss: 18406.9154\n",
      "Epoch 160/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 23547.7516 - val_loss: 19510.1907\n",
      "Epoch 161/1000\n",
      "2304/2304 [==============================] - 1s 455us/step - loss: 23700.4968 - val_loss: 19711.2351\n",
      "Epoch 162/1000\n",
      "2304/2304 [==============================] - 1s 447us/step - loss: 23521.9805 - val_loss: 19353.0765\n",
      "Epoch 163/1000\n",
      "2304/2304 [==============================] - 1s 440us/step - loss: 23289.4974 - val_loss: 18495.0024\n",
      "Epoch 164/1000\n",
      "2304/2304 [==============================] - 1s 467us/step - loss: 23146.1177 - val_loss: 18525.8958\n",
      "Epoch 165/1000\n",
      "2304/2304 [==============================] - 1s 474us/step - loss: 23378.2289 - val_loss: 18674.6753\n",
      "Epoch 166/1000\n",
      "2304/2304 [==============================] - 1s 432us/step - loss: 22990.6150 - val_loss: 18405.3405\n",
      "Epoch 167/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 23208.1085 - val_loss: 18444.8079\n",
      "Epoch 168/1000\n",
      "2304/2304 [==============================] - 1s 464us/step - loss: 23341.9950 - val_loss: 18214.6930\n",
      "Epoch 169/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 22968.4293 - val_loss: 18287.0789\n",
      "Epoch 170/1000\n",
      "2304/2304 [==============================] - 1s 453us/step - loss: 23336.9285 - val_loss: 19328.6353\n",
      "Epoch 171/1000\n",
      "2304/2304 [==============================] - 1s 455us/step - loss: 23004.5811 - val_loss: 22645.0320\n",
      "Epoch 172/1000\n",
      "2304/2304 [==============================] - 1s 460us/step - loss: 23450.7607 - val_loss: 18237.4190\n",
      "Epoch 173/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 23322.7922 - val_loss: 18528.1455\n",
      "Epoch 174/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 22646.3451 - val_loss: 18115.4026\n",
      "Epoch 175/1000\n",
      "2304/2304 [==============================] - 1s 453us/step - loss: 22568.9873 - val_loss: 19539.2561\n",
      "Epoch 176/1000\n",
      "2304/2304 [==============================] - 1s 459us/step - loss: 23544.2008 - val_loss: 17933.2717\n",
      "Epoch 177/1000\n",
      "2304/2304 [==============================] - 1s 458us/step - loss: 22497.0116 - val_loss: 17354.5006\n",
      "Epoch 178/1000\n",
      "2304/2304 [==============================] - 1s 458us/step - loss: 22879.9811 - val_loss: 17579.1927\n",
      "Epoch 179/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 22938.6665 - val_loss: 18413.8741\n",
      "Epoch 180/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 22665.2603 - val_loss: 19428.7250\n",
      "Epoch 181/1000\n",
      "2304/2304 [==============================] - 1s 461us/step - loss: 22319.2484 - val_loss: 17366.1969\n",
      "Epoch 182/1000\n",
      "2304/2304 [==============================] - 1s 453us/step - loss: 22669.5232 - val_loss: 18838.8342\n",
      "Epoch 183/1000\n",
      "2304/2304 [==============================] - 1s 452us/step - loss: 22451.1597 - val_loss: 18472.8874\n",
      "Epoch 184/1000\n",
      "2304/2304 [==============================] - 1s 461us/step - loss: 22706.7617 - val_loss: 18255.4424\n",
      "Epoch 185/1000\n",
      "2304/2304 [==============================] - 1s 463us/step - loss: 22957.0078 - val_loss: 17473.3416\n",
      "Epoch 186/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 22531.9464 - val_loss: 16844.6493\n",
      "Epoch 187/1000\n",
      "2304/2304 [==============================] - 1s 474us/step - loss: 22240.5742 - val_loss: 17564.5887\n",
      "Epoch 188/1000\n",
      "2304/2304 [==============================] - 1s 471us/step - loss: 22642.6283 - val_loss: 17112.6908\n",
      "Epoch 189/1000\n",
      "2304/2304 [==============================] - 1s 462us/step - loss: 22320.6819 - val_loss: 17639.8543\n",
      "Epoch 190/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 22117.3255 - val_loss: 17005.3269\n",
      "Epoch 191/1000\n",
      "2304/2304 [==============================] - 1s 463us/step - loss: 22249.5553 - val_loss: 18395.3013\n",
      "Epoch 192/1000\n",
      "2304/2304 [==============================] - 1s 456us/step - loss: 22401.0267 - val_loss: 16680.4410\n",
      "Epoch 193/1000\n",
      "2304/2304 [==============================] - 1s 466us/step - loss: 22384.8159 - val_loss: 17162.2615\n",
      "Epoch 194/1000\n",
      "2304/2304 [==============================] - 1s 475us/step - loss: 21983.3552 - val_loss: 18605.2577\n",
      "Epoch 195/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 22257.2526 - val_loss: 17034.5954\n",
      "Epoch 196/1000\n",
      "2304/2304 [==============================] - 1s 465us/step - loss: 22122.9456 - val_loss: 17328.9022\n",
      "Epoch 197/1000\n",
      "2304/2304 [==============================] - 1s 471us/step - loss: 22467.4348 - val_loss: 17312.1251\n",
      "Epoch 198/1000\n",
      "2304/2304 [==============================] - 1s 480us/step - loss: 22034.0424 - val_loss: 16346.6881\n",
      "Epoch 199/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 21772.8173 - val_loss: 16449.1762\n",
      "Epoch 200/1000\n",
      "2304/2304 [==============================] - 1s 466us/step - loss: 21616.0995 - val_loss: 19556.2224\n",
      "Epoch 201/1000\n",
      "2304/2304 [==============================] - 1s 464us/step - loss: 22326.2749 - val_loss: 16196.6720\n",
      "Epoch 202/1000\n",
      "2304/2304 [==============================] - 1s 452us/step - loss: 22678.2432 - val_loss: 16400.9500\n",
      "Epoch 203/1000\n",
      "2304/2304 [==============================] - 1s 462us/step - loss: 21954.1421 - val_loss: 17115.8815\n",
      "Epoch 204/1000\n",
      "2304/2304 [==============================] - 1s 455us/step - loss: 21574.3031 - val_loss: 16127.1571\n",
      "Epoch 205/1000\n",
      "2304/2304 [==============================] - 1s 480us/step - loss: 22338.9508 - val_loss: 19444.3208\n",
      "Epoch 206/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 21852.7934 - val_loss: 19308.3233\n",
      "Epoch 207/1000\n",
      "2304/2304 [==============================] - 1s 473us/step - loss: 21872.5774 - val_loss: 17685.5849\n",
      "Epoch 208/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 21486.7361 - val_loss: 16639.5072\n",
      "Epoch 209/1000\n",
      "2304/2304 [==============================] - 1s 464us/step - loss: 21571.2908 - val_loss: 16761.9651\n",
      "Epoch 210/1000\n",
      "2304/2304 [==============================] - 1s 461us/step - loss: 21631.3609 - val_loss: 15658.4923\n",
      "Epoch 211/1000\n",
      "2304/2304 [==============================] - 1s 464us/step - loss: 21486.4054 - val_loss: 15814.1505\n",
      "Epoch 212/1000\n",
      "2304/2304 [==============================] - 1s 455us/step - loss: 21541.5087 - val_loss: 17495.6494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213/1000\n",
      "2304/2304 [==============================] - 1s 449us/step - loss: 22039.3132 - val_loss: 18907.8804\n",
      "Epoch 214/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 21535.5008 - val_loss: 16334.5270\n",
      "Epoch 215/1000\n",
      "2304/2304 [==============================] - 1s 474us/step - loss: 21397.5464 - val_loss: 15527.8804\n",
      "Epoch 216/1000\n",
      "2304/2304 [==============================] - 1s 459us/step - loss: 20970.6127 - val_loss: 15893.2644\n",
      "Epoch 217/1000\n",
      "2304/2304 [==============================] - 1s 454us/step - loss: 21382.1890 - val_loss: 16356.3695\n",
      "Epoch 218/1000\n",
      "2304/2304 [==============================] - 1s 451us/step - loss: 21161.6115 - val_loss: 17358.3467\n",
      "Epoch 219/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 21313.9134 - val_loss: 16149.4819\n",
      "Epoch 220/1000\n",
      "2304/2304 [==============================] - 1s 451us/step - loss: 21500.9419 - val_loss: 15796.3191\n",
      "Epoch 221/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 21536.2349 - val_loss: 15471.1339\n",
      "Epoch 222/1000\n",
      "2304/2304 [==============================] - 1s 463us/step - loss: 21088.8941 - val_loss: 15343.0244\n",
      "Epoch 223/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 21667.5794 - val_loss: 17580.5199\n",
      "Epoch 224/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 20965.1858 - val_loss: 15803.6322\n",
      "Epoch 225/1000\n",
      "2304/2304 [==============================] - 1s 449us/step - loss: 21442.6807 - val_loss: 17598.5028\n",
      "Epoch 226/1000\n",
      "2304/2304 [==============================] - 1s 471us/step - loss: 21107.7338 - val_loss: 15927.4279\n",
      "Epoch 227/1000\n",
      "2304/2304 [==============================] - 1s 459us/step - loss: 20982.8748 - val_loss: 16270.2747\n",
      "Epoch 228/1000\n",
      "2304/2304 [==============================] - 1s 451us/step - loss: 21420.9400 - val_loss: 15423.4538\n",
      "Epoch 229/1000\n",
      "2304/2304 [==============================] - 1s 449us/step - loss: 20855.8819 - val_loss: 16644.8780\n",
      "Epoch 230/1000\n",
      "2304/2304 [==============================] - 1s 458us/step - loss: 20768.6303 - val_loss: 15575.5588\n",
      "Epoch 231/1000\n",
      "2304/2304 [==============================] - 1s 455us/step - loss: 21261.2271 - val_loss: 16437.3966\n",
      "Epoch 232/1000\n",
      "2304/2304 [==============================] - 1s 457us/step - loss: 21497.1832 - val_loss: 15420.6945\n",
      "Epoch 233/1000\n",
      "2304/2304 [==============================] - 1s 447us/step - loss: 20544.3840 - val_loss: 15224.6794\n",
      "Epoch 234/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 21233.0104 - val_loss: 15038.2329\n",
      "Epoch 235/1000\n",
      "2304/2304 [==============================] - 1s 449us/step - loss: 21491.5446 - val_loss: 17434.5473\n",
      "Epoch 236/1000\n",
      "2304/2304 [==============================] - 1s 454us/step - loss: 21326.1712 - val_loss: 16683.8884\n",
      "Epoch 237/1000\n",
      "2304/2304 [==============================] - 1s 450us/step - loss: 21116.4700 - val_loss: 15612.9380\n",
      "Epoch 238/1000\n",
      "2304/2304 [==============================] - 1s 463us/step - loss: 22033.4280 - val_loss: 15167.3380\n",
      "Epoch 239/1000\n",
      "2304/2304 [==============================] - 1s 449us/step - loss: 21327.5555 - val_loss: 15373.3279\n",
      "Epoch 240/1000\n",
      "2304/2304 [==============================] - 1s 444us/step - loss: 20829.2319 - val_loss: 15861.8656\n",
      "Epoch 241/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 20925.8254 - val_loss: 15428.4299\n",
      "Epoch 242/1000\n",
      "2304/2304 [==============================] - 1s 510us/step - loss: 21137.2842 - val_loss: 19346.3964\n",
      "Epoch 243/1000\n",
      "2304/2304 [==============================] - 1s 495us/step - loss: 20912.4358 - val_loss: 15086.4801\n",
      "Epoch 244/1000\n",
      "2304/2304 [==============================] - 1s 451us/step - loss: 20783.2399 - val_loss: 17047.4020\n",
      "Epoch 245/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 20600.0964 - val_loss: 16235.3380\n",
      "Epoch 246/1000\n",
      "2304/2304 [==============================] - 1s 459us/step - loss: 20756.1085 - val_loss: 15630.0819\n",
      "Epoch 247/1000\n",
      "2304/2304 [==============================] - 1s 487us/step - loss: 20794.2793 - val_loss: 15813.6791\n",
      "Epoch 248/1000\n",
      "2304/2304 [==============================] - 1s 463us/step - loss: 20578.2091 - val_loss: 14941.2435\n",
      "Epoch 249/1000\n",
      "2304/2304 [==============================] - 1s 459us/step - loss: 20526.0308 - val_loss: 17806.7003\n",
      "Epoch 250/1000\n",
      "2304/2304 [==============================] - 1s 583us/step - loss: 20334.8256 - val_loss: 18193.9887\n",
      "Epoch 251/1000\n",
      "2304/2304 [==============================] - 1s 530us/step - loss: 20293.9275 - val_loss: 17792.9437\n",
      "Epoch 252/1000\n",
      "2304/2304 [==============================] - 1s 451us/step - loss: 20567.1465 - val_loss: 15440.0669\n",
      "Epoch 253/1000\n",
      "2304/2304 [==============================] - 1s 450us/step - loss: 20427.0153 - val_loss: 14923.4824\n",
      "Epoch 254/1000\n",
      "2304/2304 [==============================] - 1s 461us/step - loss: 20450.5625 - val_loss: 16042.9484\n",
      "Epoch 255/1000\n",
      "2304/2304 [==============================] - 1s 455us/step - loss: 20198.3318 - val_loss: 16222.9939\n",
      "Epoch 256/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 20533.8341 - val_loss: 16831.9639\n",
      "Epoch 257/1000\n",
      "2304/2304 [==============================] - 1s 509us/step - loss: 20154.9191 - val_loss: 14754.8845\n",
      "Epoch 258/1000\n",
      "2304/2304 [==============================] - 1s 469us/step - loss: 20387.4427 - val_loss: 15878.7856\n",
      "Epoch 259/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 20359.2988 - val_loss: 16003.4819\n",
      "Epoch 260/1000\n",
      "2304/2304 [==============================] - 1s 461us/step - loss: 20531.9542 - val_loss: 16200.7362\n",
      "Epoch 261/1000\n",
      "2304/2304 [==============================] - 1s 454us/step - loss: 20018.3629 - val_loss: 15491.6521\n",
      "Epoch 262/1000\n",
      "2304/2304 [==============================] - 1s 481us/step - loss: 20267.9428 - val_loss: 15069.8928\n",
      "Epoch 263/1000\n",
      "2304/2304 [==============================] - 1s 479us/step - loss: 20530.1369 - val_loss: 15627.3145\n",
      "Epoch 264/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 20294.7257 - val_loss: 17765.7470\n",
      "Epoch 265/1000\n",
      "2304/2304 [==============================] - 1s 436us/step - loss: 20360.8740 - val_loss: 14508.3821\n",
      "Epoch 266/1000\n",
      "2304/2304 [==============================] - 1s 425us/step - loss: 20267.8221 - val_loss: 15420.8517\n",
      "Epoch 267/1000\n",
      "2304/2304 [==============================] - 1s 444us/step - loss: 19862.5230 - val_loss: 14701.4737\n",
      "Epoch 268/1000\n",
      "2304/2304 [==============================] - 1s 433us/step - loss: 19815.6728 - val_loss: 14743.0286\n",
      "Epoch 269/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 19757.1898 - val_loss: 16036.5053\n",
      "Epoch 270/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 19572.5032 - val_loss: 15336.2828\n",
      "Epoch 271/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 19700.6566 - val_loss: 15040.9445\n",
      "Epoch 272/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 19473.6894 - val_loss: 15602.6384\n",
      "Epoch 273/1000\n",
      "2304/2304 [==============================] - 1s 454us/step - loss: 19906.8170 - val_loss: 14509.2126\n",
      "Epoch 274/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 19699.7736 - val_loss: 15772.0936\n",
      "Epoch 275/1000\n",
      "2304/2304 [==============================] - 1s 434us/step - loss: 20105.0729 - val_loss: 14584.7437\n",
      "Epoch 276/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 20147.8612 - val_loss: 15057.5087\n",
      "Epoch 277/1000\n",
      "2304/2304 [==============================] - 1s 454us/step - loss: 19881.5120 - val_loss: 18572.5668\n",
      "Epoch 278/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 19517.4842 - val_loss: 16397.4856\n",
      "Epoch 279/1000\n",
      "2304/2304 [==============================] - 1s 433us/step - loss: 19976.7596 - val_loss: 15108.0453\n",
      "Epoch 280/1000\n",
      "2304/2304 [==============================] - 1s 436us/step - loss: 19935.8306 - val_loss: 15692.7591\n",
      "Epoch 281/1000\n",
      "2304/2304 [==============================] - 1s 447us/step - loss: 19793.6322 - val_loss: 14389.2686\n",
      "Epoch 282/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 19412.6952 - val_loss: 15312.0001\n",
      "Epoch 283/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 20059.2136 - val_loss: 15273.2969\n",
      "Epoch 284/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 19604.2384 - val_loss: 14420.2558\n",
      "Epoch 285/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 19496.6790 - val_loss: 14708.9673\n",
      "Epoch 286/1000\n",
      "2304/2304 [==============================] - 1s 440us/step - loss: 20049.3460 - val_loss: 16034.7254\n",
      "Epoch 287/1000\n",
      "2304/2304 [==============================] - 1s 430us/step - loss: 19478.0735 - val_loss: 14571.8502\n",
      "Epoch 288/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 19667.3767 - val_loss: 14611.5370\n",
      "Epoch 289/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 19504.3439 - val_loss: 14015.6003\n",
      "Epoch 290/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 19758.8318 - val_loss: 16997.5221\n",
      "Epoch 291/1000\n",
      "2304/2304 [==============================] - 1s 447us/step - loss: 19601.7732 - val_loss: 17615.9217\n",
      "Epoch 292/1000\n",
      "2304/2304 [==============================] - 1s 427us/step - loss: 19661.4567 - val_loss: 16816.5259\n",
      "Epoch 293/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 19131.7665 - val_loss: 14086.0776\n",
      "Epoch 294/1000\n",
      "2304/2304 [==============================] - 1s 444us/step - loss: 19576.5483 - val_loss: 14701.9938\n",
      "Epoch 295/1000\n",
      "2304/2304 [==============================] - 1s 461us/step - loss: 19444.5205 - val_loss: 14263.2803\n",
      "Epoch 296/1000\n",
      "2304/2304 [==============================] - 1s 451us/step - loss: 19304.4485 - val_loss: 14911.7827\n",
      "Epoch 297/1000\n",
      "2304/2304 [==============================] - 2s 687us/step - loss: 19703.8128 - val_loss: 16053.4814\n",
      "Epoch 298/1000\n",
      "2304/2304 [==============================] - 1s 462us/step - loss: 19346.9465 - val_loss: 14261.0299\n",
      "Epoch 299/1000\n",
      "2304/2304 [==============================] - 1s 434us/step - loss: 19704.9612 - val_loss: 14267.3405\n",
      "Epoch 300/1000\n",
      "2304/2304 [==============================] - 1s 436us/step - loss: 19352.9026 - val_loss: 14148.4302\n",
      "Epoch 301/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 19498.7444 - val_loss: 14138.5259\n",
      "Epoch 302/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 18726.3407 - val_loss: 15887.5956\n",
      "Epoch 303/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 19946.6447 - val_loss: 16257.2649\n",
      "Epoch 304/1000\n",
      "2304/2304 [==============================] - 1s 457us/step - loss: 19362.7704 - val_loss: 13983.4914\n",
      "Epoch 305/1000\n",
      "2304/2304 [==============================] - 1s 450us/step - loss: 18790.8474 - val_loss: 14023.9126\n",
      "Epoch 306/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 19160.9730 - val_loss: 14827.6932\n",
      "Epoch 307/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 19190.6534 - val_loss: 18906.2787\n",
      "Epoch 308/1000\n",
      "2304/2304 [==============================] - 1s 449us/step - loss: 19401.0639 - val_loss: 14333.4869\n",
      "Epoch 309/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 19238.1993 - val_loss: 19174.5764\n",
      "Epoch 310/1000\n",
      "2304/2304 [==============================] - 1s 450us/step - loss: 20029.8387 - val_loss: 14179.8829\n",
      "Epoch 311/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 18918.1769 - val_loss: 15346.9258\n",
      "Epoch 312/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 19041.3360 - val_loss: 15424.8321\n",
      "Epoch 313/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 19119.5293 - val_loss: 14270.7577\n",
      "Epoch 314/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 19292.8256 - val_loss: 14625.2434\n",
      "Epoch 315/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 19020.3923 - val_loss: 15541.8373\n",
      "Epoch 316/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 19109.0529 - val_loss: 14618.3543\n",
      "Epoch 317/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 18554.5315 - val_loss: 14239.7411\n",
      "Epoch 318/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 19012.3046 - val_loss: 14200.9190\n",
      "Epoch 319/1000\n",
      "2304/2304 [==============================] - 1s 444us/step - loss: 18846.4739 - val_loss: 14101.2692\n",
      "Epoch 320/1000\n",
      "2304/2304 [==============================] - 1s 447us/step - loss: 18813.7599 - val_loss: 14046.8665\n",
      "Epoch 321/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 18852.2364 - val_loss: 15203.3614\n",
      "Epoch 322/1000\n",
      "2304/2304 [==============================] - 1s 449us/step - loss: 19056.3984 - val_loss: 14890.5568\n",
      "Epoch 323/1000\n",
      "2304/2304 [==============================] - 1s 460us/step - loss: 18563.3026 - val_loss: 13836.5927\n",
      "Epoch 324/1000\n",
      "2304/2304 [==============================] - 1s 450us/step - loss: 19340.7887 - val_loss: 14266.8893\n",
      "Epoch 325/1000\n",
      "2304/2304 [==============================] - 1s 450us/step - loss: 19060.3305 - val_loss: 15360.9089\n",
      "Epoch 326/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 18662.2458 - val_loss: 14017.1838\n",
      "Epoch 327/1000\n",
      "2304/2304 [==============================] - 1s 452us/step - loss: 18428.4088 - val_loss: 14745.7661\n",
      "Epoch 328/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 19135.0274 - val_loss: 15207.9820\n",
      "Epoch 329/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 18923.7394 - val_loss: 14965.7481\n",
      "Epoch 330/1000\n",
      "2304/2304 [==============================] - 1s 452us/step - loss: 18535.9474 - val_loss: 17499.9861\n",
      "Epoch 331/1000\n",
      "2304/2304 [==============================] - 1s 440us/step - loss: 19013.6725 - val_loss: 15235.5978\n",
      "Epoch 332/1000\n",
      "2304/2304 [==============================] - 1s 458us/step - loss: 18596.5386 - val_loss: 15663.1766\n",
      "Epoch 333/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 18753.9279 - val_loss: 14441.8432\n",
      "Epoch 334/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 18830.9042 - val_loss: 14302.3303\n",
      "Epoch 335/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 18712.8998 - val_loss: 14407.8947\n",
      "Epoch 336/1000\n",
      "2304/2304 [==============================] - 1s 444us/step - loss: 18339.9842 - val_loss: 16368.5578\n",
      "Epoch 337/1000\n",
      "2304/2304 [==============================] - 1s 454us/step - loss: 18322.5995 - val_loss: 14554.7695\n",
      "Epoch 338/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 18302.4392 - val_loss: 18285.9036\n",
      "Epoch 339/1000\n",
      "2304/2304 [==============================] - 1s 449us/step - loss: 18712.5002 - val_loss: 16307.5702\n",
      "Epoch 340/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 18290.9863 - val_loss: 14239.9206\n",
      "Epoch 341/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 18704.4309 - val_loss: 14550.0324\n",
      "Epoch 342/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 18247.0918 - val_loss: 14879.0659\n",
      "Epoch 343/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 18006.7334 - val_loss: 18404.3097\n",
      "Epoch 344/1000\n",
      "2304/2304 [==============================] - 1s 447us/step - loss: 18743.3766 - val_loss: 14351.7088\n",
      "Epoch 345/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 18354.7083 - val_loss: 13809.4760\n",
      "Epoch 346/1000\n",
      "2304/2304 [==============================] - 1s 430us/step - loss: 18344.7377 - val_loss: 14164.0234\n",
      "Epoch 347/1000\n",
      "2304/2304 [==============================] - 1s 450us/step - loss: 17953.9372 - val_loss: 15203.0973\n",
      "Epoch 348/1000\n",
      "2304/2304 [==============================] - 1s 430us/step - loss: 18169.8337 - val_loss: 15557.0146\n",
      "Epoch 349/1000\n",
      "2304/2304 [==============================] - 1s 456us/step - loss: 18846.4769 - val_loss: 13736.5097\n",
      "Epoch 350/1000\n",
      "2304/2304 [==============================] - 1s 432us/step - loss: 18310.2781 - val_loss: 15919.4781\n",
      "Epoch 351/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 17808.3110 - val_loss: 16368.7301\n",
      "Epoch 352/1000\n",
      "2304/2304 [==============================] - 1s 444us/step - loss: 18350.3996 - val_loss: 14352.2247\n",
      "Epoch 353/1000\n",
      "2304/2304 [==============================] - 1s 455us/step - loss: 18125.6151 - val_loss: 14588.8044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 354/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 17982.2255 - val_loss: 15111.4960\n",
      "Epoch 355/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 18200.9428 - val_loss: 16779.2044\n",
      "Epoch 356/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 17829.2466 - val_loss: 15074.7010\n",
      "Epoch 357/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 18016.9196 - val_loss: 14864.5632\n",
      "Epoch 358/1000\n",
      "2304/2304 [==============================] - 1s 430us/step - loss: 17915.1799 - val_loss: 24316.4935\n",
      "Epoch 359/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 19223.4313 - val_loss: 14180.9230\n",
      "Epoch 360/1000\n",
      "2304/2304 [==============================] - 1s 426us/step - loss: 18647.8201 - val_loss: 13863.6924\n",
      "Epoch 361/1000\n",
      "2304/2304 [==============================] - 1s 436us/step - loss: 18664.2566 - val_loss: 15385.2874\n",
      "Epoch 362/1000\n",
      "2304/2304 [==============================] - 1s 444us/step - loss: 17911.7704 - val_loss: 14680.4609\n",
      "Epoch 363/1000\n",
      "2304/2304 [==============================] - 1s 452us/step - loss: 17927.6700 - val_loss: 14011.2587\n",
      "Epoch 364/1000\n",
      "2304/2304 [==============================] - 1s 452us/step - loss: 17879.9066 - val_loss: 14464.9448\n",
      "Epoch 365/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 18198.5349 - val_loss: 15854.3383\n",
      "Epoch 366/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 17806.2551 - val_loss: 16639.1137\n",
      "Epoch 367/1000\n",
      "2304/2304 [==============================] - 1s 444us/step - loss: 17713.9328 - val_loss: 15978.6447\n",
      "Epoch 368/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 18191.1775 - val_loss: 13860.3972\n",
      "Epoch 369/1000\n",
      "2304/2304 [==============================] - 1s 447us/step - loss: 17839.4774 - val_loss: 16243.3496\n",
      "Epoch 370/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 18056.7944 - val_loss: 13988.9696\n",
      "Epoch 371/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 17830.5053 - val_loss: 13991.4631\n",
      "Epoch 372/1000\n",
      "2304/2304 [==============================] - 1s 434us/step - loss: 17826.4593 - val_loss: 14288.8727\n",
      "Epoch 373/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 18230.0502 - val_loss: 13970.4442\n",
      "Epoch 374/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 18149.2701 - val_loss: 13912.5779\n",
      "Epoch 375/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 17991.8544 - val_loss: 14408.1569\n",
      "Epoch 376/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 17852.9588 - val_loss: 13976.1019\n",
      "Epoch 377/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 17220.9854 - val_loss: 14601.7443\n",
      "Epoch 378/1000\n",
      "2304/2304 [==============================] - 1s 451us/step - loss: 18052.9831 - val_loss: 14810.8670\n",
      "Epoch 379/1000\n",
      "2304/2304 [==============================] - 1s 451us/step - loss: 17936.4085 - val_loss: 15385.9314\n",
      "Epoch 380/1000\n",
      "2304/2304 [==============================] - 1s 436us/step - loss: 17604.2571 - val_loss: 22286.7142\n",
      "Epoch 381/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 18617.3345 - val_loss: 16034.6185\n",
      "Epoch 382/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 17403.8869 - val_loss: 14999.4376\n",
      "Epoch 383/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 17939.8127 - val_loss: 14766.4007\n",
      "Epoch 384/1000\n",
      "2304/2304 [==============================] - 1s 451us/step - loss: 18240.0339 - val_loss: 14057.3904\n",
      "Epoch 385/1000\n",
      "2304/2304 [==============================] - 1s 454us/step - loss: 17869.7416 - val_loss: 17336.3759\n",
      "Epoch 386/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 17826.6345 - val_loss: 15285.4909\n",
      "Epoch 387/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 17667.2922 - val_loss: 14430.3984\n",
      "Epoch 388/1000\n",
      "2304/2304 [==============================] - 1s 451us/step - loss: 17571.7256 - val_loss: 14216.6904\n",
      "Epoch 389/1000\n",
      "2304/2304 [==============================] - 1s 432us/step - loss: 17352.5210 - val_loss: 13842.0403\n",
      "Epoch 390/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 17804.1591 - val_loss: 14489.6196\n",
      "Epoch 391/1000\n",
      "2304/2304 [==============================] - 1s 436us/step - loss: 17290.5272 - val_loss: 15556.0982\n",
      "Epoch 392/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 17562.2438 - val_loss: 14057.2289\n",
      "Epoch 393/1000\n",
      "2304/2304 [==============================] - 1s 450us/step - loss: 17475.4925 - val_loss: 14393.1716\n",
      "Epoch 394/1000\n",
      "2304/2304 [==============================] - 1s 421us/step - loss: 17683.2711 - val_loss: 13867.6408\n",
      "Epoch 395/1000\n",
      "2304/2304 [==============================] - 1s 444us/step - loss: 18206.1145 - val_loss: 14119.2243\n",
      "Epoch 396/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 17573.8009 - val_loss: 14654.6191\n",
      "Epoch 397/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 17220.1603 - val_loss: 17501.2350\n",
      "Epoch 398/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 17444.1953 - val_loss: 15443.5200\n",
      "Epoch 399/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 17212.9262 - val_loss: 14271.1408\n",
      "Epoch 400/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 18024.3335 - val_loss: 14294.3726\n",
      "Epoch 401/1000\n",
      "2304/2304 [==============================] - 1s 436us/step - loss: 17486.2469 - val_loss: 13969.2431\n",
      "Epoch 402/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 17356.3129 - val_loss: 14932.2968\n",
      "Epoch 403/1000\n",
      "2304/2304 [==============================] - 1s 444us/step - loss: 17733.6313 - val_loss: 15349.2541\n",
      "Epoch 404/1000\n",
      "2304/2304 [==============================] - 1s 444us/step - loss: 17096.2496 - val_loss: 14960.6332\n",
      "Epoch 405/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 17120.7252 - val_loss: 14787.6082\n",
      "Epoch 406/1000\n",
      "2304/2304 [==============================] - 1s 430us/step - loss: 17491.6736 - val_loss: 19178.5432\n",
      "Epoch 407/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 17724.7867 - val_loss: 14122.0443\n",
      "Epoch 408/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 17579.8739 - val_loss: 14003.0985\n",
      "Epoch 409/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 17608.8391 - val_loss: 17843.3101\n",
      "Epoch 410/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 17523.1099 - val_loss: 14959.4707\n",
      "Epoch 411/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 16910.8874 - val_loss: 13747.2737\n",
      "Epoch 412/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 17539.3152 - val_loss: 14720.9098\n",
      "Epoch 413/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 17267.5506 - val_loss: 14258.4516\n",
      "Epoch 414/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 17315.9268 - val_loss: 13593.2258\n",
      "Epoch 415/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 17380.5531 - val_loss: 14079.7826\n",
      "Epoch 416/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 16996.8228 - val_loss: 15113.6179\n",
      "Epoch 417/1000\n",
      "2304/2304 [==============================] - 1s 436us/step - loss: 17600.8175 - val_loss: 18986.0353\n",
      "Epoch 418/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 17023.1818 - val_loss: 14341.5397\n",
      "Epoch 419/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 17573.1886 - val_loss: 14370.9430\n",
      "Epoch 420/1000\n",
      "2304/2304 [==============================] - 1s 444us/step - loss: 17225.8739 - val_loss: 15407.3638\n",
      "Epoch 421/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 17412.0493 - val_loss: 19036.0672\n",
      "Epoch 422/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 17108.0607 - val_loss: 15606.9727\n",
      "Epoch 423/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 17136.4948 - val_loss: 15391.9444\n",
      "Epoch 424/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 17093.5108 - val_loss: 14448.2228\n",
      "Epoch 425/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 16837.8037 - val_loss: 16118.2474\n",
      "Epoch 426/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 17296.2004 - val_loss: 13944.9400\n",
      "Epoch 427/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 17272.2626 - val_loss: 13772.1651\n",
      "Epoch 428/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 17281.5558 - val_loss: 13642.4159\n",
      "Epoch 429/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 17101.2987 - val_loss: 15898.3358\n",
      "Epoch 430/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 16709.3143 - val_loss: 19334.2600\n",
      "Epoch 431/1000\n",
      "2304/2304 [==============================] - 1s 432us/step - loss: 16868.6854 - val_loss: 16250.8959\n",
      "Epoch 432/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 17385.3112 - val_loss: 13705.7346\n",
      "Epoch 433/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 17247.9291 - val_loss: 14623.1077\n",
      "Epoch 434/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 17188.4737 - val_loss: 13852.3366\n",
      "Epoch 435/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 16656.4663 - val_loss: 14797.9058\n",
      "Epoch 436/1000\n",
      "2304/2304 [==============================] - 1s 434us/step - loss: 17156.8052 - val_loss: 13735.2475\n",
      "Epoch 437/1000\n",
      "2304/2304 [==============================] - 1s 433us/step - loss: 17050.5127 - val_loss: 17040.0685\n",
      "Epoch 438/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 16785.3803 - val_loss: 15292.6484\n",
      "Epoch 439/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 16491.9697 - val_loss: 15786.6080\n",
      "Epoch 440/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 17464.6337 - val_loss: 15852.0196\n",
      "Epoch 441/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 17213.6505 - val_loss: 15082.6042\n",
      "Epoch 442/1000\n",
      "2304/2304 [==============================] - 1s 450us/step - loss: 17345.8611 - val_loss: 14912.8041\n",
      "Epoch 443/1000\n",
      "2304/2304 [==============================] - 1s 444us/step - loss: 16981.0900 - val_loss: 18065.5751\n",
      "Epoch 444/1000\n",
      "2304/2304 [==============================] - 1s 432us/step - loss: 17532.0109 - val_loss: 16386.4432\n",
      "Epoch 445/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 17591.5213 - val_loss: 14487.9600\n",
      "Epoch 446/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 17624.2621 - val_loss: 16752.8077\n",
      "Epoch 447/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 16998.9168 - val_loss: 13496.9836\n",
      "Epoch 448/1000\n",
      "2304/2304 [==============================] - 1s 430us/step - loss: 16836.7056 - val_loss: 14631.2539\n",
      "Epoch 449/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 16696.3389 - val_loss: 16631.2494\n",
      "Epoch 450/1000\n",
      "2304/2304 [==============================] - 1s 434us/step - loss: 16983.9090 - val_loss: 15312.0523\n",
      "Epoch 451/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 16810.2610 - val_loss: 14278.9533\n",
      "Epoch 452/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 17404.3263 - val_loss: 15086.4043\n",
      "Epoch 453/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 17100.2959 - val_loss: 13936.6329\n",
      "Epoch 454/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 16667.1507 - val_loss: 18293.9448\n",
      "Epoch 455/1000\n",
      "2304/2304 [==============================] - ETA: 0s - loss: 17168.9288- ETA: 0s - - 1s 438us/step - loss: 17086.7000 - val_loss: 13579.2260\n",
      "Epoch 456/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 16701.5239 - val_loss: 14569.9128\n",
      "Epoch 457/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 16879.7268 - val_loss: 13863.3398\n",
      "Epoch 458/1000\n",
      "2304/2304 [==============================] - 1s 436us/step - loss: 16426.4349 - val_loss: 17490.6424\n",
      "Epoch 459/1000\n",
      "2304/2304 [==============================] - 1s 440us/step - loss: 17047.0664 - val_loss: 14906.5276\n",
      "Epoch 460/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 17470.8861 - val_loss: 14052.3652\n",
      "Epoch 461/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 16674.0154 - val_loss: 17727.5517\n",
      "Epoch 462/1000\n",
      "2304/2304 [==============================] - 1s 444us/step - loss: 17145.5984 - val_loss: 14692.3786\n",
      "Epoch 463/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 17166.9755 - val_loss: 14124.9136\n",
      "Epoch 464/1000\n",
      "2304/2304 [==============================] - 1s 444us/step - loss: 16679.6169 - val_loss: 19373.4150\n",
      "Epoch 465/1000\n",
      "2304/2304 [==============================] - 1s 450us/step - loss: 16774.5541 - val_loss: 14200.0199\n",
      "Epoch 466/1000\n",
      "2304/2304 [==============================] - 1s 434us/step - loss: 16595.9005 - val_loss: 14423.6457\n",
      "Epoch 467/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 16689.2204 - val_loss: 14240.4985\n",
      "Epoch 468/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 16708.0954 - val_loss: 14334.9017\n",
      "Epoch 469/1000\n",
      "2304/2304 [==============================] - 1s 449us/step - loss: 17191.4972 - val_loss: 13775.6621\n",
      "Epoch 470/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 17505.5673 - val_loss: 13765.9852\n",
      "Epoch 471/1000\n",
      "2304/2304 [==============================] - 1s 456us/step - loss: 17377.3753 - val_loss: 16874.4895\n",
      "Epoch 472/1000\n",
      "2304/2304 [==============================] - 1s 449us/step - loss: 16932.0075 - val_loss: 15280.1072\n",
      "Epoch 473/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 16530.4014 - val_loss: 14520.9200\n",
      "Epoch 474/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 17395.5364 - val_loss: 13848.9501\n",
      "Epoch 475/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 16857.5059 - val_loss: 13565.6380\n",
      "Epoch 476/1000\n",
      "2304/2304 [==============================] - 1s 455us/step - loss: 16767.7075 - val_loss: 19872.7868\n",
      "Epoch 477/1000\n",
      "2304/2304 [==============================] - 1s 447us/step - loss: 16884.9123 - val_loss: 13176.7210\n",
      "Epoch 478/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 16330.7191 - val_loss: 14813.3151\n",
      "Epoch 479/1000\n",
      "2304/2304 [==============================] - 1s 463us/step - loss: 16560.1846 - val_loss: 13560.4959\n",
      "Epoch 480/1000\n",
      "2304/2304 [==============================] - 1s 452us/step - loss: 16994.8374 - val_loss: 13748.0220\n",
      "Epoch 481/1000\n",
      "2304/2304 [==============================] - 1s 453us/step - loss: 17105.0768 - val_loss: 13977.5572\n",
      "Epoch 482/1000\n",
      "2304/2304 [==============================] - 1s 457us/step - loss: 16926.7186 - val_loss: 15421.4735\n",
      "Epoch 483/1000\n",
      "2304/2304 [==============================] - ETA: 0s - loss: 17416.383 - 1s 442us/step - loss: 17520.7117 - val_loss: 14974.8713\n",
      "Epoch 484/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 17241.7099 - val_loss: 16109.2987\n",
      "Epoch 485/1000\n",
      "2304/2304 [==============================] - 1s 467us/step - loss: 16878.6128 - val_loss: 13340.2037\n",
      "Epoch 486/1000\n",
      "2304/2304 [==============================] - 1s 460us/step - loss: 16567.2597 - val_loss: 13226.7989\n",
      "Epoch 487/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 17009.0485 - val_loss: 14334.8273\n",
      "Epoch 488/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 17224.3196 - val_loss: 15182.8221\n",
      "Epoch 489/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 16113.3428 - val_loss: 14707.3002\n",
      "Epoch 490/1000\n",
      "2304/2304 [==============================] - 1s 447us/step - loss: 16157.4614 - val_loss: 17262.7748\n",
      "Epoch 491/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 16754.3682 - val_loss: 17514.4146\n",
      "Epoch 492/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 16435.4573 - val_loss: 13711.0666\n",
      "Epoch 493/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 17039.9159 - val_loss: 15920.7148\n",
      "Epoch 494/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304/2304 [==============================] - 1s 443us/step - loss: 16618.5890 - val_loss: 13543.7893\n",
      "Epoch 495/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 17146.2448 - val_loss: 18693.5495\n",
      "Epoch 496/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 16916.5060 - val_loss: 13491.6188\n",
      "Epoch 497/1000\n",
      "2304/2304 [==============================] - 1s 440us/step - loss: 16322.7569 - val_loss: 14055.7516\n",
      "Epoch 498/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 16738.7356 - val_loss: 13729.1235\n",
      "Epoch 499/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 17037.1458 - val_loss: 19032.3017\n",
      "Epoch 500/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 17037.1399 - val_loss: 13309.6670\n",
      "Epoch 501/1000\n",
      "2304/2304 [==============================] - 1s 453us/step - loss: 16405.3647 - val_loss: 13464.1638\n",
      "Epoch 502/1000\n",
      "2304/2304 [==============================] - 1s 449us/step - loss: 17211.2725 - val_loss: 17083.4213\n",
      "Epoch 503/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 17172.4802 - val_loss: 13478.5523\n",
      "Epoch 504/1000\n",
      "2304/2304 [==============================] - 1s 449us/step - loss: 16247.5303 - val_loss: 14198.8873\n",
      "Epoch 505/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 16671.5918 - val_loss: 13535.8521\n",
      "Epoch 506/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 16185.6086 - val_loss: 14095.6265\n",
      "Epoch 507/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 16207.7347 - val_loss: 14325.4418\n",
      "Epoch 508/1000\n",
      "2304/2304 [==============================] - 1s 416us/step - loss: 16652.1647 - val_loss: 14851.3673\n",
      "Epoch 509/1000\n",
      "2304/2304 [==============================] - 1s 413us/step - loss: 16606.8829 - val_loss: 14483.6561\n",
      "Epoch 510/1000\n",
      "2304/2304 [==============================] - 1s 416us/step - loss: 16154.6917 - val_loss: 13027.8321\n",
      "Epoch 511/1000\n",
      "2304/2304 [==============================] - 1s 422us/step - loss: 16352.7531 - val_loss: 13074.6578\n",
      "Epoch 512/1000\n",
      "2304/2304 [==============================] - 1s 416us/step - loss: 16269.5398 - val_loss: 15614.6946\n",
      "Epoch 513/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 16844.1695 - val_loss: 13532.4958\n",
      "Epoch 514/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 16391.5720 - val_loss: 13839.0900\n",
      "Epoch 515/1000\n",
      "2304/2304 [==============================] - 1s 447us/step - loss: 16910.4774 - val_loss: 13006.5320\n",
      "Epoch 516/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 16441.9044 - val_loss: 13741.3823\n",
      "Epoch 517/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 16175.2031 - val_loss: 13118.6830\n",
      "Epoch 518/1000\n",
      "2304/2304 [==============================] - 1s 447us/step - loss: 16029.5415 - val_loss: 13535.7123\n",
      "Epoch 519/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 16822.5800 - val_loss: 12918.1999\n",
      "Epoch 520/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 16400.1417 - val_loss: 13436.6186\n",
      "Epoch 521/1000\n",
      "2304/2304 [==============================] - 1s 471us/step - loss: 16581.6498 - val_loss: 13149.8857\n",
      "Epoch 522/1000\n",
      "2304/2304 [==============================] - 1s 444us/step - loss: 16398.8991 - val_loss: 13370.1782\n",
      "Epoch 523/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 16296.1482 - val_loss: 16340.5884\n",
      "Epoch 524/1000\n",
      "2304/2304 [==============================] - 1s 447us/step - loss: 16327.9647 - val_loss: 13298.2406\n",
      "Epoch 525/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 16280.1426 - val_loss: 13291.0479\n",
      "Epoch 526/1000\n",
      "2304/2304 [==============================] - 1s 440us/step - loss: 16098.8848 - val_loss: 13626.7445\n",
      "Epoch 527/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 16265.7438 - val_loss: 13826.5380\n",
      "Epoch 528/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 15989.9011 - val_loss: 14713.3395\n",
      "Epoch 529/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 15569.1148 - val_loss: 13209.5831\n",
      "Epoch 530/1000\n",
      "2304/2304 [==============================] - 1s 449us/step - loss: 16541.1532 - val_loss: 17341.2461\n",
      "Epoch 531/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 16239.2389 - val_loss: 13829.2114\n",
      "Epoch 532/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 16521.4224 - val_loss: 15856.1819\n",
      "Epoch 533/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 15968.5748 - val_loss: 12955.7784\n",
      "Epoch 534/1000\n",
      "2304/2304 [==============================] - 1s 452us/step - loss: 16293.7703 - val_loss: 13159.6920\n",
      "Epoch 535/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 15915.4719 - val_loss: 15236.5337\n",
      "Epoch 536/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 16546.5183 - val_loss: 16853.1425\n",
      "Epoch 537/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 16415.3709 - val_loss: 22590.1958\n",
      "Epoch 538/1000\n",
      "2304/2304 [==============================] - 1s 440us/step - loss: 16038.1731 - val_loss: 15478.0320\n",
      "Epoch 539/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 16662.2867 - val_loss: 13633.4050\n",
      "Epoch 540/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 16813.9251 - val_loss: 12660.4432\n",
      "Epoch 541/1000\n",
      "2304/2304 [==============================] - 1s 464us/step - loss: 16527.9992 - val_loss: 14913.9604\n",
      "Epoch 542/1000\n",
      "2304/2304 [==============================] - 1s 431us/step - loss: 16079.3291 - val_loss: 13788.1918\n",
      "Epoch 543/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 16225.2902 - val_loss: 14426.3797\n",
      "Epoch 544/1000\n",
      "2304/2304 [==============================] - 1s 456us/step - loss: 15732.6024 - val_loss: 13636.5850\n",
      "Epoch 545/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 16214.2543 - val_loss: 21259.3888\n",
      "Epoch 546/1000\n",
      "2304/2304 [==============================] - 1s 451us/step - loss: 16336.5275 - val_loss: 13642.9841\n",
      "Epoch 547/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 16242.7494 - val_loss: 12891.3443\n",
      "Epoch 548/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 15609.3064 - val_loss: 15241.3289\n",
      "Epoch 549/1000\n",
      "2304/2304 [==============================] - 1s 454us/step - loss: 16055.6001 - val_loss: 15129.6870\n",
      "Epoch 550/1000\n",
      "2304/2304 [==============================] - 1s 456us/step - loss: 16056.1482 - val_loss: 13194.1173\n",
      "Epoch 551/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 16373.1419 - val_loss: 13835.9007\n",
      "Epoch 552/1000\n",
      "2304/2304 [==============================] - 1s 433us/step - loss: 16585.9757 - val_loss: 15642.5804\n",
      "Epoch 553/1000\n",
      "2304/2304 [==============================] - 1s 436us/step - loss: 16268.6899 - val_loss: 12619.5854\n",
      "Epoch 554/1000\n",
      "2304/2304 [==============================] - 1s 444us/step - loss: 16432.3227 - val_loss: 14910.7167\n",
      "Epoch 555/1000\n",
      "2304/2304 [==============================] - 1s 432us/step - loss: 15964.3666 - val_loss: 13346.9381\n",
      "Epoch 556/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 15907.5030 - val_loss: 12983.0486\n",
      "Epoch 557/1000\n",
      "2304/2304 [==============================] - 1s 436us/step - loss: 15918.8584 - val_loss: 13650.5844\n",
      "Epoch 558/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 16200.4612 - val_loss: 12951.2539\n",
      "Epoch 559/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 15835.6661 - val_loss: 14813.2323\n",
      "Epoch 560/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 16570.0578 - val_loss: 14454.0631\n",
      "Epoch 561/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 16869.6169 - val_loss: 13610.7718\n",
      "Epoch 562/1000\n",
      "2304/2304 [==============================] - 1s 447us/step - loss: 16840.4261 - val_loss: 13181.9136\n",
      "Epoch 563/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 15979.8033 - val_loss: 13446.4118\n",
      "Epoch 564/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 15563.6671 - val_loss: 13199.9821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 565/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 15908.4225 - val_loss: 15984.1726\n",
      "Epoch 566/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 16004.9399 - val_loss: 13905.7834\n",
      "Epoch 567/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 16389.2974 - val_loss: 13452.7645\n",
      "Epoch 568/1000\n",
      "2304/2304 [==============================] - 1s 436us/step - loss: 15584.9971 - val_loss: 14775.5267\n",
      "Epoch 569/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 15664.2811 - val_loss: 13648.0294\n",
      "Epoch 570/1000\n",
      "2304/2304 [==============================] - 1s 434us/step - loss: 16375.2549 - val_loss: 13240.0203\n",
      "Epoch 571/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 15903.5278 - val_loss: 14575.3229\n",
      "Epoch 572/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 16540.2915 - val_loss: 12986.2193\n",
      "Epoch 573/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 15546.8738 - val_loss: 12724.5387\n",
      "Epoch 574/1000\n",
      "2304/2304 [==============================] - 1s 447us/step - loss: 15573.6981 - val_loss: 14370.9492\n",
      "Epoch 575/1000\n",
      "2304/2304 [==============================] - 1s 444us/step - loss: 15998.9777 - val_loss: 14381.9216\n",
      "Epoch 576/1000\n",
      "2304/2304 [==============================] - 1s 451us/step - loss: 16236.6067 - val_loss: 13139.3926\n",
      "Epoch 577/1000\n",
      "2304/2304 [==============================] - 1s 453us/step - loss: 16401.3169 - val_loss: 12909.4291\n",
      "Epoch 578/1000\n",
      "2304/2304 [==============================] - 1s 449us/step - loss: 15951.6085 - val_loss: 13372.4539\n",
      "Epoch 579/1000\n",
      "2304/2304 [==============================] - ETA: 0s - loss: 15390.301 - 1s 447us/step - loss: 15447.2109 - val_loss: 16080.7027\n",
      "Epoch 580/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 15932.6666 - val_loss: 13824.2292\n",
      "Epoch 581/1000\n",
      "2304/2304 [==============================] - 1s 444us/step - loss: 15664.1162 - val_loss: 14778.0713\n",
      "Epoch 582/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 15933.7029 - val_loss: 13003.9517\n",
      "Epoch 583/1000\n",
      "2304/2304 [==============================] - 1s 434us/step - loss: 15956.3189 - val_loss: 16177.6614\n",
      "Epoch 584/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 15874.3583 - val_loss: 13080.2921\n",
      "Epoch 585/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 16791.4277 - val_loss: 14088.6852\n",
      "Epoch 586/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 15705.0799 - val_loss: 18454.0310\n",
      "Epoch 587/1000\n",
      "2304/2304 [==============================] - 1s 447us/step - loss: 15808.1309 - val_loss: 16820.5833\n",
      "Epoch 588/1000\n",
      "2304/2304 [==============================] - 1s 464us/step - loss: 16183.8922 - val_loss: 14711.2256\n",
      "Epoch 589/1000\n",
      "2304/2304 [==============================] - 1s 455us/step - loss: 15707.8248 - val_loss: 13111.5689\n",
      "Epoch 590/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 16598.3475 - val_loss: 15295.8216\n",
      "Epoch 591/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 15327.5790 - val_loss: 13901.1989\n",
      "Epoch 592/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 15704.7646 - val_loss: 13032.4837\n",
      "Epoch 593/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 15951.6286 - val_loss: 13004.5162\n",
      "Epoch 594/1000\n",
      "2304/2304 [==============================] - 1s 457us/step - loss: 15690.0711 - val_loss: 12766.9858\n",
      "Epoch 595/1000\n",
      "2304/2304 [==============================] - 1s 451us/step - loss: 15764.8322 - val_loss: 14098.1799\n",
      "Epoch 596/1000\n",
      "2304/2304 [==============================] - 1s 450us/step - loss: 16039.9761 - val_loss: 14123.3657\n",
      "Epoch 597/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 16197.0552 - val_loss: 13257.4783\n",
      "Epoch 598/1000\n",
      "2304/2304 [==============================] - 1s 455us/step - loss: 15653.8917 - val_loss: 13044.1128\n",
      "Epoch 599/1000\n",
      "2304/2304 [==============================] - 1s 461us/step - loss: 16027.9724 - val_loss: 14400.0111\n",
      "Epoch 600/1000\n",
      "2304/2304 [==============================] - 1s 463us/step - loss: 15569.4053 - val_loss: 14585.1445\n",
      "Epoch 601/1000\n",
      "2304/2304 [==============================] - 1s 453us/step - loss: 16236.5629 - val_loss: 14213.7991\n",
      "Epoch 602/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 15410.8726 - val_loss: 13744.9428\n",
      "Epoch 603/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 15623.0107 - val_loss: 13769.4555\n",
      "Epoch 604/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 15422.4580 - val_loss: 13720.9273\n",
      "Epoch 605/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 15629.3536 - val_loss: 13780.4440\n",
      "Epoch 606/1000\n",
      "2304/2304 [==============================] - ETA: 0s - loss: 15512.328 - 1s 449us/step - loss: 15667.5894 - val_loss: 15107.2038\n",
      "Epoch 607/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 15842.8541 - val_loss: 15396.6564\n",
      "Epoch 608/1000\n",
      "2304/2304 [==============================] - 1s 449us/step - loss: 16343.3226 - val_loss: 16418.3831\n",
      "Epoch 609/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 15627.2895 - val_loss: 13080.5057\n",
      "Epoch 610/1000\n",
      "2304/2304 [==============================] - 1s 429us/step - loss: 16138.2899 - val_loss: 13839.8605\n",
      "Epoch 611/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 15795.7646 - val_loss: 14323.5030\n",
      "Epoch 612/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 16000.4406 - val_loss: 14379.4820\n",
      "Epoch 613/1000\n",
      "2304/2304 [==============================] - 1s 432us/step - loss: 15908.1178 - val_loss: 12850.5682\n",
      "Epoch 614/1000\n",
      "2304/2304 [==============================] - 1s 447us/step - loss: 16145.5608 - val_loss: 13584.1879\n",
      "Epoch 615/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 15431.5824 - val_loss: 13392.1331\n",
      "Epoch 616/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 16703.4658 - val_loss: 14792.3624\n",
      "Epoch 617/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 15772.0554 - val_loss: 15436.8464\n",
      "Epoch 618/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 15374.9280 - val_loss: 13229.2396\n",
      "Epoch 619/1000\n",
      "2304/2304 [==============================] - 1s 451us/step - loss: 15145.3295 - val_loss: 14809.5443\n",
      "Epoch 620/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 15543.2908 - val_loss: 16547.6276\n",
      "Epoch 621/1000\n",
      "2304/2304 [==============================] - 1s 440us/step - loss: 15959.9643 - val_loss: 16376.9515\n",
      "Epoch 622/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 15781.9054 - val_loss: 15017.2809\n",
      "Epoch 623/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 15574.1608 - val_loss: 12977.8039\n",
      "Epoch 624/1000\n",
      "2304/2304 [==============================] - 1s 451us/step - loss: 15664.0746 - val_loss: 16512.8301\n",
      "Epoch 625/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 15434.6600 - val_loss: 13110.2442\n",
      "Epoch 626/1000\n",
      "2304/2304 [==============================] - 1s 444us/step - loss: 15833.0265 - val_loss: 13771.7392\n",
      "Epoch 627/1000\n",
      "2304/2304 [==============================] - 1s 444us/step - loss: 15141.2832 - val_loss: 12895.8720\n",
      "Epoch 628/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 15806.2560 - val_loss: 12599.8844\n",
      "Epoch 629/1000\n",
      "2304/2304 [==============================] - 1s 456us/step - loss: 15253.5951 - val_loss: 15702.9138\n",
      "Epoch 630/1000\n",
      "2304/2304 [==============================] - ETA: 0s - loss: 16220.005 - 1s 446us/step - loss: 16211.7160 - val_loss: 13587.7423\n",
      "Epoch 631/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 15374.0106 - val_loss: 16109.5302\n",
      "Epoch 632/1000\n",
      "2304/2304 [==============================] - 1s 433us/step - loss: 15973.8315 - val_loss: 14559.4370\n",
      "Epoch 633/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 15757.8239 - val_loss: 14605.0098\n",
      "Epoch 634/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 15382.4610 - val_loss: 15446.6752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 635/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 15981.6069 - val_loss: 14574.5966\n",
      "Epoch 636/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 15255.1359 - val_loss: 16051.3238\n",
      "Epoch 637/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 15448.6940 - val_loss: 13010.1859\n",
      "Epoch 638/1000\n",
      "2304/2304 [==============================] - 1s 451us/step - loss: 15537.2544 - val_loss: 14263.5028\n",
      "Epoch 639/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 15717.0864 - val_loss: 13682.7131\n",
      "Epoch 640/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 15358.8598 - val_loss: 13410.3975\n",
      "Epoch 641/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 15642.4729 - val_loss: 13778.9329\n",
      "Epoch 642/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 16350.4903 - val_loss: 21586.3186\n",
      "Epoch 643/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 15697.2444 - val_loss: 13705.7611\n",
      "Epoch 644/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 15046.5153 - val_loss: 16719.6757\n",
      "Epoch 645/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 15784.5210 - val_loss: 13796.9106\n",
      "Epoch 646/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 15669.4205 - val_loss: 12644.5263\n",
      "Epoch 647/1000\n",
      "2304/2304 [==============================] - 1s 436us/step - loss: 15642.5598 - val_loss: 13163.1053\n",
      "Epoch 648/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 15469.4455 - val_loss: 14157.4993\n",
      "Epoch 649/1000\n",
      "2304/2304 [==============================] - 1s 431us/step - loss: 15148.3762 - val_loss: 12870.2361\n",
      "Epoch 650/1000\n",
      "2304/2304 [==============================] - 1s 440us/step - loss: 15614.2960 - val_loss: 12830.4788\n",
      "Epoch 651/1000\n",
      "2304/2304 [==============================] - 1s 440us/step - loss: 15455.5890 - val_loss: 14309.9376\n",
      "Epoch 652/1000\n",
      "2304/2304 [==============================] - 1s 451us/step - loss: 15504.0487 - val_loss: 12620.4909\n",
      "Epoch 653/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 15521.0770 - val_loss: 13617.4397\n",
      "Epoch 654/1000\n",
      "2304/2304 [==============================] - 1s 436us/step - loss: 15487.8799 - val_loss: 13259.1685\n",
      "Epoch 655/1000\n",
      "2304/2304 [==============================] - 1s 433us/step - loss: 15204.8147 - val_loss: 13958.5722\n",
      "Epoch 656/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 15702.2916 - val_loss: 13165.9372\n",
      "Epoch 657/1000\n",
      "2304/2304 [==============================] - 1s 440us/step - loss: 15200.5446 - val_loss: 14146.9903\n",
      "Epoch 658/1000\n",
      "2304/2304 [==============================] - 1s 424us/step - loss: 15479.8287 - val_loss: 13355.3974\n",
      "Epoch 659/1000\n",
      "2304/2304 [==============================] - 1s 447us/step - loss: 15440.6843 - val_loss: 13331.1294\n",
      "Epoch 660/1000\n",
      "2304/2304 [==============================] - 1s 427us/step - loss: 15174.4019 - val_loss: 12858.8573\n",
      "Epoch 661/1000\n",
      "2304/2304 [==============================] - 1s 440us/step - loss: 15110.9219 - val_loss: 12799.9921\n",
      "Epoch 662/1000\n",
      "2304/2304 [==============================] - ETA: 0s - loss: 15598.324 - 1s 448us/step - loss: 15515.2419 - val_loss: 12942.6975\n",
      "Epoch 663/1000\n",
      "2304/2304 [==============================] - 1s 449us/step - loss: 16275.1000 - val_loss: 16717.4047\n",
      "Epoch 664/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 16169.6125 - val_loss: 12684.8459\n",
      "Epoch 665/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 15119.5771 - val_loss: 17778.7259\n",
      "Epoch 666/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 15477.4023 - val_loss: 13685.2982\n",
      "Epoch 667/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 15982.5753 - val_loss: 14503.3253\n",
      "Epoch 668/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 15682.8486 - val_loss: 15272.2460\n",
      "Epoch 669/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 15223.0342 - val_loss: 12861.6631\n",
      "Epoch 670/1000\n",
      "2304/2304 [==============================] - 1s 458us/step - loss: 15199.3037 - val_loss: 12720.9584\n",
      "Epoch 671/1000\n",
      "2304/2304 [==============================] - 1s 432us/step - loss: 15367.5364 - val_loss: 12661.7466\n",
      "Epoch 672/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 15171.9637 - val_loss: 13398.3861\n",
      "Epoch 673/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 15272.5848 - val_loss: 13891.8174\n",
      "Epoch 674/1000\n",
      "2304/2304 [==============================] - 1s 457us/step - loss: 15364.4168 - val_loss: 13129.6698\n",
      "Epoch 675/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 15352.5513 - val_loss: 13137.0173\n",
      "Epoch 676/1000\n",
      "2304/2304 [==============================] - 1s 434us/step - loss: 15420.8494 - val_loss: 14306.8728\n",
      "Epoch 677/1000\n",
      "2304/2304 [==============================] - 1s 436us/step - loss: 15816.9606 - val_loss: 13225.2205\n",
      "Epoch 678/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 15520.9441 - val_loss: 12961.2857\n",
      "Epoch 679/1000\n",
      "2304/2304 [==============================] - 1s 444us/step - loss: 16166.1388 - val_loss: 13093.5504\n",
      "Epoch 680/1000\n",
      "2304/2304 [==============================] - 1s 440us/step - loss: 15809.7020 - val_loss: 12755.8231\n",
      "Epoch 681/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 15275.4096 - val_loss: 14574.4372\n",
      "Epoch 682/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 15262.1467 - val_loss: 15008.6220\n",
      "Epoch 683/1000\n",
      "2304/2304 [==============================] - 1s 436us/step - loss: 15541.8515 - val_loss: 12507.0198\n",
      "Epoch 684/1000\n",
      "2304/2304 [==============================] - 1s 449us/step - loss: 15458.2967 - val_loss: 12919.8445\n",
      "Epoch 685/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 15089.8102 - val_loss: 13483.9403\n",
      "Epoch 686/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 15078.7916 - val_loss: 13308.9251\n",
      "Epoch 687/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 14969.1277 - val_loss: 15465.7025\n",
      "Epoch 688/1000\n",
      "2304/2304 [==============================] - 1s 434us/step - loss: 15335.7592 - val_loss: 12952.5697\n",
      "Epoch 689/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 15527.9751 - val_loss: 17052.4783\n",
      "Epoch 690/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 15339.5878 - val_loss: 14415.3618\n",
      "Epoch 691/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 15469.8665 - val_loss: 14247.9914\n",
      "Epoch 692/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 15702.3830 - val_loss: 17907.0751\n",
      "Epoch 693/1000\n",
      "2304/2304 [==============================] - 1s 432us/step - loss: 15765.4085 - val_loss: 13254.7512\n",
      "Epoch 694/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 15332.0656 - val_loss: 18456.6299\n",
      "Epoch 695/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 15447.8551 - val_loss: 12613.2257\n",
      "Epoch 696/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 14687.8422 - val_loss: 13083.2457\n",
      "Epoch 697/1000\n",
      "2304/2304 [==============================] - 1s 451us/step - loss: 15330.3883 - val_loss: 18014.6404\n",
      "Epoch 698/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 15781.9598 - val_loss: 16162.5326\n",
      "Epoch 699/1000\n",
      "2304/2304 [==============================] - 1s 444us/step - loss: 15493.8309 - val_loss: 17285.5977\n",
      "Epoch 700/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 15423.0763 - val_loss: 12625.9207\n",
      "Epoch 701/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 15297.0811 - val_loss: 13373.4003\n",
      "Epoch 702/1000\n",
      "2304/2304 [==============================] - 1s 433us/step - loss: 15426.0078 - val_loss: 13488.4394\n",
      "Epoch 703/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 15466.9557 - val_loss: 14845.1572\n",
      "Epoch 704/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 15065.8811 - val_loss: 12732.2395\n",
      "Epoch 705/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 15026.7516 - val_loss: 13223.4333\n",
      "Epoch 706/1000\n",
      "2304/2304 [==============================] - 1s 440us/step - loss: 15230.2854 - val_loss: 14756.2749\n",
      "Epoch 707/1000\n",
      "2304/2304 [==============================] - 1s 440us/step - loss: 15406.9381 - val_loss: 13308.5025\n",
      "Epoch 708/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 15260.3629 - val_loss: 18116.0258\n",
      "Epoch 709/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 15283.3038 - val_loss: 13337.3922\n",
      "Epoch 710/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 15137.5314 - val_loss: 13265.3038\n",
      "Epoch 711/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 15759.4561 - val_loss: 14528.8233\n",
      "Epoch 712/1000\n",
      "2304/2304 [==============================] - 1s 434us/step - loss: 15241.5850 - val_loss: 12762.3783\n",
      "Epoch 713/1000\n",
      "2304/2304 [==============================] - 1s 429us/step - loss: 14968.4679 - val_loss: 13901.8424\n",
      "Epoch 714/1000\n",
      "2304/2304 [==============================] - 1s 433us/step - loss: 15471.8470 - val_loss: 13121.6925\n",
      "Epoch 715/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 15340.8782 - val_loss: 16421.3447\n",
      "Epoch 716/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 15203.0577 - val_loss: 13425.4123\n",
      "Epoch 717/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 15364.1613 - val_loss: 14678.5288\n",
      "Epoch 718/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 15190.8276 - val_loss: 13293.1317\n",
      "Epoch 719/1000\n",
      "2304/2304 [==============================] - 1s 434us/step - loss: 15485.2657 - val_loss: 16684.7799\n",
      "Epoch 720/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 16062.7201 - val_loss: 12756.5438\n",
      "Epoch 721/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 15582.1366 - val_loss: 13934.6589\n",
      "Epoch 722/1000\n",
      "2304/2304 [==============================] - 1s 436us/step - loss: 14930.4560 - val_loss: 14532.9120\n",
      "Epoch 723/1000\n",
      "2304/2304 [==============================] - 1s 434us/step - loss: 15068.7575 - val_loss: 12634.7672\n",
      "Epoch 724/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 15363.4014 - val_loss: 13434.6976\n",
      "Epoch 725/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 15257.4327 - val_loss: 18779.9918\n",
      "Epoch 726/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 15877.7786 - val_loss: 13005.0797\n",
      "Epoch 727/1000\n",
      "2304/2304 [==============================] - 1s 451us/step - loss: 15261.8529 - val_loss: 12943.7489\n",
      "Epoch 728/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 15631.8654 - val_loss: 12685.7935\n",
      "Epoch 729/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 15082.9631 - val_loss: 12853.9259\n",
      "Epoch 730/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 14789.6620 - val_loss: 13291.1079\n",
      "Epoch 731/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 14779.1322 - val_loss: 12923.7221\n",
      "Epoch 732/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 14957.5461 - val_loss: 14639.1517\n",
      "Epoch 733/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 15424.5130 - val_loss: 12906.5334\n",
      "Epoch 734/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 15092.7149 - val_loss: 12938.7308\n",
      "Epoch 735/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 14993.9403 - val_loss: 13545.9049\n",
      "Epoch 736/1000\n",
      "2304/2304 [==============================] - 1s 449us/step - loss: 14567.7017 - val_loss: 13553.1889\n",
      "Epoch 737/1000\n",
      "2304/2304 [==============================] - 1s 452us/step - loss: 15193.0467 - val_loss: 16555.1944\n",
      "Epoch 738/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 15208.2199 - val_loss: 13473.8713\n",
      "Epoch 739/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 15558.0719 - val_loss: 13306.9156\n",
      "Epoch 740/1000\n",
      "2304/2304 [==============================] - 1s 434us/step - loss: 15205.8590 - val_loss: 13603.1274\n",
      "Epoch 741/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 14711.1822 - val_loss: 17975.5598\n",
      "Epoch 742/1000\n",
      "2304/2304 [==============================] - 1s 434us/step - loss: 15439.0352 - val_loss: 18252.7317\n",
      "Epoch 743/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 14858.0257 - val_loss: 13080.7335\n",
      "Epoch 744/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 15334.4735 - val_loss: 12367.6314\n",
      "Epoch 745/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 15592.4208 - val_loss: 12456.6742\n",
      "Epoch 746/1000\n",
      "2304/2304 [==============================] - 1s 444us/step - loss: 15372.3296 - val_loss: 13681.8867\n",
      "Epoch 747/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 15552.5056 - val_loss: 13359.9924\n",
      "Epoch 748/1000\n",
      "2304/2304 [==============================] - 1s 455us/step - loss: 15025.2245 - val_loss: 13198.7190\n",
      "Epoch 749/1000\n",
      "2304/2304 [==============================] - 1s 431us/step - loss: 15587.6711 - val_loss: 13291.3897\n",
      "Epoch 750/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 15234.3599 - val_loss: 13104.8654\n",
      "Epoch 751/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 15286.4515 - val_loss: 13389.0263\n",
      "Epoch 752/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 15011.1919 - val_loss: 13671.0663\n",
      "Epoch 753/1000\n",
      "2304/2304 [==============================] - 1s 431us/step - loss: 14678.4676 - val_loss: 13171.7875\n",
      "Epoch 754/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 14898.6622 - val_loss: 16745.6186\n",
      "Epoch 755/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 14864.0150 - val_loss: 14502.9875\n",
      "Epoch 756/1000\n",
      "2304/2304 [==============================] - 1s 433us/step - loss: 14996.6706 - val_loss: 14170.1225\n",
      "Epoch 757/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 14958.4556 - val_loss: 13103.1940\n",
      "Epoch 758/1000\n",
      "2304/2304 [==============================] - 1s 429us/step - loss: 14827.0645 - val_loss: 12728.4479\n",
      "Epoch 759/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 15163.0861 - val_loss: 13287.7952\n",
      "Epoch 760/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 14740.0526 - val_loss: 12897.9880\n",
      "Epoch 761/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 14966.5553 - val_loss: 13621.3832\n",
      "Epoch 762/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 14799.1578 - val_loss: 14286.3744\n",
      "Epoch 763/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 14436.9415 - val_loss: 13498.1127\n",
      "Epoch 764/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 15144.7726 - val_loss: 12865.2890\n",
      "Epoch 765/1000\n",
      "2304/2304 [==============================] - 1s 447us/step - loss: 15369.3907 - val_loss: 13159.7846\n",
      "Epoch 766/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 14932.8306 - val_loss: 13319.9050\n",
      "Epoch 767/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 14700.5915 - val_loss: 12496.0247\n",
      "Epoch 768/1000\n",
      "2304/2304 [==============================] - 1s 432us/step - loss: 14422.4766 - val_loss: 12903.3622\n",
      "Epoch 769/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 14772.8422 - val_loss: 12729.0690\n",
      "Epoch 770/1000\n",
      "2304/2304 [==============================] - ETA: 0s - loss: 15411.607 - 1s 442us/step - loss: 15346.0232 - val_loss: 14042.6256\n",
      "Epoch 771/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 15231.9011 - val_loss: 13969.7030\n",
      "Epoch 772/1000\n",
      "2304/2304 [==============================] - 1s 436us/step - loss: 15155.9134 - val_loss: 14454.8772\n",
      "Epoch 773/1000\n",
      "2304/2304 [==============================] - 1s 422us/step - loss: 15267.4308 - val_loss: 13259.2004\n",
      "Epoch 774/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 14596.1594 - val_loss: 13166.8524\n",
      "Epoch 775/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304/2304 [==============================] - 1s 420us/step - loss: 14974.0176 - val_loss: 15539.5823\n",
      "Epoch 776/1000\n",
      "2304/2304 [==============================] - 1s 440us/step - loss: 15169.9119 - val_loss: 13228.7943\n",
      "Epoch 777/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 14630.1758 - val_loss: 13663.6288\n",
      "Epoch 778/1000\n",
      "2304/2304 [==============================] - 1s 456us/step - loss: 14674.2638 - val_loss: 12558.5938\n",
      "Epoch 779/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 14920.9787 - val_loss: 13532.3238\n",
      "Epoch 780/1000\n",
      "2304/2304 [==============================] - 1s 430us/step - loss: 15182.5596 - val_loss: 15951.0021\n",
      "Epoch 781/1000\n",
      "2304/2304 [==============================] - 1s 424us/step - loss: 14665.4194 - val_loss: 12908.2029\n",
      "Epoch 782/1000\n",
      "2304/2304 [==============================] - 1s 426us/step - loss: 14690.0533 - val_loss: 14167.9302\n",
      "Epoch 783/1000\n",
      "2304/2304 [==============================] - 1s 420us/step - loss: 14653.5695 - val_loss: 17030.5262\n",
      "Epoch 784/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 15122.4909 - val_loss: 12568.5641\n",
      "Epoch 785/1000\n",
      "2304/2304 [==============================] - 1s 455us/step - loss: 15265.6583 - val_loss: 12658.0560\n",
      "Epoch 786/1000\n",
      "2304/2304 [==============================] - 1s 461us/step - loss: 14948.2878 - val_loss: 13892.8752\n",
      "Epoch 787/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 14933.4436 - val_loss: 14457.1670\n",
      "Epoch 788/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 15085.2040 - val_loss: 13747.4289\n",
      "Epoch 789/1000\n",
      "2304/2304 [==============================] - 1s 431us/step - loss: 15379.0492 - val_loss: 19403.6576\n",
      "Epoch 790/1000\n",
      "2304/2304 [==============================] - 1s 425us/step - loss: 15351.0965 - val_loss: 15456.3321\n",
      "Epoch 791/1000\n",
      "2304/2304 [==============================] - 1s 421us/step - loss: 14805.4371 - val_loss: 12852.0827\n",
      "Epoch 792/1000\n",
      "2304/2304 [==============================] - 1s 430us/step - loss: 14794.1490 - val_loss: 12984.7817\n",
      "Epoch 793/1000\n",
      "2304/2304 [==============================] - 1s 434us/step - loss: 14434.4702 - val_loss: 12845.6965\n",
      "Epoch 794/1000\n",
      "2304/2304 [==============================] - 1s 455us/step - loss: 14672.0893 - val_loss: 13594.8156\n",
      "Epoch 795/1000\n",
      "2304/2304 [==============================] - 1s 452us/step - loss: 14665.5351 - val_loss: 13367.0068\n",
      "Epoch 796/1000\n",
      "2304/2304 [==============================] - 1s 442us/step - loss: 14691.3304 - val_loss: 12902.3428\n",
      "Epoch 797/1000\n",
      "2304/2304 [==============================] - 1s 440us/step - loss: 15322.3794 - val_loss: 13206.3453\n",
      "Epoch 798/1000\n",
      "2304/2304 [==============================] - 1s 428us/step - loss: 14591.8013 - val_loss: 12691.5457\n",
      "Epoch 799/1000\n",
      "2304/2304 [==============================] - 1s 423us/step - loss: 14238.9320 - val_loss: 12883.0565\n",
      "Epoch 800/1000\n",
      "2304/2304 [==============================] - 1s 422us/step - loss: 14562.4019 - val_loss: 13239.7346\n",
      "Epoch 801/1000\n",
      "2304/2304 [==============================] - 1s 422us/step - loss: 14895.1009 - val_loss: 12925.1134\n",
      "Epoch 802/1000\n",
      "2304/2304 [==============================] - 1s 426us/step - loss: 14640.8927 - val_loss: 12759.0829\n",
      "Epoch 803/1000\n",
      "2304/2304 [==============================] - 1s 418us/step - loss: 14646.1457 - val_loss: 12718.9311\n",
      "Epoch 804/1000\n",
      "2304/2304 [==============================] - 1s 422us/step - loss: 14635.7928 - val_loss: 12636.7899\n",
      "Epoch 805/1000\n",
      "2304/2304 [==============================] - 1s 424us/step - loss: 13996.3517 - val_loss: 14381.3851\n",
      "Epoch 806/1000\n",
      "2304/2304 [==============================] - 1s 428us/step - loss: 14318.8125 - val_loss: 15468.8791\n",
      "Epoch 807/1000\n",
      "2304/2304 [==============================] - 1s 443us/step - loss: 14880.0421 - val_loss: 13126.8431\n",
      "Epoch 808/1000\n",
      "2304/2304 [==============================] - 1s 451us/step - loss: 15080.8034 - val_loss: 14136.3962\n",
      "Epoch 809/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 15469.5417 - val_loss: 12694.9700\n",
      "Epoch 810/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 14256.0116 - val_loss: 12779.1585\n",
      "Epoch 811/1000\n",
      "2304/2304 [==============================] - 1s 422us/step - loss: 15062.1193 - val_loss: 13845.3082\n",
      "Epoch 812/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 14080.0018 - val_loss: 13449.0193\n",
      "Epoch 813/1000\n",
      "2304/2304 [==============================] - 1s 419us/step - loss: 14612.0249 - val_loss: 13226.6961\n",
      "Epoch 814/1000\n",
      "2304/2304 [==============================] - 1s 419us/step - loss: 14793.5876 - val_loss: 12672.4397\n",
      "Epoch 815/1000\n",
      "2304/2304 [==============================] - 1s 424us/step - loss: 14473.2528 - val_loss: 12693.7991\n",
      "Epoch 816/1000\n",
      "2304/2304 [==============================] - 1s 456us/step - loss: 14533.8183 - val_loss: 13692.2471\n",
      "Epoch 817/1000\n",
      "2304/2304 [==============================] - 1s 428us/step - loss: 14729.4543 - val_loss: 13083.3531\n",
      "Epoch 818/1000\n",
      "2304/2304 [==============================] - 1s 423us/step - loss: 14497.7936 - val_loss: 13903.4400\n",
      "Epoch 819/1000\n",
      "2304/2304 [==============================] - 1s 416us/step - loss: 14109.7411 - val_loss: 13386.7339\n",
      "Epoch 820/1000\n",
      "2304/2304 [==============================] - 1s 423us/step - loss: 14994.6687 - val_loss: 14384.5567\n",
      "Epoch 821/1000\n",
      "2304/2304 [==============================] - 1s 424us/step - loss: 14667.1439 - val_loss: 16717.5792\n",
      "Epoch 822/1000\n",
      "2304/2304 [==============================] - 1s 421us/step - loss: 15194.1799 - val_loss: 12673.5339\n",
      "Epoch 823/1000\n",
      "2304/2304 [==============================] - 1s 424us/step - loss: 14383.1840 - val_loss: 13179.6093\n",
      "Epoch 824/1000\n",
      "2304/2304 [==============================] - 1s 448us/step - loss: 14762.0785 - val_loss: 12920.0712\n",
      "Epoch 825/1000\n",
      "2304/2304 [==============================] - 1s 426us/step - loss: 14621.4354 - val_loss: 13191.5815\n",
      "Epoch 826/1000\n",
      "2304/2304 [==============================] - 1s 432us/step - loss: 14784.9669 - val_loss: 12863.7764\n",
      "Epoch 827/1000\n",
      "2304/2304 [==============================] - 1s 416us/step - loss: 14490.9969 - val_loss: 12752.3748\n",
      "Epoch 828/1000\n",
      "2304/2304 [==============================] - 1s 420us/step - loss: 14351.1834 - val_loss: 12576.6446\n",
      "Epoch 829/1000\n",
      "2304/2304 [==============================] - 1s 428us/step - loss: 15036.8720 - val_loss: 12418.5804\n",
      "Epoch 830/1000\n",
      "2304/2304 [==============================] - 1s 413us/step - loss: 14464.0178 - val_loss: 12871.3090\n",
      "Epoch 831/1000\n",
      "2304/2304 [==============================] - 1s 415us/step - loss: 14654.7483 - val_loss: 13201.5221\n",
      "Epoch 832/1000\n",
      "2304/2304 [==============================] - 1s 424us/step - loss: 14462.2237 - val_loss: 13925.0163\n",
      "Epoch 833/1000\n",
      "2304/2304 [==============================] - 1s 423us/step - loss: 14949.5650 - val_loss: 13039.8272\n",
      "Epoch 834/1000\n",
      "2304/2304 [==============================] - 1s 416us/step - loss: 14270.4275 - val_loss: 12572.5281\n",
      "Epoch 835/1000\n",
      "2304/2304 [==============================] - 1s 426us/step - loss: 14900.3856 - val_loss: 12620.9649\n",
      "Epoch 836/1000\n",
      "2304/2304 [==============================] - 1s 409us/step - loss: 14709.6670 - val_loss: 12989.1388\n",
      "Epoch 837/1000\n",
      "2304/2304 [==============================] - 1s 429us/step - loss: 14657.4319 - val_loss: 13228.2369\n",
      "Epoch 838/1000\n",
      "2304/2304 [==============================] - 1s 472us/step - loss: 14724.9100 - val_loss: 12757.6617\n",
      "Epoch 839/1000\n",
      "2304/2304 [==============================] - 1s 430us/step - loss: 14601.4974 - val_loss: 12417.1430\n",
      "Epoch 840/1000\n",
      "2304/2304 [==============================] - 1s 426us/step - loss: 14986.3101 - val_loss: 17222.0426\n",
      "Epoch 841/1000\n",
      "2304/2304 [==============================] - 1s 411us/step - loss: 15089.0709 - val_loss: 12555.9500\n",
      "Epoch 842/1000\n",
      "2304/2304 [==============================] - 1s 416us/step - loss: 14993.7458 - val_loss: 14066.9829\n",
      "Epoch 843/1000\n",
      "2304/2304 [==============================] - 1s 415us/step - loss: 14532.6220 - val_loss: 18880.1531\n",
      "Epoch 844/1000\n",
      "2304/2304 [==============================] - 1s 415us/step - loss: 14584.9105 - val_loss: 13635.9832\n",
      "Epoch 845/1000\n",
      "2304/2304 [==============================] - 1s 422us/step - loss: 14396.5144 - val_loss: 12778.6250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 846/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 14060.4311 - val_loss: 13225.5006\n",
      "Epoch 847/1000\n",
      "2304/2304 [==============================] - 1s 429us/step - loss: 14654.3758 - val_loss: 13023.3650\n",
      "Epoch 848/1000\n",
      "2304/2304 [==============================] - 1s 420us/step - loss: 14654.8375 - val_loss: 13103.5657\n",
      "Epoch 849/1000\n",
      "2304/2304 [==============================] - 1s 431us/step - loss: 14830.6239 - val_loss: 13058.2414\n",
      "Epoch 850/1000\n",
      "2304/2304 [==============================] - 1s 413us/step - loss: 14764.2105 - val_loss: 13815.4281\n",
      "Epoch 851/1000\n",
      "2304/2304 [==============================] - 1s 419us/step - loss: 14613.9552 - val_loss: 13237.9735\n",
      "Epoch 852/1000\n",
      "2304/2304 [==============================] - 1s 418us/step - loss: 14611.7756 - val_loss: 13520.5362\n",
      "Epoch 853/1000\n",
      "2304/2304 [==============================] - 1s 422us/step - loss: 14431.8364 - val_loss: 13894.3668\n",
      "Epoch 854/1000\n",
      "2304/2304 [==============================] - 1s 427us/step - loss: 15137.1521 - val_loss: 22284.5248\n",
      "Epoch 855/1000\n",
      "2304/2304 [==============================] - 1s 432us/step - loss: 14813.4715 - val_loss: 13260.7255\n",
      "Epoch 856/1000\n",
      "2304/2304 [==============================] - 1s 413us/step - loss: 14475.8784 - val_loss: 13210.7836\n",
      "Epoch 857/1000\n",
      "2304/2304 [==============================] - 1s 427us/step - loss: 14933.4577 - val_loss: 12749.1952\n",
      "Epoch 858/1000\n",
      "2304/2304 [==============================] - 1s 412us/step - loss: 14291.2518 - val_loss: 14971.0328\n",
      "Epoch 859/1000\n",
      "2304/2304 [==============================] - 1s 441us/step - loss: 15208.3715 - val_loss: 15258.7614\n",
      "Epoch 860/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 14987.8951 - val_loss: 13586.2548\n",
      "Epoch 861/1000\n",
      "2304/2304 [==============================] - 1s 430us/step - loss: 14492.0720 - val_loss: 17675.6073\n",
      "Epoch 862/1000\n",
      "2304/2304 [==============================] - 1s 425us/step - loss: 14506.5116 - val_loss: 12518.2817\n",
      "Epoch 863/1000\n",
      "2304/2304 [==============================] - 1s 429us/step - loss: 14188.5038 - val_loss: 13342.0729\n",
      "Epoch 864/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 15359.8370 - val_loss: 16495.8379\n",
      "Epoch 865/1000\n",
      "2304/2304 [==============================] - 1s 435us/step - loss: 14965.1403 - val_loss: 13083.3793\n",
      "Epoch 866/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 14175.7473 - val_loss: 13919.3206\n",
      "Epoch 867/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 14389.9764 - val_loss: 13285.3221\n",
      "Epoch 868/1000\n",
      "2304/2304 [==============================] - 1s 439us/step - loss: 14221.4755 - val_loss: 16674.3418\n",
      "Epoch 869/1000\n",
      "2304/2304 [==============================] - 1s 418us/step - loss: 13982.3276 - val_loss: 14725.3013\n",
      "Epoch 870/1000\n",
      "2304/2304 [==============================] - 1s 370us/step - loss: 14150.5493 - val_loss: 12991.6504\n",
      "Epoch 871/1000\n",
      "2304/2304 [==============================] - 1s 369us/step - loss: 14241.1971 - val_loss: 14242.6713\n",
      "Epoch 872/1000\n",
      "2304/2304 [==============================] - 1s 372us/step - loss: 14235.1650 - val_loss: 13106.2846\n",
      "Epoch 873/1000\n",
      "2304/2304 [==============================] - 1s 382us/step - loss: 14888.9844 - val_loss: 13805.2642\n",
      "Epoch 874/1000\n",
      "2304/2304 [==============================] - 1s 380us/step - loss: 14627.0407 - val_loss: 13571.2920\n",
      "Epoch 875/1000\n",
      "2304/2304 [==============================] - 1s 371us/step - loss: 14100.9699 - val_loss: 13488.7944\n",
      "Epoch 876/1000\n",
      "2304/2304 [==============================] - 1s 428us/step - loss: 14497.7654 - val_loss: 12479.8602\n",
      "Epoch 877/1000\n",
      "2304/2304 [==============================] - 1s 449us/step - loss: 14594.8237 - val_loss: 12449.0825\n",
      "Epoch 878/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 14203.2660 - val_loss: 12609.4652\n",
      "Epoch 879/1000\n",
      "2304/2304 [==============================] - 1s 436us/step - loss: 14727.8350 - val_loss: 12740.9682\n",
      "Epoch 880/1000\n",
      "2304/2304 [==============================] - 1s 438us/step - loss: 14200.1342 - val_loss: 12994.0223\n",
      "Epoch 881/1000\n",
      "2304/2304 [==============================] - 1s 425us/step - loss: 14441.8817 - val_loss: 16508.9571\n",
      "Epoch 882/1000\n",
      "2304/2304 [==============================] - 1s 444us/step - loss: 14590.5564 - val_loss: 12704.1016\n",
      "Epoch 883/1000\n",
      "2304/2304 [==============================] - 1s 437us/step - loss: 14141.6053 - val_loss: 14625.2396\n",
      "Epoch 884/1000\n",
      "2304/2304 [==============================] - 1s 377us/step - loss: 14457.2792 - val_loss: 13181.9282\n",
      "Epoch 885/1000\n",
      "2304/2304 [==============================] - 1s 395us/step - loss: 14154.0893 - val_loss: 13142.0646\n",
      "Epoch 886/1000\n",
      "2304/2304 [==============================] - 1s 418us/step - loss: 14099.0075 - val_loss: 12615.8368\n",
      "Epoch 887/1000\n",
      "2304/2304 [==============================] - 1s 416us/step - loss: 14432.0832 - val_loss: 13688.7647\n",
      "Epoch 888/1000\n",
      "2304/2304 [==============================] - 1s 418us/step - loss: 14763.6511 - val_loss: 12546.1383\n",
      "Epoch 889/1000\n",
      "2304/2304 [==============================] - 1s 400us/step - loss: 14257.1427 - val_loss: 15314.1020\n",
      "Epoch 890/1000\n",
      "2304/2304 [==============================] - 1s 407us/step - loss: 14685.3420 - val_loss: 12802.6904\n",
      "Epoch 891/1000\n",
      "2304/2304 [==============================] - 1s 398us/step - loss: 14311.2269 - val_loss: 13009.1204\n",
      "Epoch 892/1000\n",
      "2304/2304 [==============================] - 1s 418us/step - loss: 14441.2347 - val_loss: 13246.5764\n",
      "Epoch 893/1000\n",
      "2304/2304 [==============================] - 1s 421us/step - loss: 13962.6542 - val_loss: 13388.3987\n",
      "Epoch 894/1000\n",
      "2304/2304 [==============================] - 1s 403us/step - loss: 14644.0992 - val_loss: 16022.9132\n",
      "Epoch 895/1000\n",
      "2304/2304 [==============================] - 1s 408us/step - loss: 14487.1708 - val_loss: 13156.4069\n",
      "Epoch 896/1000\n",
      "2304/2304 [==============================] - 1s 422us/step - loss: 14494.2526 - val_loss: 12960.4321\n",
      "Epoch 897/1000\n",
      "2304/2304 [==============================] - 1s 416us/step - loss: 14206.2011 - val_loss: 13049.3552\n",
      "Epoch 898/1000\n",
      "2304/2304 [==============================] - 1s 408us/step - loss: 14461.0843 - val_loss: 13081.9360\n",
      "Epoch 899/1000\n",
      "2304/2304 [==============================] - 1s 418us/step - loss: 14231.0203 - val_loss: 12909.8227\n",
      "Epoch 900/1000\n",
      "2304/2304 [==============================] - 1s 413us/step - loss: 14248.6927 - val_loss: 15215.5780\n",
      "Epoch 901/1000\n",
      "2304/2304 [==============================] - 1s 436us/step - loss: 14475.0888 - val_loss: 12317.9968\n",
      "Epoch 902/1000\n",
      "2304/2304 [==============================] - 1s 424us/step - loss: 14409.6735 - val_loss: 12467.8517\n",
      "Epoch 903/1000\n",
      "2304/2304 [==============================] - 1s 434us/step - loss: 14274.7537 - val_loss: 13747.5079\n",
      "Epoch 904/1000\n",
      "2304/2304 [==============================] - 1s 416us/step - loss: 14452.9215 - val_loss: 12402.7182\n",
      "Epoch 905/1000\n",
      "2304/2304 [==============================] - 1s 410us/step - loss: 14406.0842 - val_loss: 13519.5157\n",
      "Epoch 906/1000\n",
      "2304/2304 [==============================] - 1s 419us/step - loss: 14147.5470 - val_loss: 15100.2449\n",
      "Epoch 907/1000\n",
      "2304/2304 [==============================] - 1s 421us/step - loss: 14430.0315 - val_loss: 13061.8966\n",
      "Epoch 908/1000\n",
      "2304/2304 [==============================] - 1s 392us/step - loss: 14037.1031 - val_loss: 15047.8518\n",
      "Epoch 909/1000\n",
      "2304/2304 [==============================] - 1s 404us/step - loss: 14497.5600 - val_loss: 12198.8537\n",
      "Epoch 910/1000\n",
      "2304/2304 [==============================] - 1s 411us/step - loss: 14366.8323 - val_loss: 14644.8727\n",
      "Epoch 911/1000\n",
      "2304/2304 [==============================] - 1s 408us/step - loss: 14138.7617 - val_loss: 13198.3730\n",
      "Epoch 912/1000\n",
      "2304/2304 [==============================] - 1s 387us/step - loss: 14551.6633 - val_loss: 12640.6007\n",
      "Epoch 913/1000\n",
      "2304/2304 [==============================] - 1s 395us/step - loss: 13842.8162 - val_loss: 13103.7514\n",
      "Epoch 914/1000\n",
      "2304/2304 [==============================] - ETA: 0s - loss: 14027.096 - 1s 406us/step - loss: 14014.5653 - val_loss: 13519.0226\n",
      "Epoch 915/1000\n",
      "2304/2304 [==============================] - 1s 406us/step - loss: 14198.5005 - val_loss: 13368.7286\n",
      "Epoch 916/1000\n",
      "2304/2304 [==============================] - 1s 399us/step - loss: 14822.5560 - val_loss: 15374.5320\n",
      "Epoch 917/1000\n",
      "2304/2304 [==============================] - 1s 402us/step - loss: 14289.0921 - val_loss: 14090.4929\n",
      "Epoch 918/1000\n",
      "2304/2304 [==============================] - 1s 414us/step - loss: 14206.7270 - val_loss: 13044.4802\n",
      "Epoch 919/1000\n",
      "2304/2304 [==============================] - 1s 413us/step - loss: 14460.7795 - val_loss: 13302.6180\n",
      "Epoch 920/1000\n",
      "2304/2304 [==============================] - 1s 433us/step - loss: 14155.9907 - val_loss: 13949.3602\n",
      "Epoch 921/1000\n",
      "2304/2304 [==============================] - 1s 446us/step - loss: 14249.6631 - val_loss: 13610.0707\n",
      "Epoch 922/1000\n",
      "2304/2304 [==============================] - 1s 423us/step - loss: 13840.5041 - val_loss: 13909.9199\n",
      "Epoch 923/1000\n",
      "2304/2304 [==============================] - 1s 417us/step - loss: 13814.5286 - val_loss: 12826.1274\n",
      "Epoch 924/1000\n",
      "2304/2304 [==============================] - 1s 419us/step - loss: 14904.2527 - val_loss: 13254.0662\n",
      "Epoch 925/1000\n",
      "2304/2304 [==============================] - 1s 401us/step - loss: 14411.9720 - val_loss: 12841.3170\n",
      "Epoch 926/1000\n",
      "2304/2304 [==============================] - 1s 406us/step - loss: 14629.6957 - val_loss: 15033.8485\n",
      "Epoch 927/1000\n",
      "2304/2304 [==============================] - 1s 416us/step - loss: 14348.2908 - val_loss: 14876.2320\n",
      "Epoch 928/1000\n",
      "2304/2304 [==============================] - 1s 420us/step - loss: 14075.3151 - val_loss: 13959.1821\n",
      "Epoch 929/1000\n",
      "2304/2304 [==============================] - 1s 409us/step - loss: 14288.5601 - val_loss: 12458.3268\n",
      "Epoch 930/1000\n",
      "2304/2304 [==============================] - 1s 418us/step - loss: 14819.0479 - val_loss: 12861.2873\n",
      "Epoch 931/1000\n",
      "2304/2304 [==============================] - 1s 455us/step - loss: 13920.4318 - val_loss: 12759.2584\n",
      "Epoch 932/1000\n",
      "2304/2304 [==============================] - 1s 410us/step - loss: 14030.5567 - val_loss: 16690.3853\n",
      "Epoch 933/1000\n",
      "2304/2304 [==============================] - 1s 409us/step - loss: 14995.1577 - val_loss: 12408.4145\n",
      "Epoch 934/1000\n",
      "2304/2304 [==============================] - 1s 418us/step - loss: 14364.7210 - val_loss: 13108.9323\n",
      "Epoch 935/1000\n",
      "2304/2304 [==============================] - 1s 413us/step - loss: 14219.7713 - val_loss: 12570.9672\n",
      "Epoch 936/1000\n",
      "2304/2304 [==============================] - 1s 426us/step - loss: 14441.0540 - val_loss: 12885.1096\n",
      "Epoch 937/1000\n",
      "2304/2304 [==============================] - 1s 421us/step - loss: 14052.5804 - val_loss: 12657.4459\n",
      "Epoch 938/1000\n",
      "2304/2304 [==============================] - 1s 445us/step - loss: 13923.9323 - val_loss: 13260.1057\n",
      "Epoch 939/1000\n",
      "2304/2304 [==============================] - 1s 440us/step - loss: 14013.6045 - val_loss: 13729.9309\n",
      "Epoch 940/1000\n",
      "2304/2304 [==============================] - 1s 383us/step - loss: 13959.4256 - val_loss: 12934.7212\n",
      "Epoch 941/1000\n",
      "2304/2304 [==============================] - 1s 429us/step - loss: 14320.5392 - val_loss: 13995.1145\n",
      "Epoch 942/1000\n",
      "2304/2304 [==============================] - ETA: 0s - loss: 14821.987 - 1s 402us/step - loss: 14817.3598 - val_loss: 12546.6118\n",
      "Epoch 943/1000\n",
      "2304/2304 [==============================] - 1s 400us/step - loss: 13632.6041 - val_loss: 12900.9206\n",
      "Epoch 944/1000\n",
      "2304/2304 [==============================] - 1s 392us/step - loss: 14375.1462 - val_loss: 12765.3758\n",
      "Epoch 945/1000\n",
      "2304/2304 [==============================] - 1s 378us/step - loss: 14128.5347 - val_loss: 14432.7357\n",
      "Epoch 946/1000\n",
      "2304/2304 [==============================] - 1s 371us/step - loss: 14197.4131 - val_loss: 13281.8154\n",
      "Epoch 947/1000\n",
      "2304/2304 [==============================] - 1s 373us/step - loss: 14080.5850 - val_loss: 12181.3528\n",
      "Epoch 948/1000\n",
      "2304/2304 [==============================] - 1s 384us/step - loss: 14324.9667 - val_loss: 12349.6915\n",
      "Epoch 949/1000\n",
      "2304/2304 [==============================] - 1s 399us/step - loss: 14012.5511 - val_loss: 13018.6412\n",
      "Epoch 950/1000\n",
      "2304/2304 [==============================] - 1s 390us/step - loss: 14255.0074 - val_loss: 15873.9080\n",
      "Epoch 951/1000\n",
      "2304/2304 [==============================] - 1s 412us/step - loss: 14717.7372 - val_loss: 12762.9510\n",
      "Epoch 952/1000\n",
      "2304/2304 [==============================] - 1s 383us/step - loss: 13974.3436 - val_loss: 12546.4765\n",
      "Epoch 953/1000\n",
      "2304/2304 [==============================] - 1s 372us/step - loss: 14234.0405 - val_loss: 13405.9529\n",
      "Epoch 954/1000\n",
      "2304/2304 [==============================] - 1s 369us/step - loss: 14021.4645 - val_loss: 12466.7760\n",
      "Epoch 955/1000\n",
      "2304/2304 [==============================] - 1s 369us/step - loss: 13925.6977 - val_loss: 13453.8253\n",
      "Epoch 956/1000\n",
      "2304/2304 [==============================] - 1s 369us/step - loss: 14474.5282 - val_loss: 12504.4543\n",
      "Epoch 957/1000\n",
      "2304/2304 [==============================] - 1s 372us/step - loss: 13990.5396 - val_loss: 12301.0943\n",
      "Epoch 958/1000\n",
      "2304/2304 [==============================] - 1s 371us/step - loss: 13936.6069 - val_loss: 13217.6736\n",
      "Epoch 959/1000\n",
      "2304/2304 [==============================] - 1s 374us/step - loss: 14229.4402 - val_loss: 12295.4087\n",
      "Epoch 960/1000\n",
      "2304/2304 [==============================] - 1s 373us/step - loss: 14244.5842 - val_loss: 12878.1249\n",
      "Epoch 961/1000\n",
      "2304/2304 [==============================] - 1s 382us/step - loss: 13966.3603 - val_loss: 13099.1012\n",
      "Epoch 962/1000\n",
      "2304/2304 [==============================] - 1s 377us/step - loss: 14448.5880 - val_loss: 13052.2593\n",
      "Epoch 963/1000\n",
      "2304/2304 [==============================] - 1s 377us/step - loss: 13804.9836 - val_loss: 13043.3453\n",
      "Epoch 964/1000\n",
      "2304/2304 [==============================] - 1s 377us/step - loss: 14432.2674 - val_loss: 12421.3675\n",
      "Epoch 965/1000\n",
      "2304/2304 [==============================] - 1s 389us/step - loss: 13866.9203 - val_loss: 12710.0364\n",
      "Epoch 966/1000\n",
      "2304/2304 [==============================] - 1s 383us/step - loss: 14118.5367 - val_loss: 12608.4325\n",
      "Epoch 967/1000\n",
      "2304/2304 [==============================] - 1s 383us/step - loss: 13863.9051 - val_loss: 12497.9934\n",
      "Epoch 968/1000\n",
      "2304/2304 [==============================] - 1s 382us/step - loss: 14055.5219 - val_loss: 12798.6343\n",
      "Epoch 969/1000\n",
      "2304/2304 [==============================] - 1s 382us/step - loss: 14203.0176 - val_loss: 12626.2237\n",
      "Epoch 970/1000\n",
      "2304/2304 [==============================] - 1s 370us/step - loss: 14280.9816 - val_loss: 12754.1671\n",
      "Epoch 971/1000\n",
      "2304/2304 [==============================] - 1s 373us/step - loss: 13951.1702 - val_loss: 12712.6477\n",
      "Epoch 972/1000\n",
      "2304/2304 [==============================] - 1s 383us/step - loss: 14304.2816 - val_loss: 17329.0146\n",
      "Epoch 973/1000\n",
      "2304/2304 [==============================] - 1s 371us/step - loss: 15169.8071 - val_loss: 12271.1659\n",
      "Epoch 974/1000\n",
      "2304/2304 [==============================] - 1s 384us/step - loss: 14481.7509 - val_loss: 13241.9281\n",
      "Epoch 975/1000\n",
      "2304/2304 [==============================] - 1s 374us/step - loss: 13924.4217 - val_loss: 13457.2602\n",
      "Epoch 976/1000\n",
      "2304/2304 [==============================] - 1s 374us/step - loss: 14402.3264 - val_loss: 12988.4250\n",
      "Epoch 977/1000\n",
      "2304/2304 [==============================] - 1s 373us/step - loss: 13917.5887 - val_loss: 14214.5680\n",
      "Epoch 978/1000\n",
      "2304/2304 [==============================] - 1s 374us/step - loss: 13851.6644 - val_loss: 14613.6863\n",
      "Epoch 979/1000\n",
      "2304/2304 [==============================] - 1s 375us/step - loss: 13638.6078 - val_loss: 12895.8128\n",
      "Epoch 980/1000\n",
      "2304/2304 [==============================] - 1s 371us/step - loss: 14048.2012 - val_loss: 15794.4410\n",
      "Epoch 981/1000\n",
      "2304/2304 [==============================] - 1s 377us/step - loss: 14248.7375 - val_loss: 12595.4897\n",
      "Epoch 982/1000\n",
      "2304/2304 [==============================] - 1s 378us/step - loss: 13976.9222 - val_loss: 12944.9743\n",
      "Epoch 983/1000\n",
      "2304/2304 [==============================] - 1s 380us/step - loss: 13878.1132 - val_loss: 14461.1452\n",
      "Epoch 984/1000\n",
      "2304/2304 [==============================] - 1s 377us/step - loss: 13750.8418 - val_loss: 13647.0555\n",
      "Epoch 985/1000\n",
      "2304/2304 [==============================] - 1s 383us/step - loss: 13821.2843 - val_loss: 15657.8466\n",
      "Epoch 986/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304/2304 [==============================] - 1s 387us/step - loss: 13951.3668 - val_loss: 12503.4262\n",
      "Epoch 987/1000\n",
      "2304/2304 [==============================] - 1s 374us/step - loss: 13732.6873 - val_loss: 12674.2793\n",
      "Epoch 988/1000\n",
      "2304/2304 [==============================] - 1s 384us/step - loss: 14143.0375 - val_loss: 12801.1951\n",
      "Epoch 989/1000\n",
      "2304/2304 [==============================] - 1s 376us/step - loss: 14586.6882 - val_loss: 13891.1528\n",
      "Epoch 990/1000\n",
      "2304/2304 [==============================] - 1s 376us/step - loss: 14043.9540 - val_loss: 12848.9969\n",
      "Epoch 991/1000\n",
      "2304/2304 [==============================] - 1s 383us/step - loss: 14709.4745 - val_loss: 13196.8980\n",
      "Epoch 992/1000\n",
      "2304/2304 [==============================] - 1s 379us/step - loss: 13989.1206 - val_loss: 13358.9111\n",
      "Epoch 993/1000\n",
      "2304/2304 [==============================] - 1s 373us/step - loss: 13895.6108 - val_loss: 12177.6148\n",
      "Epoch 994/1000\n",
      "2304/2304 [==============================] - 1s 377us/step - loss: 13983.3783 - val_loss: 12892.7427\n",
      "Epoch 995/1000\n",
      "2304/2304 [==============================] - 1s 377us/step - loss: 14490.0895 - val_loss: 14749.1190\n",
      "Epoch 996/1000\n",
      "2304/2304 [==============================] - 1s 376us/step - loss: 14125.6700 - val_loss: 13738.9878\n",
      "Epoch 997/1000\n",
      "2304/2304 [==============================] - 1s 388us/step - loss: 14066.8977 - val_loss: 12453.9828\n",
      "Epoch 998/1000\n",
      "2304/2304 [==============================] - 1s 388us/step - loss: 13551.5174 - val_loss: 14619.2152\n",
      "Epoch 999/1000\n",
      "2304/2304 [==============================] - 1s 386us/step - loss: 13681.4783 - val_loss: 12727.4434\n",
      "Epoch 1000/1000\n",
      "2304/2304 [==============================] - 1s 383us/step - loss: 14083.6308 - val_loss: 13065.0160\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU,PReLU,ELU\n",
    "from keras.layers import Dropout\n",
    "\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 50, init = 'he_uniform',activation='relu',input_dim = 174))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 25, init = 'he_uniform',activation='relu'))\n",
    "\n",
    "# Adding the third hidden layer\n",
    "classifier.add(Dense(output_dim = 50, init = 'he_uniform',activation='relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'he_uniform'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(loss=root_mean_squared_error, optimizer='Adamax')\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "model_history=classifier.fit(X_train.values, y_train.values,validation_split=0.20, batch_size = 10, nb_epoch = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_pred=classifier.predict(df_Test.drop(['SalePrice'],axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
